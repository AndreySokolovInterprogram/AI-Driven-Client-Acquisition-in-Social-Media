{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Loading libraries"
      ],
      "metadata": {
        "id": "rbv3wIXkQpkf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the model"
      ],
      "metadata": {
        "id": "r2v_X2fA0Df5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547,
          "referenced_widgets": [
            "9bfbbf6b2788413e9825119c2bf9ac5e",
            "f32b16e780be498bb4f744d197cc8593",
            "6332c1263c0145689d5c2f31d4b7aae4",
            "64c01a44fbd4413abf7a06f8557a3bd9",
            "afdf93ffe26042eba359838315caeecd",
            "6618559b44ed42f0926b7fc72a7e9cee",
            "f1a26b9b7a5641509c60ab93685f24c5",
            "763fea6a35bb49c78349c5b95edb81eb",
            "d75cc18818be43ebbc401f5ee1ea07f8",
            "7a5fe1e88c6b470e97c7340dff8b5e04",
            "8041746e69db46f1ba854042af97c2a0",
            "5892c9e0b8c848d089f36383f95855a4",
            "a3eb824f601d41d8a5293e104c301f1b",
            "004473f248464f53945d69e2062a4a73",
            "4473beeeeac54391b007f92cdac3d508",
            "85176b6368cf40daad7f92233bde056f",
            "b858f7dd7e8d405fbdd0b71d445d1cd0",
            "8161d30cb1a94ff7a66e89d0539a48e6",
            "e2172d3469c64fdc8ae7940bd3c7899d",
            "0da5662be4d2468ea4d42b856e6b5c35",
            "c080653a9c3942408a2c4fcad873b64f",
            "18f60522f64d457aa1fa6282ece4cf11",
            "170303c89d6e41c9b37c92d680bd4994",
            "1455e42bd4374209a0ec35f1075b0f78",
            "bf93b837e12c492ea4a72d0a6828a8d0",
            "5f875e44b7dd424088ef0a38fc771dd1",
            "388167f32c8041b9adbc846b9f6572d6",
            "f6c9a5f110fa49739d5d065f9976a113",
            "de5a839f2c7d423b8a8f679b4f7de236",
            "d7c5584e3fe44ab2a1a186acd8ac8196",
            "4e8297bdc86a47f1a5acc60cf9072019",
            "ff03578a3cb04eb892fa7cf1c0c5a83f",
            "3d940fc7282441959d3076ea96aaa99b",
            "7c1549e113294c8b9d9bf3337db05c77",
            "0096b132e7c44e50b865385d52f22c0f",
            "20d46d5824d442d2bdd4e6f0d1fceea9",
            "83ca88088f9849198233804741ba4211",
            "95f8cf51af5648a89a68716848b00b0f",
            "99ae1e7be34642c1ad2b9be0d38c8726",
            "f6ec23b184c649a7bbc04a434058a50e",
            "5243ff1682b54062ba8db6b2c0ab49c5",
            "b578f2ce289c4fe3a30dbfdbbb591ff7",
            "554c2bfec7b14c859adec48e6549d627",
            "4dc362b4808e45c392f2b1df7ca9d678",
            "eb98ff27f2ce4e7eb1d5c5777cbffc9b",
            "ea1f3066ce964911bb0c599846c68603",
            "baff9cff82d540b283236a9693c5c0ae",
            "78ccee3b6d974d39b2942ff31ffb1e62",
            "917f5834edf14bf084d2e2e5bbb3fc99",
            "4910201f021047319416b346fa505941",
            "5b0d4fef4a034223b45610418e742c51",
            "5d70a8218146485b8d1fb0d727c8cd0e",
            "a3352cbadb65489da31c4cfe1a6fd44c",
            "1003e607faa943ecbc342e266090edc9",
            "00d38f78e23848069949ca5cdaaa7c57",
            "2c7a8b91c7a44006bd2290b339ea6f1d",
            "982d2ecd8c144171993689398eb86bfc",
            "c40b82dc90c34fc4a7f4f3c37141b382",
            "d99044dd0ce24885b5ee130edf0efbea",
            "dac960d3eb424d638573996d40198fff",
            "eb2642e44dd04e70a1789bad8f3ff93b",
            "6242c11c1cba42a99ef5d73b3452599b",
            "90ff89d0dffb436784a5877e966ca75d",
            "daff739a9d6a442bb17fde3c5dcad095",
            "bc929340b9bc4651a192a003f3fcee43",
            "8ebfe00547ea4a939d6dc719728053d8",
            "1e3fab3dffea4219b50094d4b76f7217",
            "21bc7a06c67c45f28e5354df8200b060",
            "f116143a4cc64ea08cf5842a06a3be98",
            "cbd36a54e2ea4ed690cfb0dca43c3938",
            "fc5d9e6ea01a4d77a2832cc4d5e597bc",
            "b002fc5e37264677b7756eabdd715d4f",
            "b6b6aa69a8e5469f8295d427bf30e435",
            "1164dbd997a341a1aeff242e1d005455",
            "87527ac11a9b42bc98506ac7b65d75b5",
            "43ed4c0bb46840ae88e07d9c9f6b056c",
            "ec38c85c2bb44fba891c605f27f8329a",
            "742a4f1896ab4a74968ef82576841bb4",
            "1f290c49c9b74a1c960dc7b5d06e8859",
            "bac4484f0bdd408f8c07ea49ee9ee2cb",
            "9a7c4616944e457a9ceafe45d58bf38d",
            "f8565f3fe7f24360a161148eb336ed23",
            "e26834b0f64b477ab6002ef87ff3d7bc",
            "30ddebd285714ba98b5b3517e4d13023",
            "e10d82f013d74e71a9cab54c68ca2202",
            "e28cdd9abb44458797ebf1887555ccae",
            "65480ffa18df47b5a389555bd5d86bcd",
            "7b31ab103ae74b81b9eb9c960925d6ac",
            "7c44ce90c4a4423e9e902348a3c833a0",
            "521b96c7e9254212ae4d6cd8596c4522",
            "a7ee6486809a46e3ba297aac3e06d3ce",
            "223b33b5c8974bdda59791634b1646c5",
            "ca746093536a4f7aa31abe53ab8b71aa",
            "97cc3a8b706847cd9f47ceac68e742d5",
            "0210d577dd03405ead358b9d22657f2d",
            "c82cbe20734d4d33a9041defa9282002",
            "b5d6837d5d5b4fabbcc209110da1ae3c",
            "9bf1eb33a2984337a6f4a544bd4d531d",
            "634ab60f17c844b08bb1372280f00c6b",
            "49ed9d94c4064bc8b7f1408270aa28c4",
            "9cf6995b25ff4fc9bd8cedba7ba9b0d6",
            "ad1b40b1901a490798eaeb3d7ed428a1",
            "1c857b16686b40e6857480824acd0181",
            "aab3cf928cb14a998f110f709e1aaa4b",
            "4f812f7d79f549a4bb719995a3f7ee04",
            "808f4a93ca3149419d274e39b58e48f1",
            "4fbb103e4ff44ed4af430265b6860081",
            "835623fede0c4b07a2f3e7abf6ee5357",
            "0cf462c06a1445c9b7307e52ec6f0f01",
            "b3c4c2a85e24415084ea94b4f5ca7bbf",
            "a214f2a5d1cd4bcaad15e43e804c7011",
            "78f7c708ffc744018a09dfb9c4378656",
            "db409ad0173f44c5ac94213348509d3d",
            "36ecf8e8ad5b44e4b9c248fb4c161e7b",
            "1f40b0ca60b944d69a6a567b97b3aed3",
            "d11fb2ff9d6749759b97c3b189e7c1bc",
            "25967f48b6d74e739bf1dba562706d50",
            "216d72d45fc344e6a3d0f80559272a7b",
            "a2d5b118b6164345930b5788b3a113e0",
            "89fb687ef7994429b6a1f3d0baca6a58",
            "9ef63e5c7ad945b08c9381233d37fa9f",
            "c11f54348b194631829614e4075bc034",
            "8d7a833b069345a09418226fe2b763d2",
            "247ffbd719654c8e875408e5c57fcc15",
            "b8dad73666274b8eb5951bc500368c24",
            "fc237eebfafd4c088624b258ae96dd90",
            "a097e223f8c244bf92be2e8ddfe88482",
            "073bc04d1b3244479d02063d185b28e8",
            "fdbc48e61daf4cd68a7cefa6f191aede",
            "17dc32be9f1b49c7a6464765d4ced0b5",
            "0e07b42d3e824ae690dac7cf78b6e1f3",
            "d469298dd3f3419fad1e21dd6083ba9a"
          ]
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "fccaceff-0e9b-4c4b-cfa3-a6f9f2db4c89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Fast Llama patching release 2024.6\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. Xformers = 0.0.26.post1. FA = False.\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "lightblue/suzume-llama-3-8B-multilingual-orpo-borda-half does not have a padding token! Will use pad_token = <|reserved_special_token_250|>.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# The name of the models to upload\n",
        "models = [\n",
        "    \"lightblue/suzume-llama-3-8B-multilingual-orpo-borda-half\",\n",
        "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Qwen2-7B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\"\n",
        "] # Try more models at https://huggingface.co/models\n",
        "n=2\n",
        "model_name = models[n].split('/')[1]\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = models[n], # Reminder we support ANY Hugging Face model!\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Modification Parameters LoRA"
      ],
      "metadata": {
        "id": "SXd9bTZd1aaL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "069507e0-57dd-4db0-fa53-5ad871b996dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2024.6 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 1,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing a dataset for training"
      ],
      "metadata": {
        "id": "vITh0KVJ10qX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post Generation"
      ],
      "metadata": {
        "id": "MgnJ7QaI6X1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "# 10.5281/zenodo.17054610\n",
        "record_id = '17054610'\n",
        "filename_to_download = 'vk_posts_balanced.xlsx'\n",
        "\n",
        "api_url = f'https://zenodo.org/api/records/{record_id}'\n",
        "\n",
        "try:\n",
        "    print(f\"Searching for file '{filename_to_download}' in Zenodo repository with ID: {record_id}...\")\n",
        "\n",
        "    response = requests.get(api_url)\n",
        "    response.raise_for_status()\n",
        "    data = response.json()\n",
        "\n",
        "    file_url = None\n",
        "    for file in data.get('files', []):\n",
        "        if file.get('key') == filename_to_download:\n",
        "            file_url = file.get('links', {}).get('self')\n",
        "            break\n",
        "\n",
        "    if file_url:\n",
        "        print(f\"File '{filename_to_download}' found. Starting download...\")\n",
        "\n",
        "        file_response = requests.get(file_url, stream=True)\n",
        "        file_response.raise_for_status()\n",
        "\n",
        "        # Save the file to the current directory\n",
        "        with open(filename_to_download, 'wb') as f:\n",
        "            for chunk in file_response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "\n",
        "        print(f\"File '{filename_to_download}' downloaded successfully.\")\n",
        "    else:\n",
        "        print(f\"File '{filename_to_download}' not found in the repository.\")\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"An error occurred while accessing Zenodo: {e}\")\n",
        "\n",
        "posts = pd.read_excel('vk_posts_balanced.xlsx')\n",
        "\n",
        "# Converting all columns to a string type\n",
        "posts['Text'] = posts['text'].astype(str)\n",
        "\n",
        "# Dictionary for replacing values\n",
        "replacement_dict = {\n",
        "    0: 'optional to nothing (0)',\n",
        "    1: 'binding to discount (1)',\n",
        "    2: 'obliging the group to make a gift (2)',\n",
        "    3: 'obliging the group to give cashback (3)',\n",
        "    4: 'obligating the group to deliver the goods on time (4)'\n",
        "}\n",
        "posts[\"Instruction\"] = \"Generate an advertising post for the food delivery group promising \"\n",
        "posts[\"Instruction\"] = posts[\"Instruction\"] + posts[\"class\"].map(replacement_dict)\n",
        "posts['Instruction'] = posts['Instruction'].astype(str)\n",
        "\n",
        "posts[\"sys_promt\"] = (\"You are an expert in generating VK group posts in the field of food delivery.\n",
        "\"You will be given instructions for generating a group post.\n",
        "\"Your task is to create a group post that follows the instructions.\"\n",
        ")\n",
        "posts['sys_promt'] = posts['sys_promt'].astype(str)\n",
        "\n",
        "# Replacing NaN values with empty strings\n",
        "posts = posts.fillna('')\n",
        "\n",
        "train = posts\n",
        "train.head(3)\n",
        "\n",
        "from datasets import Dataset\n",
        "# Создание Dataset\n",
        "ds = Dataset.from_dict({\n",
        "    \"output\": posts['Text'],\n",
        "    \"input\": posts['Instruction'],\n",
        "    \"instruction\": posts['sys_promt']\n",
        "})\n",
        "\n",
        "ds[0]"
      ],
      "metadata": {
        "id": "0zaRgzt-_t5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting a dataset to a suitable format"
      ],
      "metadata": {
        "id": "kkXvBogi6hiQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = ds\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training\n",
        "Now we will use the `SFTTrainer` from Huggingface TRL! More documentation is here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer ). We do 60 steps for speed, but we can set `num_train_epochs=1` for a full run and disable `max_steps=None'. We also support the `DPOTrainer` from TRL!"
      ],
      "metadata": {
        "id": "idAEIeSQ3xdS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123,
          "referenced_widgets": [
            "172748d42b794cef84878f924307e18a",
            "359a4cc0de514b0e89c0b2b38c343c1f",
            "671b821699aa40aaa1c106fd2bb14a16",
            "b193e9268a2c4f95810e0b16195308c5",
            "af1afee788614d43be2a25fe9a3e9cc8",
            "7516708ace3e4fa195bbc950b605e256",
            "eef842904774449a847cf82825ad4643",
            "a19bb7ef723642c0927dc813dbd59733",
            "fb75757ff1774e099c5bad91f429dc83",
            "c27d5c53d511485fba9d080bb31f40f5",
            "ab1b418e834245548576c72bb576d048"
          ]
        },
        "outputId": "aadeb040-dcf3-4f85-9321-cedc6f748c20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "\n",
        "        # Use num_train_epochs = 1, warmup_ratio for full training runs!\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,\n",
        "\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 5,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ejIt2xSNKKp",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6af2c40e-5200-463a-c526-4bcd9d596ab1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.748 GB.\n",
            "5.594 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "#@title Indicators of available video memory\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "255be78a-7e95-42bb-88f3-bc6281515096"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 2,626 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 60\n",
            " \"-____-\"     Number of trainable parameters = 41,943,040\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 09:51, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.590400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.733800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.309400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.162900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.154400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.094800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.996300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.092400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.025600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.211700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>1.180400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.174100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCqnaKmlO1U9",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "999a6213-d83e-411e-cdd7-0f3ff61317e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "613.6526 seconds used for training.\n",
            "10.23 minutes used for training.\n",
            "Peak reserved memory = 8.668 GB.\n",
            "Peak reserved memory for training = 3.074 GB.\n",
            "Peak reserved memory % of max memory = 58.774 %.\n",
            "Peak reserved memory for training % of max memory = 20.844 %.\n"
          ]
        }
      ],
      "source": [
        "#@title Video memory usage indicators and statistics\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Запуск модели"
      ],
      "metadata": {
        "id": "ekOmTR1hSNcr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating posts"
      ],
      "metadata": {
        "id": "U8X6EYaKuCy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "post_result=''\n",
        "for Instruction in posts[\"Instruction\"].unique():\n",
        "    # alpaca_prompt = Copied from above\n",
        "    FastLanguageModel.for_inference(model) # Unsloth has 2x faster inference!\n",
        "    inputs = tokenizer(\n",
        "    [\n",
        "        alpaca_prompt.format(\n",
        "            posts[\"sys_promt\"][0], # instruction\n",
        "            Instruction, # input\n",
        "            \"\", # output - leave this blank for generation!\n",
        "        )\n",
        "    ], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 1000, use_cache = True)\n",
        "    message_answer = tokenizer.batch_decode(outputs)[0].rsplit('Response:', 1)[-1]\n",
        "    match = re.search(r'[012345]', str(Instruction))\n",
        "    post_result+=str(int(match.group()))+' - '+str(message_answer)+'\\n'\n",
        "print(post_result)"
      ],
      "metadata": {
        "id": "CkCJQUJ7s4Jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "post_result=''\n",
        "for Instruction in posts[\"Instruction\"].unique():\n",
        "    # alpaca_prompt = Copied from above\n",
        "    FastLanguageModel.for_inference(model) # Unsloth has 2x faster inference!\n",
        "    inputs = tokenizer(\n",
        "    [\n",
        "        alpaca_prompt.format(\n",
        "            \"Answer in English\\n \"+str(posts[\"sys_promt\"][0]), # instruction\n",
        "            \"Answer in English\\n \"+Instruction, # input\n",
        "            \"\", # output - leave this blank for generation!\n",
        "        )\n",
        "    ], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 1000, use_cache = True)\n",
        "    message_answer = tokenizer.batch_decode(outputs)[0].rsplit('Response:', 1)[-1]\n",
        "    match = re.search(r'[012345]', str(Instruction))\n",
        "    post_result+=str(int(match.group()))+' - '+str(message_answer)+'\\n'\n",
        "print(post_result)"
      ],
      "metadata": {
        "id": "9fwa-j2g_Bdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "my_file = open(f\"{model_name}_post_gen.txt\", \"w+\")\n",
        "my_file.write(post_result)\n",
        "my_file.close()"
      ],
      "metadata": {
        "id": "_eyPf3OIL3AV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}