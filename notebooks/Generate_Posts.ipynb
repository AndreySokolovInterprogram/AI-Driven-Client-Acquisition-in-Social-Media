{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Loading libraries"
      ],
      "metadata": {
        "id": "rbv3wIXkQpkf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the model"
      ],
      "metadata": {
        "id": "r2v_X2fA0Df5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412,
          "referenced_widgets": [
            "11835392bc8d4c979169c89459bca361",
            "3682344c471149adb4d98cb5a29fe48a",
            "6ae5af97b2b74d19b69a49129338c70a",
            "ec0eb2ed58924c77a8244173f4165d61",
            "0168ccb85fc44734ac5ef6d7c1f8743b",
            "d0a61f09e0e74430ad84d0ebdf123abb",
            "1a89f9ebd6f842fd9821594d699a81ec",
            "efc9e7cf85a248bba845ea6232ff336a",
            "84b2e4e722b747d291a8198a018424e3",
            "dc88ec855db54762a968b2d4fb9f3c22",
            "7ff3256f1ff44dd7a7702cdf54081252",
            "84608c7306be4628a1e60f584d673f10",
            "6491ad8c27af4ecd9b4f2768fb70191e",
            "67b7199415b644f18f893c41cf5bdaf4",
            "e23c011e42b64880a0240ad82b941846",
            "902ff20b4b034d0cad6b786f7c06eafb",
            "5a586d3592a0424886c77032847573e8",
            "a144eacfab9b45c1ab981269facbeaa1",
            "eadab466a40648e28809715adb020cf7",
            "94d363deffe74d408d3f631abae5303e",
            "f618b6b061dd4eb888c6be9eee33a95c",
            "211f6acb28c04fe6b7e1acce1b1b9669",
            "aa93157808fe42658dd5d76646d81f46",
            "df31fc258d144103b3ded5bba17a57e8",
            "717ccaee364d4055815a55edb9cf6d29",
            "f3fbd7e919b145edbfcbc838300aac97",
            "f1524664f94d4502bf57ebd1e2230618",
            "35cb8b732880462eaac5a3725c33a476",
            "3f2c3824477d40e6ac2d79d017b5717f",
            "3cfe0ceb08614cd5bae7d3106d443198",
            "dee9bedc82b741a0866b363c6d554a30",
            "bad4169cb6924fb9be544b5504836654",
            "d51117e6b6934545b635044f0a07019b",
            "d2589495a0b74ba9ad51c4f4537fa12d",
            "1438c6f750714839823278eb998991e8",
            "55e9d0e01a914d48a6ede2af94d36dcd",
            "cd8a26b58ea84b688353271e153d7d57",
            "d8fdc97039424c6fac81a9cbc554fb4d",
            "f2493aa38ca4413e86399663af7dfaa8",
            "b66a1090b5834c4595e68c6a18f3a1cd",
            "1ae35fc07bb64f579fb45d9ee9cd5179",
            "dc7476d16fbe46c7900a2abea3defdf5",
            "090a8ae0368a48e9a897c5b9f6d23171",
            "16b1976816b14491afcd3884cb581b00",
            "791a287f09614621bf10da1484744645",
            "a8b20567a2d440cda3a84a15b8871a8f",
            "4b1e008694254172ba0a33a4d2051302",
            "e1513292311a4ac791f7cc28137e7f39",
            "f910fc733b3e4168b072e893a96c7ab1",
            "3f3f13737de641f89793e7523d775de0",
            "08a367ca0c0642a39fcc0bda4ad5acf5",
            "d1b2cc1937a644bdab128ab8178dba25",
            "a753d700f82d4eeea2e33427bf861dbe",
            "c0d7e68bb2084bb29da3198db331a88b",
            "7ef3264a443343808a6f76fe33b06bd9",
            "5a719d1a9e2c4d8dad54aa5d5ec403be",
            "b273f9dfeabf44b1a6c0386058200123",
            "a895a07d225144f6abdf1b1dc5fa2a9c",
            "39f56dd60183438ba1712c4ddc7fd39e",
            "d56ddf9a18a4474298ae202decb687ea",
            "600d5d17dca648d397561de4428424b4",
            "04b5d92e2a6a44c4ace6dbc37f9f69eb",
            "3d6299f4fdd94f6799726417fc43dd97",
            "795c5aeb3fb9455e851b873d1d301536",
            "89104875879647b986613637701bf3a9",
            "7e4d23ef8feb431ebb16321b2e6dd86b",
            "89463c12c3824c39a94f9842678c7f7e",
            "c4670e3972884be3955180b80e123d14",
            "023cc446ad524fb9a2c188bfce088464",
            "ac758d5b90f74c69b307ffa63121586d",
            "1479ec003aa849c6863ef894bd22451b",
            "6f4563aad02a4f0cb29009fed3fc1752",
            "36e897205fa74b80b4f1cdd71116c696",
            "76714cf459a74b858ee5ae630510fa45",
            "ff0dd4c984f14b70bec79fe3da3c37be",
            "c6bf2c17713e4a94a518922c518a88b9",
            "4b2d6050469f43808d20f69fce8b7362",
            "2805800863fd45c68e60026e9df50354",
            "e0730ebf4a8a4d5a909cb25037b0a844",
            "13bbc0c8b87b4d16ad30b8e5c5358493",
            "66e1d6c10c0847e79607bbaaecbcf239",
            "f1cc63435d034470b793d296e3d640f4",
            "e6ecb62310ba457ea8dac83c0ad88a0e",
            "ede54ea241834ba98a09abf7b0181161",
            "3a6a1feaff1e4ef79544432a12226666",
            "a47d90475dac4f4bbb3a69f647b766dc",
            "620407e8b339499384378638890e270b",
            "891a8c8e6c0b4aedbeb82da14866a84b"
          ]
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "39298434-5500-472b-93c1-2f9b8a562afd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.11.4: Fast Qwen2 patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# The name of the models to upload\n",
        "models = [\n",
        "    \"lightblue/suzume-llama-3-8B-multilingual-orpo-borda-half\",\n",
        "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Qwen2-7B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\"\n",
        "] # Try more models at https://huggingface.co/models\n",
        "n=2\n",
        "model_name = models[n].split('/')[1]\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = models[n], # Reminder we support ANY Hugging Face model!\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Modification Parameters LoRA"
      ],
      "metadata": {
        "id": "SXd9bTZd1aaL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6bZsfBuZDeCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "577d2258-b7dd-4193-e5bd-d2d33143ec83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.11.4 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 1,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing a dataset for training"
      ],
      "metadata": {
        "id": "vITh0KVJ10qX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post Generation"
      ],
      "metadata": {
        "id": "MgnJ7QaI6X1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# 10.5281/zenodo.17054610\n",
        "record_id = '17054610'\n",
        "filename_to_download = 'vk_posts_balanced.xlsx'\n",
        "\n",
        "api_url = f'https://zenodo.org/api/records/{record_id}'\n",
        "\n",
        "try:\n",
        "    print(f\"Searching for file '{filename_to_download}' in Zenodo repository with ID: {record_id}...\")\n",
        "\n",
        "    response = requests.get(api_url)\n",
        "    response.raise_for_status()\n",
        "    data = response.json()\n",
        "\n",
        "    file_url = None\n",
        "    for file in data.get('files', []):\n",
        "        if file.get('key') == filename_to_download:\n",
        "            file_url = file.get('links', {}).get('self')\n",
        "            break\n",
        "\n",
        "    if file_url:\n",
        "        print(f\"File '{filename_to_download}' found. Starting download...\")\n",
        "\n",
        "        file_response = requests.get(file_url, stream=True)\n",
        "        file_response.raise_for_status()\n",
        "\n",
        "        # Save the file to the current directory\n",
        "        with open(filename_to_download, 'wb') as f:\n",
        "            for chunk in file_response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "\n",
        "        print(f\"File '{filename_to_download}' downloaded successfully.\")\n",
        "    else:\n",
        "        print(f\"File '{filename_to_download}' not found in the repository.\")\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"An error occurred while accessing Zenodo: {e}\")\n",
        "\n",
        "posts = pd.read_excel('vk_posts_balanced.xlsx')\n",
        "\n",
        "# Converting all columns to a string type\n",
        "posts['Text'] = posts['text'].astype(str)\n",
        "\n",
        "# Dictionary for replacing values\n",
        "replacement_dict = {\n",
        "    0: 'optional to nothing (0)',\n",
        "    1: 'binding to discount (1)',\n",
        "    2: 'obliging the group to make a gift (2)',\n",
        "    3: 'obliging the group to give cashback (3)',\n",
        "    4: 'obligating the group to deliver the goods on time (4)'\n",
        "}\n",
        "posts[\"Instruction\"] = \"Generate an advertising post for the food delivery group promising \"\n",
        "posts[\"Instruction\"] = posts[\"Instruction\"] + posts[\"class\"].map(replacement_dict)\n",
        "posts['Instruction'] = posts['Instruction'].astype(str)\n",
        "\n",
        "posts[\"sys_promt\"] = (\"\"\"You are an expert in generating VK group posts in the field of food delivery.\n",
        "You will be given instructions for generating a group post.\n",
        "Your task is to create a group post that follows the instructions.\"\"\"\n",
        ")\n",
        "posts['sys_promt'] = posts['sys_promt'].astype(str)\n",
        "\n",
        "# Replacing NaN values with empty strings\n",
        "posts = posts.fillna('')\n",
        "\n",
        "train = posts\n",
        "train.head(3)\n",
        "\n",
        "from datasets import Dataset\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ Dataset\n",
        "ds = Dataset.from_dict({\n",
        "    \"output\": posts['Text'],\n",
        "    \"input\": posts['Instruction'],\n",
        "    \"instruction\": posts['sys_promt']\n",
        "})\n",
        "\n",
        "ds[0]"
      ],
      "metadata": {
        "id": "0zaRgzt-_t5K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "319f65d2-a99d-4d72-ceb4-f42beefce38d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching for file 'vk_posts_balanced.xlsx' in Zenodo repository with ID: 17054610...\n",
            "File 'vk_posts_balanced.xlsx' found. Starting download...\n",
            "File 'vk_posts_balanced.xlsx' downloaded successfully.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'output': '–†–æ–∑—ã–≥—Ä—ã—à 80 —Ä–æ–ª–ª–æ–≤ - —Å–µ—Ç ONE LOVE \\n–û—Ç —Ç–µ–±—è –Ω—É–∂–µ–Ω —Ç–æ–ª—å–∫–æ –ª–∞–π–∫üí™\\n\\n–î–ª—è —É—á–∞—Å—Ç–∏—è –≤ –∫–æ–Ω–∫—É—Ä—Å–µ: \\n- –ü–æ—Å—Ç–∞–≤—å –ª–∞–π–∫ –Ω–∞ –ø–æ—Å—Ç –∏ —Å—Ç–æ—Ä–∏–∑\\n- –ù–∞–ø–∏—à–∏ –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö —Ç–≤–æ–π –ª—é–±–∏–º—ã–π —Å–µ—Ç \\n\\n–ò—Ç–æ–≥–∏ –ø–æ–¥–≤–µ–¥–µ–º –≤ —Å—Ä–µ–¥—É –≤–µ—á–µ—Ä–æ–º, —Å–ª—É—á–∞–π–Ω—ã–º –æ–±—Ä–∞–∑–æ–º.',\n",
              " 'input': 'Generate an advertising post for the food delivery group promising obliging the group to make a gift (2)',\n",
              " 'instruction': 'You are an expert in generating VK group posts in the field of food delivery. \\nYou will be given instructions for generating a group post. \\nYour task is to create a group post that follows the instructions.'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting a dataset to a suitable format"
      ],
      "metadata": {
        "id": "kkXvBogi6hiQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LjY75GoYUCB8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e58000b16bcb40bd9124ed3c0f4faca7",
            "79ba182e4be04204b8eee91826df902c",
            "a83336a337984514bc210dd3c6f9c0bc",
            "1aab06fe455147799b88876d24d66b25",
            "b3661df93c104a9d91bfed353963d473",
            "801b2c3014cf45f9b7dc9360a669fa7b",
            "d28d305a53154ce59ecf91f7dffce958",
            "88685c2f07e140d2a6a1afca025e5cab",
            "358e03252e9f499485ba6deab5b77f32",
            "a7509ef5de354cc9820a369a3fcc6883",
            "cad1d31ab44547fd8d7ffd87169f7176"
          ]
        },
        "outputId": "f215a3ea-f57e-4948-e348-cbd1859048fc"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = ds\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training\n",
        "Now we will use the `SFTTrainer` from Huggingface TRL! More documentation is here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer ). We do 60 steps for speed, but we can set `num_train_epochs=1` for a full run and disable `max_steps=None'. We also support the `DPOTrainer` from TRL!"
      ],
      "metadata": {
        "id": "idAEIeSQ3xdS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "95_Nn-89DhsL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "860e19ffd2124053bdedfcc4b9c89c1f",
            "7cd52f5c0e484c45a154278e2079ec69",
            "cef1b8e3902f4fc089ddaa1d639625d4",
            "8a5867ecb3824dfcbaba78ce94c2c526",
            "1bd82065272e4ff3b1a713ac9d7ed6fd",
            "7f2de5ee9bc5416598d1bb784a60a7fb",
            "aa742c7e05c64aed9c264cfc59ee0795",
            "c33c3b6001d74deca9d3803711808d29",
            "e3edad2bd6b44f0b8b51d3cdcbc8f63f",
            "21938c36a089427f957ee7e36c0860b4",
            "4f7324f09e9b498cbbb49b3253ee6043"
          ]
        },
        "outputId": "9ade0c98-06f7-469b-d150-4cf5d7a9013b"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "\n",
        "        # Use num_train_epochs = 1, warmup_ratio for full training runs!\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,\n",
        "\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 5,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2ejIt2xSNKKp",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eb4a4c6-c147-472d-ff7c-8095f05093c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "6.725 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "#@title Indicators of available video memory\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCqnaKmlO1U9",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Video memory usage indicators and statistics\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## –ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏"
      ],
      "metadata": {
        "id": "ekOmTR1hSNcr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating posts"
      ],
      "metadata": {
        "id": "U8X6EYaKuCy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "post_result=''\n",
        "for Instruction in posts[\"Instruction\"].unique():\n",
        "    # alpaca_prompt = Copied from above\n",
        "    FastLanguageModel.for_inference(model) # Unsloth has 2x faster inference!\n",
        "    inputs = tokenizer(\n",
        "    [\n",
        "        alpaca_prompt.format(\n",
        "            posts[\"sys_promt\"][0], # instruction\n",
        "            Instruction, # input\n",
        "            \"\", # output - leave this blank for generation!\n",
        "        )\n",
        "    ], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 1000, use_cache = True)\n",
        "    message_answer = tokenizer.batch_decode(outputs)[0].rsplit('Response:', 1)[-1]\n",
        "    match = re.search(r'[012345]', str(Instruction))\n",
        "    post_result+=str(int(match.group()))+' - '+str(message_answer)+'\\n'\n",
        "print(post_result)"
      ],
      "metadata": {
        "id": "CkCJQUJ7s4Jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "post_result=''\n",
        "for Instruction in posts[\"Instruction\"].unique():\n",
        "    # alpaca_prompt = Copied from above\n",
        "    FastLanguageModel.for_inference(model) # Unsloth has 2x faster inference!\n",
        "    inputs = tokenizer(\n",
        "    [\n",
        "        alpaca_prompt.format(\n",
        "            \"Answer in English\\n \"+str(posts[\"sys_promt\"][0]), # instruction\n",
        "            \"Answer in English\\n \"+Instruction, # input\n",
        "            \"\", # output - leave this blank for generation!\n",
        "        )\n",
        "    ], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 1000, use_cache = True)\n",
        "    message_answer = tokenizer.batch_decode(outputs)[0].rsplit('Response:', 1)[-1]\n",
        "    match = re.search(r'[012345]', str(Instruction))\n",
        "    post_result+=str(int(match.group()))+' - '+str(message_answer)+'\\n'\n",
        "print(post_result)"
      ],
      "metadata": {
        "id": "9fwa-j2g_Bdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "my_file = open(f\"{model_name}_post_gen.txt\", \"w+\")\n",
        "my_file.write(post_result)\n",
        "my_file.close()"
      ],
      "metadata": {
        "id": "_eyPf3OIL3AV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}