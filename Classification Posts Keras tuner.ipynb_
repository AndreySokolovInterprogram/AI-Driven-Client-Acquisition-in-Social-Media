{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"15qbq0TgHxzq1Vg4KUW0h9PLAuaoI2Cjy","timestamp":1711644535181},{"file_id":"1OQa2GPDnxMXakLbSGrVfrdGmZHJPKID2","timestamp":1711471928003},{"file_id":"1I3cMOwbJuVM3omKfmSCCAGYaXSNtRY7q","timestamp":1711033279115},{"file_id":"1q9S2o573OS6i1zPrA0VQOLWhhE84qJ9Z","timestamp":1709902691875}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#!pip install tensorflow_text"],"metadata":{"id":"1Q35sHBsxK2L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import requests\n","import nltk\n","import spacy\n","import tensorflow as tf\n","#import tensorflow_text as tf_text\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","import pandas as pd\n","import numpy as np\n","from tensorflow.keras import utils"],"metadata":{"id":"HWUEoReRwSsr"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3mwqd2m3b_A0","colab":{"base_uri":"https://localhost:8080/","height":667},"executionInfo":{"status":"ok","timestamp":1711866952231,"user_tz":-300,"elapsed":23552,"user":{"displayName":"Andrey Sokolov","userId":"00506076773780018870"}},"outputId":"65541c90-38df-47d9-ebad-8f8ec202ce7c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/MyDrive/Colab Notebooks/Neural network Article/Pickle\n"]},{"output_type":"execute_result","data":{"text/plain":["                                                –ü—Ä–æ–µ–∫—Ç  \\\n","0                     –û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è   \n","1                     –û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è   \n","2                     –û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è   \n","3                     –û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è   \n","4                     –û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è   \n","..                                                 ...   \n","282                   –û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è   \n","283  –û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è   \n","284                   –û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è   \n","285                                    –û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è   \n","286                   –û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è   \n","\n","                             –ê–∫–∫–∞—É–Ω—Ç       –î–∞—Ç–∞  \\\n","0               tommyfish\\ntommyfish 2023-02-26   \n","1               tommyfish\\ntommyfish 2023-02-27   \n","2               tommyfish\\ntommyfish 2023-02-28   \n","3               tommyfish\\ntommyfish 2023-03-01   \n","4               tommyfish\\ntommyfish 2023-03-02   \n","..                               ...        ...   \n","282             tommyfish\\ntommyfish 2023-12-26   \n","283  tommyfish\\ntommyfish\\ntommyfish 2023-12-27   \n","284             tommyfish\\ntommyfish 2023-12-28   \n","285                        tommyfish 2023-12-30   \n","286             tommyfish\\ntommyfish 2023-12-31   \n","\n","                                 –¢–∏–ø  \\\n","0           –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ   \n","1                 –í–∏–¥–µ–æ\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ   \n","2           –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ   \n","3           –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ   \n","4           –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ   \n","..                               ...   \n","282         –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ   \n","283  –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\\n–¢–µ–∫—Å—Ç   \n","284         –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ   \n","285                            –¢–µ–∫—Å—Ç   \n","286         –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ   \n","\n","                                                –°—Å—ã–ª–∫–∞  \\\n","0    https://vk.com/wall-171840666_168847\\nhttps://...   \n","1    https://vk.com/wall-171840666_169542\\nhttps://...   \n","2    https://vk.com/wall-171840666_170560\\nhttps://...   \n","3    https://vk.com/wall-171840666_171068\\nhttps://...   \n","4    https://vk.com/wall-171840666_171847\\nhttps://...   \n","..                                                 ...   \n","282  https://vk.com/wall-171840666_287124\\nhttps://...   \n","283  https://vk.com/wall-171840666_287403\\nhttps://...   \n","284  https://vk.com/wall-171840666_287651\\nhttps://...   \n","285               https://vk.com/wall-171840666_287948   \n","286  https://vk.com/wall-171840666_288482\\nhttps://...   \n","\n","                                                 –¢–µ–∫—Å—Ç  –õ–∞–π–∫–æ–≤  –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤  \\\n","0    –°–∫–æ—Ä–æ –î–µ–Ω—å —Ä–æ–∂–¥–µ–Ω–∏—è? \\n–ê –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–Ω —É–∂–µ —Å–µ–≥...      13             0   \n","1    –ü–æ–¥–≤–æ–¥–∏–º –∏—Ç–æ–≥–∏ –∫–æ–Ω–∫—É—Ä—Å–∞ —Ñ–æ—Ç–æ-–æ—Ç–∑—ã–≤–æ–≤üì∏ –ü–æ–∑–¥—Ä–∞–≤–ª...       7             0   \n","2    nan\\n–°–∫–æ—Ä–æ –î–µ–Ω—å —Ä–æ–∂–¥–µ–Ω–∏—è? \\n–ê –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–Ω —É–∂...      24             0   \n","3    –£ –Ω–∞—Å –µ—Å—Ç—å —Å–µ—Ç—ã –ª—é–±–æ–π —Å–ª—É—á–∞–π! üòå \\n‚Äì –ù–∞ –±–æ–ª—å—à—É—é...      14             0   \n","4    –¢–∞–∫ —Ä–µ–±—è—Ç, —Å–µ–≥–æ–¥–Ω—è –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–µ–Ω—å –¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–æ...      13             2   \n","..                                                 ...     ...           ...   \n","282  –î–æ –ù–æ–≤–æ–≥–æ –≥–æ–¥–∞ –æ—Å—Ç–∞–ª–æ—Å—å –≤—Å–µ–≥–æ 6 –¥–Ω–µ–π! üéÑ \\n \\n–ö...      13             0   \n","283  –ê –í–´ –ë–£–î–ï–¢–ï –†–ê–ë–û–¢–ê–¢–¨ –í –ü–†–ê–ó–î–ù–ò–ö–ò? üéÑ \\n \\n–ß–µ–º –±...       6             0   \n","284  –ò —Ä–æ–ª–ª—ãüòç\\n–ë–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö —Ä–æ–ª–ª–æ–≤ –º–Ω–æ–≥–æ –Ω–µ –±—ã–≤–∞–µ—Ç!\\n...      15             0   \n","285  –î–æ –ù–æ–≤–æ–≥–æ –≥–æ–¥–∞ 2 –¥–Ω—è!\\n–û–∂–∏–¥–∞–Ω–∏–µ –Ω–∞ —Ç–µ–∫—É—â–∏–µ –∑–∞–∫...       8             0   \n","286  –î–†–£–ó–¨–Ø, –ü–û–ó–î–†–ê–í–õ–Ø–ï–ú –° –ù–ê–°–¢–£–ü–ê–Æ–©–ò–ú! \\n–ë—É–¥—å—Ç–µ —Å—á...      30             2   \n","\n","     –†–µ–ø–æ—Å—Ç–æ–≤  –ü—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ –ö–ª–∞—Å—Å –ø–æ—Å—Ç–∞  –ü—Ä–∏—Ä–æ—Å—Ç  \n","0           3        3006           2      569  \n","1           0        3396           0      438  \n","2           3        4211           2      528  \n","3           0        3759           0      449  \n","4           1        3036           2      421  \n","..        ...         ...         ...      ...  \n","282         2        3781           0        0  \n","283         4        4195           0       73  \n","284         0       11370           2        0  \n","285         4       15770           0       87  \n","286         0       27214           0        0  \n","\n","[287 rows x 12 columns]"],"text/html":["\n","  <div id=\"df-ad273415-f68c-412f-adc9-f5683999b9f3\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>–ü—Ä–æ–µ–∫—Ç</th>\n","      <th>–ê–∫–∫–∞—É–Ω—Ç</th>\n","      <th>–î–∞—Ç–∞</th>\n","      <th>–¢–∏–ø</th>\n","      <th>–°—Å—ã–ª–∫–∞</th>\n","      <th>–¢–µ–∫—Å—Ç</th>\n","      <th>–õ–∞–π–∫–æ–≤</th>\n","      <th>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤</th>\n","      <th>–†–µ–ø–æ—Å—Ç–æ–≤</th>\n","      <th>–ü—Ä–æ—Å–º–æ—Ç—Ä–æ–≤</th>\n","      <th>–ö–ª–∞—Å—Å –ø–æ—Å—Ç–∞</th>\n","      <th>–ü—Ä–∏—Ä–æ—Å—Ç</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è</td>\n","      <td>tommyfish\\ntommyfish</td>\n","      <td>2023-02-26</td>\n","      <td>–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ</td>\n","      <td>https://vk.com/wall-171840666_168847\\nhttps://...</td>\n","      <td>–°–∫–æ—Ä–æ –î–µ–Ω—å —Ä–æ–∂–¥–µ–Ω–∏—è? \\n–ê –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–Ω —É–∂–µ —Å–µ–≥...</td>\n","      <td>13</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>3006</td>\n","      <td>2</td>\n","      <td>569</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è</td>\n","      <td>tommyfish\\ntommyfish</td>\n","      <td>2023-02-27</td>\n","      <td>–í–∏–¥–µ–æ\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ</td>\n","      <td>https://vk.com/wall-171840666_169542\\nhttps://...</td>\n","      <td>–ü–æ–¥–≤–æ–¥–∏–º –∏—Ç–æ–≥–∏ –∫–æ–Ω–∫—É—Ä—Å–∞ —Ñ–æ—Ç–æ-–æ—Ç–∑—ã–≤–æ–≤üì∏ –ü–æ–∑–¥—Ä–∞–≤–ª...</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3396</td>\n","      <td>0</td>\n","      <td>438</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è</td>\n","      <td>tommyfish\\ntommyfish</td>\n","      <td>2023-02-28</td>\n","      <td>–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ</td>\n","      <td>https://vk.com/wall-171840666_170560\\nhttps://...</td>\n","      <td>nan\\n–°–∫–æ—Ä–æ –î–µ–Ω—å —Ä–æ–∂–¥–µ–Ω–∏—è? \\n–ê –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–Ω —É–∂...</td>\n","      <td>24</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>4211</td>\n","      <td>2</td>\n","      <td>528</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è</td>\n","      <td>tommyfish\\ntommyfish</td>\n","      <td>2023-03-01</td>\n","      <td>–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ</td>\n","      <td>https://vk.com/wall-171840666_171068\\nhttps://...</td>\n","      <td>–£ –Ω–∞—Å –µ—Å—Ç—å —Å–µ—Ç—ã –ª—é–±–æ–π —Å–ª—É—á–∞–π! üòå \\n‚Äì –ù–∞ –±–æ–ª—å—à—É—é...</td>\n","      <td>14</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3759</td>\n","      <td>0</td>\n","      <td>449</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è</td>\n","      <td>tommyfish\\ntommyfish</td>\n","      <td>2023-03-02</td>\n","      <td>–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ</td>\n","      <td>https://vk.com/wall-171840666_171847\\nhttps://...</td>\n","      <td>–¢–∞–∫ —Ä–µ–±—è—Ç, —Å–µ–≥–æ–¥–Ω—è –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–µ–Ω—å –¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–æ...</td>\n","      <td>13</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>3036</td>\n","      <td>2</td>\n","      <td>421</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>282</th>\n","      <td>–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è</td>\n","      <td>tommyfish\\ntommyfish</td>\n","      <td>2023-12-26</td>\n","      <td>–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ</td>\n","      <td>https://vk.com/wall-171840666_287124\\nhttps://...</td>\n","      <td>–î–æ –ù–æ–≤–æ–≥–æ –≥–æ–¥–∞ –æ—Å—Ç–∞–ª–æ—Å—å –≤—Å–µ–≥–æ 6 –¥–Ω–µ–π! üéÑ \\n \\n–ö...</td>\n","      <td>13</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>3781</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>283</th>\n","      <td>–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è</td>\n","      <td>tommyfish\\ntommyfish\\ntommyfish</td>\n","      <td>2023-12-27</td>\n","      <td>–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\\n–¢–µ–∫—Å—Ç</td>\n","      <td>https://vk.com/wall-171840666_287403\\nhttps://...</td>\n","      <td>–ê –í–´ –ë–£–î–ï–¢–ï –†–ê–ë–û–¢–ê–¢–¨ –í –ü–†–ê–ó–î–ù–ò–ö–ò? üéÑ \\n \\n–ß–µ–º –±...</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>4195</td>\n","      <td>0</td>\n","      <td>73</td>\n","    </tr>\n","    <tr>\n","      <th>284</th>\n","      <td>–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è</td>\n","      <td>tommyfish\\ntommyfish</td>\n","      <td>2023-12-28</td>\n","      <td>–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ</td>\n","      <td>https://vk.com/wall-171840666_287651\\nhttps://...</td>\n","      <td>–ò —Ä–æ–ª–ª—ãüòç\\n–ë–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö —Ä–æ–ª–ª–æ–≤ –º–Ω–æ–≥–æ –Ω–µ –±—ã–≤–∞–µ—Ç!\\n...</td>\n","      <td>15</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>11370</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>285</th>\n","      <td>–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è</td>\n","      <td>tommyfish</td>\n","      <td>2023-12-30</td>\n","      <td>–¢–µ–∫—Å—Ç</td>\n","      <td>https://vk.com/wall-171840666_287948</td>\n","      <td>–î–æ –ù–æ–≤–æ–≥–æ –≥–æ–¥–∞ 2 –¥–Ω—è!\\n–û–∂–∏–¥–∞–Ω–∏–µ –Ω–∞ —Ç–µ–∫—É—â–∏–µ –∑–∞–∫...</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>15770</td>\n","      <td>0</td>\n","      <td>87</td>\n","    </tr>\n","    <tr>\n","      <th>286</th>\n","      <td>–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è</td>\n","      <td>tommyfish\\ntommyfish</td>\n","      <td>2023-12-31</td>\n","      <td>–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ</td>\n","      <td>https://vk.com/wall-171840666_288482\\nhttps://...</td>\n","      <td>–î–†–£–ó–¨–Ø, –ü–û–ó–î–†–ê–í–õ–Ø–ï–ú –° –ù–ê–°–¢–£–ü–ê–Æ–©–ò–ú! \\n–ë—É–¥—å—Ç–µ —Å—á...</td>\n","      <td>30</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>27214</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>287 rows √ó 12 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ad273415-f68c-412f-adc9-f5683999b9f3')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-ad273415-f68c-412f-adc9-f5683999b9f3 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-ad273415-f68c-412f-adc9-f5683999b9f3');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-57d2ad16-c2b9-4b51-8980-ac6773db7ea3\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-57d2ad16-c2b9-4b51-8980-ac6773db7ea3')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-57d2ad16-c2b9-4b51-8980-ac6773db7ea3 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"posts","summary":"{\n  \"name\": \"posts\",\n  \"rows\": 287,\n  \"fields\": [\n    {\n      \"column\": \"\\u041f\\u0440\\u043e\\u0435\\u043a\\u0442\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"\\u041e\\u0431\\u0449\\u0430\\u044f \\u043a\\u0430\\u0442\\u0435\\u0433\\u043e\\u0440\\u0438\\u044f\\n\\u041e\\u0431\\u0449\\u0430\\u044f \\u043a\\u0430\\u0442\\u0435\\u0433\\u043e\\u0440\\u0438\\u044f\\n\\u041e\\u0431\\u0449\\u0430\\u044f \\u043a\\u0430\\u0442\\u0435\\u0433\\u043e\\u0440\\u0438\\u044f\",\n          \"\\u041e\\u0431\\u0449\\u0430\\u044f \\u043a\\u0430\\u0442\\u0435\\u0433\\u043e\\u0440\\u0438\\u044f\\n\\u041e\\u0431\\u0449\\u0430\\u044f \\u043a\\u0430\\u0442\\u0435\\u0433\\u043e\\u0440\\u0438\\u044f\\n\\u041e\\u0431\\u0449\\u0430\\u044f \\u043a\\u0430\\u0442\\u0435\\u0433\\u043e\\u0440\\u0438\\u044f\\n\\u041e\\u0431\\u0449\\u0430\\u044f \\u043a\\u0430\\u0442\\u0435\\u0433\\u043e\\u0440\\u0438\\u044f\\n\\u041e\\u0431\\u0449\\u0430\\u044f \\u043a\\u0430\\u0442\\u0435\\u0433\\u043e\\u0440\\u0438\\u044f\",\n          \"\\u041e\\u0431\\u0449\\u0430\\u044f \\u043a\\u0430\\u0442\\u0435\\u0433\\u043e\\u0440\\u0438\\u044f\\n\\u041e\\u0431\\u0449\\u0430\\u044f \\u043a\\u0430\\u0442\\u0435\\u0433\\u043e\\u0440\\u0438\\u044f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u0410\\u043a\\u043a\\u0430\\u0443\\u043d\\u0442\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"tommyfish\\ntommyfish\\ntommyfish\",\n          \"tommyfish\\ntommyfish\\ntommyfish\\ntommyfish\\ntommyfish\",\n          \"tommyfish\\ntommyfish\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u0414\\u0430\\u0442\\u0430\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2023-02-26 00:00:00\",\n        \"max\": \"2023-12-31 00:00:00\",\n        \"num_unique_values\": 287,\n        \"samples\": [\n          \"2023-03-07 00:00:00\",\n          \"2023-11-29 00:00:00\",\n          \"2023-07-27 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u0422\\u0438\\u043f\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"\\u0418\\u0437\\u043e\\u0431\\u0440\\u0430\\u0436\\u0435\\u043d\\u0438\\u0435\\n\\u0418\\u0437\\u043e\\u0431\\u0440\\u0430\\u0436\\u0435\\u043d\\u0438\\u0435\",\n          \"\\u0418\\u0437\\u043e\\u0431\\u0440\\u0430\\u0436\\u0435\\u043d\\u0438\\u0435\\n\\u0418\\u0437\\u043e\\u0431\\u0440\\u0430\\u0436\\u0435\\u043d\\u0438\\u0435\\n\\u0418\\u0437\\u043e\\u0431\\u0440\\u0430\\u0436\\u0435\\u043d\\u0438\\u0435\\n\\u0418\\u0437\\u043e\\u0431\\u0440\\u0430\\u0436\\u0435\\u043d\\u0438\\u0435\\n\\u0418\\u0437\\u043e\\u0431\\u0440\\u0430\\u0436\\u0435\\u043d\\u0438\\u0435\",\n          \"\\u0412\\u0438\\u0434\\u0435\\u043e\\n\\u0412\\u0438\\u0434\\u0435\\u043e\\n\\u0418\\u0437\\u043e\\u0431\\u0440\\u0430\\u0436\\u0435\\u043d\\u0438\\u0435\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u0421\\u0441\\u044b\\u043b\\u043a\\u0430\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 287,\n        \"samples\": [\n          \"https://vk.com/wall-171840666_175921\",\n          \"https://vk.com/wall-171840666_279217\",\n          \"https://vk.com/wall-171840666_233333\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u0422\\u0435\\u043a\\u0441\\u0442\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 237,\n        \"samples\": [\n          \"\\ud83e\\udd74\\n\\u0411\\u0435\\u0441\\u043f\\u043b\\u0430\\u0442\\u043d\\u044b\\u0445 \\u0440\\u043e\\u043b\\u043b\\u043e\\u0432 \\u043c\\u043d\\u043e\\u0433\\u043e \\u043d\\u0435 \\u0431\\u044b\\u0432\\u0430\\u0435\\u0442!\\n\\u0414\\u0430\\u0440\\u0438\\u043c \\u043d\\u0430\\u0448\\u0438\\u043c \\u0438\\u043c\\u0435\\u043d\\u0438\\u043d\\u043d\\u0438\\u043a\\u0430\\u043c \\u041c\\u0438\\u043d\\u0438 \\u0421\\u0435\\u0442 \\u043f\\u0440\\u0438 \\u0437\\u0430\\u043a\\u0430\\u0437\\u0435 \\u043e\\u0442 1200 \\u0440\\u0443\\u0431\\u043b\\u0435\\u0439.\\n\\n\\u2705\\u0421\\u0435\\u043a\\u0440\\u0435\\u0442\\u043d\\u044b\\u0439 \\u043f\\u0440\\u043e\\u043c\\u043e\\u043a\\u043e\\u0434: \\u0414\\u0420\\n\\ud83d\\udcac\\u0410\\u043a\\u0446\\u0438\\u044f \\u0434\\u0435\\u0439\\u0441\\u0442\\u0432\\u0443\\u0435\\u0442 3 \\u0434\\u043d\\u044f \\u0434\\u043e \\u0438 3 \\u0434\\u043d\\u044f \\u043f\\u043e\\u0441\\u043b\\u0435 \\u0441\\u043e\\u0431\\u044b\\u0442\\u0438\\u044f. \\ud83c\\udf63\\n\\u26a1\\ufe0f\\u0421\\u043a\\u0430\\u0447\\u0430\\u0439 \\u043f\\u0440\\u0438\\u043b\\u043e\\u0436\\u0435\\u043d\\u0438\\u0435 \\u043f\\u043e \\u0441\\u0441\\u044b\\u043b\\u043a\\u0435 \\u0438 \\u0441\\u0434\\u0435\\u043b\\u0430\\u0439 \\u0441\\u0432\\u043e\\u0439 \\u0437\\u0430\\u043a\\u0430\\u0437: vk.cc/cn7BHM\",\n          \"\\ud83d\\ude05\\n\\u0421\\u043a\\u043e\\u0440\\u043e \\u0414\\u0435\\u043d\\u044c \\u0440\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u044f? \\n\\u0410 \\u043c\\u043e\\u0436\\u0435\\u0442 \\u0431\\u044b\\u0442\\u044c \\u043e\\u043d \\u0443\\u0436\\u0435 \\u0441\\u0435\\u0433\\u043e\\u0434\\u043d\\u044f?\\ud83d\\ude3b \\n \\n\\u041e\\u0442\\u043c\\u0435\\u0442\\u044c\\u0442\\u0435 \\u044d\\u0442\\u043e\\u0442 \\u0437\\u0430\\u043c\\u0435\\u0447\\u0430\\u0442\\u0435\\u043b\\u044c\\u043d\\u044b\\u0439 \\u043f\\u0440\\u0430\\u0437\\u0434\\u043d\\u0438\\u043a \\u0432 \\u043a\\u0440\\u0443\\u0433\\u0443 \\u0440\\u043e\\u0434\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u0438\\u043a\\u043e\\u0432 \\u0438 \\u0431\\u043b\\u0438\\u0437\\u043a\\u0438\\u0445! \\u0410 \\u043e \\u043f\\u043e\\u0434\\u0433\\u043e\\u0442\\u043e\\u0432\\u043a\\u0435 \\u0432\\u043a\\u0443\\u0441\\u043d\\u043e\\u0433\\u043e \\u0437\\u0430\\u0441\\u0442\\u043e\\u043b\\u044c\\u044f \\u043f\\u043e\\u0437\\u0430\\u0431\\u043e\\u0442\\u0438\\u043c\\u0441\\u044f \\u043c\\u044b\\u2764 \\n \\n\\u0414\\u0430\\u0440\\u0438\\u043c \\u0432\\u0441\\u0435\\u043c \\u0438\\u043c\\u0435\\u043d\\u0438\\u043d\\u043d\\u0438\\u043a\\u0430\\u043c \\u041c\\u0438\\u043d\\u0438 \\u0421\\u0435\\u0442! \\u041f\\u0440\\u0438 \\u043c\\u0438\\u043d\\u0438\\u043c\\u0430\\u043b\\u044c\\u043d\\u043e\\u043c \\u0437\\u0430\\u043a\\u0430\\u0437\\u0435 \\u043e\\u0442 1200 \\u0440\\u0443\\u0431\\u043b\\u0435\\u0439\\ud83c\\udf81 \\n \\n\\ud83c\\udf71\\u0427\\u0442\\u043e \\u0432\\u0445\\u043e\\u0434\\u0438\\u0442 \\u0432 \\u0441\\u043e\\u0441\\u0442\\u0430\\u0432 \\u0441\\u0435\\u0442\\u0430? \\n- \\u0424\\u0438\\u043b\\u0430\\u0434\\u0435\\u043b\\u044c\\u0444\\u0438\\u044f \\u043b\\u0430\\u0439\\u0442 \\u0441 \\u043a\\u0440\\u0435\\u0432\\u0435\\u0442\\u043a\\u043e\\u0439\\n- \\u0416\\u0430\\u0440\\u0435\\u043d\\u043d\\u044b\\u0439 \\u0440\\u043e\\u043b\\u043b \\u0441 \\u043a\\u0443\\u0440\\u043e\\u0447\\u043a\\u043e\\u0439\\n\\u041f\\u0440\\u043e\\u043c\\u043e\\u043a\\u043e\\u0434: \\u0414\\u0420\\n\\n**\\u0410\\u043a\\u0446\\u0438\\u044f \\u0434\\u0435\\u0439\\u0441\\u0442\\u0432\\u0443\\u0435\\u0442 3 \\u0434\\u043d\\u044f \\u0434\\u043e \\u0438 3 \\u0434\\u043d\\u044f \\u043f\\u043e\\u0441\\u043b\\u0435 \\u0441\\u043e\\u0431\\u044b\\u0442\\u0438\\u044f\",\n          \"\\u0427\\u0442\\u043e\\u0431\\u044b \\u043d\\u0435 \\u0431\\u044b\\u043b\\u043e \\u0433\\u0440\\u0443\\u0441\\u0442\\u043d\\u043e, \\u0434\\u043d\\u0438 \\u0438 \\u0432\\u0435\\u0447\\u0435\\u0440\\u0430 \\u043d\\u0443\\u0436\\u043d\\u043e \\u0440\\u0430\\u0437\\u0431\\u0430\\u0432\\u043b\\u044f\\u0442\\u044c \\u044f\\u0440\\u043a\\u0438\\u043c\\u0438 \\u043a\\u0440\\u0430\\u0441\\u043a\\u0430\\u043c\\u0438 \\u0438 \\u0432\\u043a\\u0443\\u0441\\u043d\\u043e\\u0439 \\u0435\\u0434\\u043e\\u0439 \\ud83d\\ude0b\\ud83d\\udc4d\\ud83c\\udffb\\n\\u0410 \\u043c\\u044b \\u0441 \\u0440\\u0430\\u0434\\u043e\\u0441\\u0442\\u044c\\u044e \\u0432\\u0430\\u043c \\u0432 \\u044d\\u0442\\u043e\\u043c \\u043f\\u043e\\u043c\\u043e\\u0436\\u0435\\u043c \\u2764\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u041b\\u0430\\u0439\\u043a\\u043e\\u0432\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 237,\n        \"min\": 4,\n        \"max\": 2765,\n        \"num_unique_values\": 68,\n        \"samples\": [\n          50,\n          57,\n          8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u041a\\u043e\\u043c\\u043c\\u0435\\u043d\\u0442\\u0430\\u0440\\u0438\\u0435\\u0432\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 150,\n        \"min\": 0,\n        \"max\": 2447,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          0,\n          100,\n          20\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u0420\\u0435\\u043f\\u043e\\u0441\\u0442\\u043e\\u0432\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 50,\n        \"min\": 0,\n        \"max\": 677,\n        \"num_unique_values\": 49,\n        \"samples\": [\n          63,\n          69,\n          72\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u041f\\u0440\\u043e\\u0441\\u043c\\u043e\\u0442\\u0440\\u043e\\u0432\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 20683,\n        \"min\": 1883,\n        \"max\": 277647,\n        \"num_unique_values\": 281,\n        \"samples\": [\n          11103,\n          7498,\n          4181\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u041a\\u043b\\u0430\\u0441\\u0441 \\u043f\\u043e\\u0441\\u0442\\u0430\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0,\n          4,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u041f\\u0440\\u0438\\u0440\\u043e\\u0441\\u0442\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 200,\n        \"min\": 0,\n        \"max\": 3064,\n        \"num_unique_values\": 109,\n        \"samples\": [\n          3064,\n          117,\n          421\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":3}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd '/content/gdrive/MyDrive/Colab Notebooks/Neural network Article/Pickle'\n","\n","posts = pd.read_excel('–î–∞—Ç–∞_—Å–µ—Ç_–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é_—Ç–æ–º–∏_—Ñ–∏—à.xlsx')\n","posts.drop(posts.index[posts['–ö–ª–∞—Å—Å –ø–æ—Å—Ç–∞']=='1,2,4'], inplace = True)\n","posts.reset_index(drop=True, inplace=True)\n","posts"]},{"cell_type":"code","source":["nb_classes = 5\n","posts.dropna()\n","posts['–¢–µ–∫—Å—Ç'] = posts['–¢–µ–∫—Å—Ç'].astype(str)"],"metadata":{"id":"0YaX-aE64soj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ–±—É—á–∞—é—â–µ–µ –∏ —Ç–µ—Å—Ç–æ–≤–æ–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞"],"metadata":{"id":"deBhvZF6Bufd"}},{"cell_type":"code","source":["#–≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–µ–ª–µ–Ω–∏—è –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–æ–≤\n","def split_dataframe(dataframe, test_proportion):\n","    total_size = len(dataframe)\n","    test_size = int(total_size * test_proportion)\n","    indices = np.arange(total_size)\n","    np.random.shuffle(indices)\n","    train_indices = indices[0:total_size-test_size]\n","    test_indices = indices[total_size - test_size:]\n","    return dataframe.iloc[train_indices], dataframe.iloc[test_indices]"],"metadata":{"id":"DQkeeM4u-vxh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train, test = split_dataframe(posts, 0.3)"],"metadata":{"id":"lGT0hKKgAX1W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train"],"metadata":{"id":"gVuwQZelLlYd","colab":{"base_uri":"https://localhost:8080/","height":719},"executionInfo":{"status":"ok","timestamp":1711866952231,"user_tz":-300,"elapsed":85,"user":{"displayName":"Andrey Sokolov","userId":"00506076773780018870"}},"outputId":"84f5ab5b-79c2-44fd-fa01-a7f302fdd345"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                –ü—Ä–æ–µ–∫—Ç  \\\n","199                   –û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è   \n","36                    –û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è   \n","160                                    –û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è   \n","263                                    –û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è   \n","186                   –û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è   \n","..                                                 ...   \n","64   –û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è   \n","203                                    –û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è   \n","281                   –û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è   \n","82                                     –û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è   \n","122                                    –û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è   \n","\n","                             –ê–∫–∫–∞—É–Ω—Ç       –î–∞—Ç–∞  \\\n","199             tommyfish\\ntommyfish 2023-09-29   \n","36              tommyfish\\ntommyfish 2023-04-03   \n","160                        tommyfish 2023-08-13   \n","263                        tommyfish 2023-12-07   \n","186             tommyfish\\ntommyfish 2023-09-11   \n","..                               ...        ...   \n","64   tommyfish\\ntommyfish\\ntommyfish 2023-05-02   \n","203                        tommyfish 2023-10-04   \n","281             tommyfish\\ntommyfish 2023-12-25   \n","82                         tommyfish 2023-05-20   \n","122                        tommyfish 2023-07-02   \n","\n","                                 –¢–∏–ø  \\\n","199         –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ   \n","36          –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ   \n","160                      –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ   \n","263                      –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ   \n","186               –í–∏–¥–µ–æ\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ   \n","..                               ...   \n","64   –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\\n–û–ø—Ä–æ—Å\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ   \n","203                      –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ   \n","281         –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ   \n","82                             –û–ø—Ä–æ—Å   \n","122                      –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ   \n","\n","                                                –°—Å—ã–ª–∫–∞  \\\n","199  https://vk.com/wall-171840666_259727\\nhttps://...   \n","36   https://vk.com/wall-171840666_186736\\nhttps://...   \n","160               https://vk.com/wall-171840666_243213   \n","263               https://vk.com/wall-171840666_281660   \n","186  https://vk.com/wall-171840666_254105\\nhttps://...   \n","..                                                 ...   \n","64   https://vk.com/wall-171840666_196206\\nhttps://...   \n","203               https://vk.com/wall-171840666_261593   \n","281  https://vk.com/wall-171840666_286874\\nhttps://...   \n","82                https://vk.com/wall-171840666_201855   \n","122               https://vk.com/wall-171840666_218925   \n","\n","                                                 –¢–µ–∫—Å—Ç  –õ–∞–π–∫–æ–≤  –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤  \\\n","199  –í–∫—É—Å–Ω–æ...#–æ—Ç–∑—ã–≤—ã@tommyfish\\n–ë–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö —Ä–æ–ª–ª–æ–≤ ...      12             1   \n","36   –î—Ä—É–∑—å—è, –∑–∞ –ø—Ä–æ—à–ª—É—é –Ω–µ–¥–µ–ª—é —Å–æ–±—Ä–∞–ª–æ—Å—å –º–µ–Ω–µ–µ 30 —Ñ...       8             0   \n","160  –ó–∞–∫–∞–∑—ã–≤–∞–π—Ç–µ –¥–æ—Å—Ç–∞–≤–∫—É –∏ –≤–∫–ª—é—á–∞–π—Ç–µ —Å–µ—Ä–∏–∞–ª –∏–∑ –Ω–∞—à...      16             0   \n","263  –°–∫–æ—Ä–æ –î–µ–Ω—å —Ä–æ–∂–¥–µ–Ω–∏—è? \\n–ê –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–Ω —É–∂–µ —Å–µ–≥...      10             0   \n","186  –ü–æ–¥–≤–æ–¥–∏–º –∏—Ç–æ–≥–∏ –∫–æ–Ω–∫—É—Ä—Å–∞ —Ñ–æ—Ç–æ-–æ—Ç–∑—ã–≤–æ–≤üì∏ –ü–æ–∑–¥—Ä–∞–≤–ª...       7             1   \n","..                                                 ...     ...           ...   \n","64   nan\\n- –û—Ç—á–µ—Ç –∑–∞ 01.05.2023 ‚úÖ\\n\\n–ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –ø—Ä...      48             0   \n","203                                                nan      43             2   \n","281  –î—Ä—É–∑—å—è, –∑–∞ –ø—Ä–æ—à–ª—É—é –Ω–µ–¥–µ–ª—é —Å–æ–±—Ä–∞–ª–æ—Å—å –º–µ–Ω–µ–µ 30 —Ñ...       9             0   \n","82   –í–Ω–∏–º–∞–Ω–∏–µ, –ª—é–±–∏—Ç–µ–ª–∏ —Å—É—à–∏!\\n\\n–î–∞–≤–∞–π—Ç–µ –≤—ã–±–µ—Ä–µ–º –ª—é...       6             0   \n","122  –ë–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö —Ä–æ–ª–ª–æ–≤ –º–Ω–æ–≥–æ –Ω–µ –±—ã–≤–∞–µ—Ç!\\n–î–∞—Ä–∏–º –Ω–∞—à–∏...       9             0   \n","\n","     –†–µ–ø–æ—Å—Ç–æ–≤  –ü—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ –ö–ª–∞—Å—Å –ø–æ—Å—Ç–∞  –ü—Ä–∏—Ä–æ—Å—Ç  \n","199         0        6274           2        0  \n","36          1        5708           2       48  \n","160         1        7498           0       41  \n","263         4        2683           2        0  \n","186         0        3013           2        0  \n","..        ...         ...         ...      ...  \n","64         65       11006           0       25  \n","203        20       15586           0      206  \n","281         0        3255           2        0  \n","82          2        1984           0       47  \n","122         2        4161           2        0  \n","\n","[201 rows x 12 columns]"],"text/html":["\n","  <div id=\"df-f5226093-2635-4f75-b10f-0a12ebbbea2d\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>–ü—Ä–æ–µ–∫—Ç</th>\n","      <th>–ê–∫–∫–∞—É–Ω—Ç</th>\n","      <th>–î–∞—Ç–∞</th>\n","      <th>–¢–∏–ø</th>\n","      <th>–°—Å—ã–ª–∫–∞</th>\n","      <th>–¢–µ–∫—Å—Ç</th>\n","      <th>–õ–∞–π–∫–æ–≤</th>\n","      <th>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤</th>\n","      <th>–†–µ–ø–æ—Å—Ç–æ–≤</th>\n","      <th>–ü—Ä–æ—Å–º–æ—Ç—Ä–æ–≤</th>\n","      <th>–ö–ª–∞—Å—Å –ø–æ—Å—Ç–∞</th>\n","      <th>–ü—Ä–∏—Ä–æ—Å—Ç</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>199</th>\n","      <td>–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è</td>\n","      <td>tommyfish\\ntommyfish</td>\n","      <td>2023-09-29</td>\n","      <td>–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ</td>\n","      <td>https://vk.com/wall-171840666_259727\\nhttps://...</td>\n","      <td>–í–∫—É—Å–Ω–æ...#–æ—Ç–∑—ã–≤—ã@tommyfish\\n–ë–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö —Ä–æ–ª–ª–æ–≤ ...</td>\n","      <td>12</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>6274</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è</td>\n","      <td>tommyfish\\ntommyfish</td>\n","      <td>2023-04-03</td>\n","      <td>–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ</td>\n","      <td>https://vk.com/wall-171840666_186736\\nhttps://...</td>\n","      <td>–î—Ä—É–∑—å—è, –∑–∞ –ø—Ä–æ—à–ª—É—é –Ω–µ–¥–µ–ª—é —Å–æ–±—Ä–∞–ª–æ—Å—å –º–µ–Ω–µ–µ 30 —Ñ...</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>5708</td>\n","      <td>2</td>\n","      <td>48</td>\n","    </tr>\n","    <tr>\n","      <th>160</th>\n","      <td>–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è</td>\n","      <td>tommyfish</td>\n","      <td>2023-08-13</td>\n","      <td>–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ</td>\n","      <td>https://vk.com/wall-171840666_243213</td>\n","      <td>–ó–∞–∫–∞–∑—ã–≤–∞–π—Ç–µ –¥–æ—Å—Ç–∞–≤–∫—É –∏ –≤–∫–ª—é—á–∞–π—Ç–µ —Å–µ—Ä–∏–∞–ª –∏–∑ –Ω–∞—à...</td>\n","      <td>16</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>7498</td>\n","      <td>0</td>\n","      <td>41</td>\n","    </tr>\n","    <tr>\n","      <th>263</th>\n","      <td>–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è</td>\n","      <td>tommyfish</td>\n","      <td>2023-12-07</td>\n","      <td>–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ</td>\n","      <td>https://vk.com/wall-171840666_281660</td>\n","      <td>–°–∫–æ—Ä–æ –î–µ–Ω—å —Ä–æ–∂–¥–µ–Ω–∏—è? \\n–ê –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–Ω —É–∂–µ —Å–µ–≥...</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>2683</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>186</th>\n","      <td>–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è</td>\n","      <td>tommyfish\\ntommyfish</td>\n","      <td>2023-09-11</td>\n","      <td>–í–∏–¥–µ–æ\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ</td>\n","      <td>https://vk.com/wall-171840666_254105\\nhttps://...</td>\n","      <td>–ü–æ–¥–≤–æ–¥–∏–º –∏—Ç–æ–≥–∏ –∫–æ–Ω–∫—É—Ä—Å–∞ —Ñ–æ—Ç–æ-–æ—Ç–∑—ã–≤–æ–≤üì∏ –ü–æ–∑–¥—Ä–∞–≤–ª...</td>\n","      <td>7</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3013</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>64</th>\n","      <td>–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è</td>\n","      <td>tommyfish\\ntommyfish\\ntommyfish</td>\n","      <td>2023-05-02</td>\n","      <td>–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\\n–û–ø—Ä–æ—Å\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ</td>\n","      <td>https://vk.com/wall-171840666_196206\\nhttps://...</td>\n","      <td>nan\\n- –û—Ç—á–µ—Ç –∑–∞ 01.05.2023 ‚úÖ\\n\\n–ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –ø—Ä...</td>\n","      <td>48</td>\n","      <td>0</td>\n","      <td>65</td>\n","      <td>11006</td>\n","      <td>0</td>\n","      <td>25</td>\n","    </tr>\n","    <tr>\n","      <th>203</th>\n","      <td>–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è</td>\n","      <td>tommyfish</td>\n","      <td>2023-10-04</td>\n","      <td>–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ</td>\n","      <td>https://vk.com/wall-171840666_261593</td>\n","      <td>nan</td>\n","      <td>43</td>\n","      <td>2</td>\n","      <td>20</td>\n","      <td>15586</td>\n","      <td>0</td>\n","      <td>206</td>\n","    </tr>\n","    <tr>\n","      <th>281</th>\n","      <td>–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è\\n–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è</td>\n","      <td>tommyfish\\ntommyfish</td>\n","      <td>2023-12-25</td>\n","      <td>–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ</td>\n","      <td>https://vk.com/wall-171840666_286874\\nhttps://...</td>\n","      <td>–î—Ä—É–∑—å—è, –∑–∞ –ø—Ä–æ—à–ª—É—é –Ω–µ–¥–µ–ª—é —Å–æ–±—Ä–∞–ª–æ—Å—å –º–µ–Ω–µ–µ 30 —Ñ...</td>\n","      <td>9</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3255</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>82</th>\n","      <td>–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è</td>\n","      <td>tommyfish</td>\n","      <td>2023-05-20</td>\n","      <td>–û–ø—Ä–æ—Å</td>\n","      <td>https://vk.com/wall-171840666_201855</td>\n","      <td>–í–Ω–∏–º–∞–Ω–∏–µ, –ª—é–±–∏—Ç–µ–ª–∏ —Å—É—à–∏!\\n\\n–î–∞–≤–∞–π—Ç–µ –≤—ã–±–µ—Ä–µ–º –ª—é...</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1984</td>\n","      <td>0</td>\n","      <td>47</td>\n","    </tr>\n","    <tr>\n","      <th>122</th>\n","      <td>–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è</td>\n","      <td>tommyfish</td>\n","      <td>2023-07-02</td>\n","      <td>–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ</td>\n","      <td>https://vk.com/wall-171840666_218925</td>\n","      <td>–ë–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö —Ä–æ–ª–ª–æ–≤ –º–Ω–æ–≥–æ –Ω–µ –±—ã–≤–∞–µ—Ç!\\n–î–∞—Ä–∏–º –Ω–∞—à–∏...</td>\n","      <td>9</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>4161</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>201 rows √ó 12 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f5226093-2635-4f75-b10f-0a12ebbbea2d')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-f5226093-2635-4f75-b10f-0a12ebbbea2d button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-f5226093-2635-4f75-b10f-0a12ebbbea2d');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-80913cad-5d1b-4d7e-b130-ca81633e813c\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-80913cad-5d1b-4d7e-b130-ca81633e813c')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-80913cad-5d1b-4d7e-b130-ca81633e813c button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"train","summary":"{\n  \"name\": \"train\",\n  \"rows\": 201,\n  \"fields\": [\n    {\n      \"column\": \"\\u041f\\u0440\\u043e\\u0435\\u043a\\u0442\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"\\u041e\\u0431\\u0449\\u0430\\u044f \\u043a\\u0430\\u0442\\u0435\\u0433\\u043e\\u0440\\u0438\\u044f\\n\\u041e\\u0431\\u0449\\u0430\\u044f \\u043a\\u0430\\u0442\\u0435\\u0433\\u043e\\u0440\\u0438\\u044f\",\n          \"\\u041e\\u0431\\u0449\\u0430\\u044f \\u043a\\u0430\\u0442\\u0435\\u0433\\u043e\\u0440\\u0438\\u044f\",\n          \"\\u041e\\u0431\\u0449\\u0430\\u044f \\u043a\\u0430\\u0442\\u0435\\u0433\\u043e\\u0440\\u0438\\u044f\\n\\u041e\\u0431\\u0449\\u0430\\u044f \\u043a\\u0430\\u0442\\u0435\\u0433\\u043e\\u0440\\u0438\\u044f\\n\\u041e\\u0431\\u0449\\u0430\\u044f \\u043a\\u0430\\u0442\\u0435\\u0433\\u043e\\u0440\\u0438\\u044f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u0410\\u043a\\u043a\\u0430\\u0443\\u043d\\u0442\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"tommyfish\\ntommyfish\",\n          \"tommyfish\",\n          \"tommyfish\\ntommyfish\\ntommyfish\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u0414\\u0430\\u0442\\u0430\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2023-02-26 00:00:00\",\n        \"max\": \"2023-12-31 00:00:00\",\n        \"num_unique_values\": 201,\n        \"samples\": [\n          \"2023-06-07 00:00:00\",\n          \"2023-05-30 00:00:00\",\n          \"2023-06-21 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u0422\\u0438\\u043f\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 17,\n        \"samples\": [\n          \"\\u0418\\u0437\\u043e\\u0431\\u0440\\u0430\\u0436\\u0435\\u043d\\u0438\\u0435\\n\\u0418\\u0437\\u043e\\u0431\\u0440\\u0430\\u0436\\u0435\\u043d\\u0438\\u0435\",\n          \"\\u0418\\u0437\\u043e\\u0431\\u0440\\u0430\\u0436\\u0435\\u043d\\u0438\\u0435\",\n          \"\\u0412\\u0438\\u0434\\u0435\\u043e\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u0421\\u0441\\u044b\\u043b\\u043a\\u0430\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 201,\n        \"samples\": [\n          \"https://vk.com/wall-171840666_208144\",\n          \"https://vk.com/wall-171840666_204937\\nhttps://vk.com/wall-171840666_204893\",\n          \"https://vk.com/wall-171840666_213432\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u0422\\u0435\\u043a\\u0441\\u0442\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 167,\n        \"samples\": [\n          \"\\u041f\\u043b\\u0430\\u043d \\u043d\\u0430 \\u0441\\u0435\\u0433\\u043e\\u0434\\u043d\\u044f:\\n\\u0420\\u0430\\u0434\\u043e\\u0432\\u0430\\u0442\\u044c\\u0441\\u044f \\u0432\\u043a\\u0443\\u0441\\u043d\\u0435\\u0439\\u0448\\u0435\\u043c\\u0443 \\u043f\\u043e\\u0434\\u0430\\u0440\\u043a\\u0443! \\ud83c\\udf81\\n\\u2705 \\u0422\\u043e\\u043b\\u044c\\u043a\\u043e \\u0434\\u043e 6.04 \\u043f\\u0440\\u0438 \\u0437\\u0430\\u043a\\u0430\\u0437\\u0435:\\n\\u041e\\u0442 800\\u20bd - 4 \\u0448\\u0442. \\u043f\\u043e \\u043f\\u0440\\u043e\\u043c\\u043e\\u043a\\u043e\\u0434\\u0443 \\u00ab413\\u00bb\\n\\u041e\\u0442 1000\\u20bd \\u2013 8 \\u0448\\u0442. \\u043f\\u043e \\u043f\\u0440\\u043e\\u043c\\u043e\\u043a\\u043e\\u0434\\u0443 \\u00ab813\\u00bb \\u0412\\u044b \\u043f\\u043e\\u043b\\u0443\\u0447\\u0438\\u0442\\u0435 \\u043d\\u0435\\u0432\\u0435\\u0440\\u043e\\u044f\\u0442\\u043d\\u043e \\u0432\\u043a\\u0443\\u0441\\u043d\\u044b\\u0439 \\u0440\\u043e\\u043b\\u043b  \\u041c\\u0435\\u0445\\u0438\\u043a\\u043e \\u0430\\u0431\\u0441\\u043e\\u043b\\u044e\\u0442\\u043d\\u043e \\u0431\\u0435\\u0441\\u043f\\u043b\\u0430\\u0442\\u043d\\u043e \\ud83d\\ude3b\\n\\u041d\\u0435 \\u0443\\u043f\\u0443\\u0441\\u0442\\u0438\\u0442\\u0435 \\u0432\\u043e\\u0437\\u043c\\u043e\\u0436\\u043d\\u043e\\u0441\\u0442\\u044c \\u0438 \\u043e\\u0444\\u043e\\u0440\\u043c\\u043b\\u044f\\u0439\\u0442\\u0435 \\u0437\\u0430\\u043a\\u0430\\u0437 \\ud83d\\udc49 https://vk.com/app6408974_-215401732\",\n          \"\\u0414\\u043e \\u041d\\u043e\\u0432\\u043e\\u0433\\u043e \\u0433\\u043e\\u0434\\u0430 \\u043e\\u0441\\u0442\\u0430\\u043b\\u043e\\u0441\\u044c \\u0432\\u0441\\u0435\\u0433\\u043e 6 \\u0434\\u043d\\u0435\\u0439! \\ud83c\\udf84 \\n \\n\\u041a\\u0430\\u043a \\u0438\\u0434\\u0451\\u0442 \\u043f\\u043e\\u0434\\u0433\\u043e\\u0442\\u043e\\u0432\\u043a\\u0430? \\u043d\\u0435 \\u0431\\u043e\\u0438\\u0442\\u0435\\u0441\\u044c, \\u0447\\u0442\\u043e-\\u0442\\u043e \\u0437\\u0430\\u0431\\u044b\\u0442\\u044c? \\u041c\\u044b \\u043f\\u0440\\u0438\\u0433\\u043e\\u0442\\u043e\\u0432\\u0438\\u043b\\u0438 \\u0447\\u0435\\u043a-\\u043b\\u0438\\u0441\\u0442 \\u0438\\u0437 4-\\u0445 \\u043e\\u0431\\u044f\\u0437\\u0430\\u0442\\u0435\\u043b\\u044c\\u043d\\u044b\\u0445 \\u043f\\u0443\\u043d\\u043a\\u0442\\u043e\\u0432, \\u0447\\u0442\\u043e\\u0431\\u044b \\u0432\\u044b \\u043f\\u0440\\u043e\\u0432\\u0435\\u0440\\u0438\\u043b\\u0438 \\u0441\\u0435\\u0431\\u044f! \\ud83d\\udc47 \\n \\n\\u2714 \\u0423\\u043a\\u0440\\u0430\\u0441\\u0438\\u0442\\u044c \\u0434\\u043e\\u043c \\u0438 \\u043f\\u043e\\u0441\\u0442\\u0430\\u0432\\u0438\\u0442\\u044c \\u0435\\u043b\\u043a\\u0443. \\n\\u2714 \\u041f\\u043e\\u0434\\u0433\\u043e\\u0442\\u043e\\u0432\\u0438\\u0442\\u044c \\u043f\\u043e\\u0434\\u0430\\u0440\\u043a\\u0438 \\u0434\\u043b\\u044f \\u0431\\u043b\\u0438\\u0437\\u043a\\u0438\\u0445. \\n\\u2714 \\u041f\\u043e\\u0441\\u043c\\u043e\\u0442\\u0440\\u0435\\u0442\\u044c \\u0445\\u043e\\u0442\\u044f \\u0431\\u044b \\u043e\\u0434\\u0438\\u043d \\u043d\\u043e\\u0432\\u043e\\u0433\\u043e\\u0434\\u043d\\u0438\\u0439 \\u0444\\u0438\\u043b\\u044c\\u043c. \\n\\u2714 \\u041f\\u043e\\u0437\\u0430\\u0431\\u043e\\u0442\\u0438\\u0442\\u044c\\u0441\\u044f \\u043e \\u043f\\u0440\\u0430\\u0437\\u0434\\u043d\\u0438\\u0447\\u043d\\u043e\\u043c \\u0441\\u0442\\u043e\\u043b\\u0435. \\n \\n\\u041f\\u043e\\u0441\\u043b\\u0435\\u0434\\u043d\\u0438\\u0439 \\u043f\\u0443\\u043d\\u043a\\u0442 - \\u0441\\u0430\\u043c\\u044b\\u0439 \\u0432\\u0430\\u0436\\u043d\\u044b\\u0439. \\u041f\\u043e\\u044d\\u0442\\u043e\\u043c\\u0443 \\u043d\\u0430\\u043f\\u043e\\u043c\\u0438\\u043d\\u0430\\u0435\\u043c, \\u0447\\u0442\\u043e \\u043f\\u0440\\u0438\\u0435\\u043c \\u043f\\u0440\\u0435\\u0434\\u0437\\u0430\\u043a\\u0430\\u0437\\u043e\\u0432 \\u043d\\u0430 31 \\u0434\\u0435\\u043a\\u0430\\u0431\\u0440\\u044f \\u043f\\u0440\\u043e\\u0434\\u043e\\u043b\\u0436\\u0430\\u0435\\u0442\\u0441\\u044f. \\u041d\\u0435 \\u0443\\u043f\\u0443\\u0441\\u0442\\u0438\\u0442\\u0435 \\u0448\\u0430\\u043d\\u0441 \\u043f\\u043e\\u0434\\u0433\\u043e\\u0442\\u043e\\u0432\\u0438\\u0442\\u044c\\u0441\\u044f \\u043a \\u041d\\u043e\\u0432\\u043e\\u043c\\u0443 \\u0433\\u043e\\u0434\\u0443 \\u0437\\u0430\\u0440\\u0430\\u043d\\u0435\\u0435 \\u0438 \\u0438\\u0437\\u0431\\u0435\\u0436\\u0430\\u0442\\u044c \\u043b\\u0438\\u0448\\u043d\\u0435\\u0439 \\u0441\\u0443\\u0435\\u0442\\u044b \\u0432 \\u043f\\u0440\\u0435\\u0434\\u043f\\u0440\\u0430\\u0437\\u0434\\u043d\\u0438\\u0447\\u043d\\u044b\\u0435 \\u0434\\u043d\\u0438! \\u2728 \\n \\n\\u041e\\u0444\\u043e\\u0440\\u043c\\u0438\\u0442\\u044c \\u043f\\u0440\\u0435\\u0434\\u0437\\u0430\\u043a\\u0430\\u0437 \\u043c\\u043e\\u0436\\u043d\\u043e \\u0447\\u0435\\u0440\\u0435\\u0437 \\u043c\\u043e\\u0431\\u0438\\u043b\\u044c\\u043d\\u043e\\u0435 \\u043f\\u0440\\u0438\\u043b\\u043e\\u0436\\u0435\\u043d\\u0438\\u0435, \\u043d\\u0430 \\u043b\\u044e\\u0431\\u043e\\u0435 \\u0441\\u0432\\u043e\\u0431\\u043e\\u0434\\u043d\\u043e\\u0435 \\u0432\\u0440\\u0435\\u043c\\u044f \\u0434\\u043e 18:00 \\u2764 \\n \\n\\ud83d\\udc49 \\u0421\\u043a\\u0430\\u0447\\u0430\\u0442\\u044c \\u043f\\u0440\\u0438\\u043b\\u043e\\u0436\\u0435\\u043d\\u0438\\u0435 \\u043c\\u043e\\u0436\\u043d\\u043e \\u043f\\u043e \\u0441\\u0441\\u044b\\u043b\\u043a\\u0435 - https://vk.cc/ccIQMv \\n \\n\\u0412\\u0430\\u0436\\u043d\\u043e, \\u043a\\u043e\\u043b\\u0438\\u0447\\u0435\\u0441\\u0442\\u0432\\u043e \\u043f\\u0440\\u0438\\u043d\\u0438\\u043c\\u0430\\u0435\\u043c\\u044b\\u0445 \\u043f\\u0440\\u0435\\u0434\\u0437\\u0430\\u043a\\u0430\\u0437\\u043e\\u0432 \\u043e\\u0433\\u0440\\u0430\\u043d\\u0438\\u0447\\u0435\\u043d\\u043e, 31 \\u0434\\u0435\\u043a\\u0430\\u0431\\u0440\\u044f \\u0430\\u043a\\u0446\\u0438\\u0438 \\u0442\\u0430\\u043a\\u0436\\u0435 \\u043d\\u0435 \\u0434\\u0435\\u0439\\u0441\\u0442\\u0432\\u0443\\u044e\\u0442.\\n\\ud83d\\ude06\",\n          \"nan\\n\\u0415\\u0449\\u0435 \\u043d\\u0435 \\u0437\\u0430\\u043a\\u0430\\u0437\\u044b\\u0432\\u0430\\u043b\\u0438 \\u0443 \\u043d\\u0430\\u0441?\\n\\n\\u0414\\u0430\\u0440\\u0438\\u043c \\\"\\u0424\\u0438\\u043b\\u0430\\u0434\\u0435\\u043b\\u044c\\u0444\\u0438\\u044e \\u043b\\u0430\\u0439\\u0442 \\u0441 \\u043b\\u043e\\u0441\\u043e\\u0441\\u0435\\u043c\\\" \\u043d\\u0430 \\u0432\\u0430\\u0448 \\u043f\\u0435\\u0440\\u0432\\u044b\\u0439 \\u0437\\u0430\\u043a\\u0430\\u0437.\\n\\u0421\\u043a\\u043e\\u0440\\u0435\\u0435 \\u0434\\u0435\\u043b\\u0430\\u0439 \\u0437\\u0430\\u043a\\u0430\\u0437 \\u0447\\u0435\\u0440\\u0435\\u0437 \\u043f\\u0440\\u0438\\u043b\\u043e\\u0436\\u0435\\u043d\\u0438\\u0435 : https://\\nvk.cc/cn7BHM\\n\\u041f\\u043e\\u0434\\u0432\\u043e\\u0434\\u0438\\u043c \\u0438\\u0442\\u043e\\u0433\\u0438 \\u043a\\u043e\\u043d\\u043a\\u0443\\u0440\\u0441\\u0430 \\u0444\\u043e\\u0442\\u043e-\\u043e\\u0442\\u0437\\u044b\\u0432\\u043e\\u0432\\ud83d\\udcf8 \\u041f\\u043e\\u0437\\u0434\\u0440\\u0430\\u0432\\u043b\\u044f\\u0435\\u043c \\u043f\\u043e\\u0431\\u0435\\u0434\\u0438\\u0442\\u0435\\u043b\\u044c\\u043d\\u0438\\u0446\\u0443 \\ud83e\\udd73 \\n \\n\\ud83c\\udf71 [id104395488|\\u042d\\u043b\\u044c\\u0432\\u0438\\u0440\\u0430 \\u0414\\u0443\\u0432\\u0430\\u043b\\u043e\\u0432\\u0430] - \\u0432\\u044b\\u0438\\u0433\\u0440\\u044b\\u0432\\u0430\\u0435\\u0442 \\u0441\\u0435\\u0442 \\\"\\u0424\\u044c\\u044e\\u0436\\u043d\\\"! \\n \\n\\u0421\\u043f\\u0430\\u0441\\u0438\\u0431\\u043e \\u0432\\u0441\\u0435\\u043c \\u0443\\u0447\\u0430\\u0441\\u0442\\u043d\\u0438\\u043a\\u0430\\u043c! \\u041a\\u043e\\u043d\\u043a\\u0443\\u0440\\u0441 \\u043f\\u0440\\u043e\\u0434\\u043e\\u043b\\u0436\\u0430\\u0435\\u0442\\u0441\\u044f\\ud83d\\ude09 \\n\\u041f\\u043e\\u0434\\u0440\\u043e\\u0431\\u043d\\u044b\\u0435 \\u0443\\u0441\\u043b\\u043e\\u0432\\u0438\\u044f \\u043a\\u043e\\u043d\\u043a\\u0443\\u0440\\u0441\\u0430\\ud83d\\udc49 https://vk.com/wall-171840666_108565\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u041b\\u0430\\u0439\\u043a\\u043e\\u0432\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 202,\n        \"min\": 4,\n        \"max\": 2765,\n        \"num_unique_values\": 57,\n        \"samples\": [\n          12,\n          23,\n          320\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u041a\\u043e\\u043c\\u043c\\u0435\\u043d\\u0442\\u0430\\u0440\\u0438\\u0435\\u0432\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 45,\n        \"min\": 0,\n        \"max\": 632,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          8,\n          632,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u0420\\u0435\\u043f\\u043e\\u0441\\u0442\\u043e\\u0432\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 32,\n        \"min\": 0,\n        \"max\": 264,\n        \"num_unique_values\": 39,\n        \"samples\": [\n          50,\n          30,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u041f\\u0440\\u043e\\u0441\\u043c\\u043e\\u0442\\u0440\\u043e\\u0432\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12374,\n        \"min\": 1883,\n        \"max\": 137830,\n        \"num_unique_values\": 199,\n        \"samples\": [\n          3154,\n          3120,\n          3581\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u041a\\u043b\\u0430\\u0441\\u0441 \\u043f\\u043e\\u0441\\u0442\\u0430\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0,\n          4,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u041f\\u0440\\u0438\\u0440\\u043e\\u0441\\u0442\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 235,\n        \"min\": 0,\n        \"max\": 3064,\n        \"num_unique_values\": 87,\n        \"samples\": [\n          141,\n          0,\n          130\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["train, test = train.reset_index(drop=True), test.reset_index(drop=True)"],"metadata":{"id":"zSpS-gIbMOlZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["–í—ã–¥–µ–ª—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã\n","\n","\n"],"metadata":{"id":"tMe0LXA4jlRA"}},{"cell_type":"code","source":["y_train = utils.to_categorical(train['–ö–ª–∞—Å—Å –ø–æ—Å—Ç–∞'], nb_classes)\n","\n","y_test = utils.to_categorical(test['–ö–ª–∞—Å—Å –ø–æ—Å—Ç–∞'], nb_classes)\n","y_test"],"metadata":{"id":"jZr17oFCj3xg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711866952232,"user_tz":-300,"elapsed":85,"user":{"displayName":"Andrey Sokolov","userId":"00506076773780018870"}},"outputId":"5fe95856-6bea-49b5-9a8d-d1dd1f4eb0cd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1., 0., 0., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 1.],\n","       [1., 0., 0., 0., 0.],\n","       [0., 1., 0., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 1.],\n","       [0., 0., 1., 0., 0.],\n","       [0., 0., 0., 0., 1.],\n","       [0., 0., 1., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [0., 0., 0., 0., 1.],\n","       [1., 0., 0., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [0., 0., 0., 0., 1.],\n","       [1., 0., 0., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 1.],\n","       [1., 0., 0., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [0., 1., 0., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [0., 1., 0., 0., 0.],\n","       [0., 0., 0., 0., 1.],\n","       [0., 0., 1., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [0., 0., 1., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [1., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 1.],\n","       [0., 0., 1., 0., 0.]], dtype=float32)"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"],"metadata":{"id":"TpNqn6Uzvs_R"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras import utils\n"],"metadata":{"id":"iG_N2aGcx1hb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n","num_words = 100000"],"metadata":{"id":"A7Dzigv52Joa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = Tokenizer(num_words=num_words)"],"metadata":{"id":"zXZDX4Es2uq0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# names = train['–ù–∞–∑–≤–∞–Ω–∏–µ']\n","descrs = train['–¢–µ–∫—Å—Ç']\n","\n","# names = names.reset_index(drop=True)\n","# descrs = descrs.reset_index(drop=True)\n","\n","\n","# for i, v in names.items():\n","#   print(i, ' ', v)\n"],"metadata":{"id":"lr2s35eUzfUA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.fit_on_texts(descrs)"],"metadata":{"id":"2Odn9its0L3D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.word_index"],"metadata":{"id":"l0Ax5QH66KCS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711866952232,"user_tz":-300,"elapsed":78,"user":{"displayName":"Andrey Sokolov","userId":"00506076773780018870"}},"outputId":"e2fa1711-59ed-4e96-95c1-344f6f56f08b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'–≤': 1,\n"," '–∏': 2,\n"," '–Ω–∞': 3,\n"," '—Å': 4,\n"," 'vk': 5,\n"," '—Å–µ—Ç': 6,\n"," 'https': 7,\n"," '‚Äî': 8,\n"," '–Ω–µ': 9,\n"," '—Ñ–æ—Ç–æ': 10,\n"," '–æ—Ç': 11,\n"," '3': 12,\n"," '–¥–Ω—è': 13,\n"," 'üç±': 14,\n"," '–ø—Ä–æ–º–æ–∫–æ–¥': 15,\n"," '–ø–æ': 16,\n"," '–ø—Ä–∏': 17,\n"," 'cc': 18,\n"," '–∞': 19,\n"," 'com': 20,\n"," '–≤—Å–µ–º': 21,\n"," '–∑–∞': 22,\n"," '–¥–∞—Ä–∏–º': 23,\n"," '–∑–∞–∫–∞–∑': 24,\n"," '–º—ã': 25,\n"," 'cn7bhm': 26,\n"," '–¥–æ': 27,\n"," '—à—Ç': 28,\n"," '–∑–∞–∫–∞–∑–µ': 29,\n"," '—Ä–æ–ª–ª–æ–≤': 30,\n"," '–∏—Ç–æ–≥–∏': 31,\n"," 'üç£': 32,\n"," '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ': 33,\n"," '–º–æ–∂–Ω–æ': 34,\n"," '–¥–ª—è': 35,\n"," '—Ä–æ–ª–ª': 36,\n"," '–≤—ã': 37,\n"," '–º–∏–Ω–∏': 38,\n"," '–µ—Å–ª–∏': 39,\n"," '–¥–µ–π—Å—Ç–≤—É–µ—Ç': 40,\n"," '—á—Ç–æ': 41,\n"," '—Ä–æ–ª–ª—ã': 42,\n"," '—Ä—É–±–ª–µ–π': 43,\n"," '–ø–æ—Å–ª–µ': 44,\n"," '—É–∂–µ': 45,\n"," '–∫–æ–Ω–∫—É—Ä—Å–∞': 46,\n"," '—ç—Ç–æ—Ç': 47,\n"," '1200': 48,\n"," '—á–µ—Ä–µ–∑': 49,\n"," '–¥–µ–Ω—å': 50,\n"," '171840666': 51,\n"," '—Å–µ–≥–æ–¥–Ω—è': 52,\n"," '–∏–º–µ–Ω–∏–Ω–Ω–∏–∫–∞–º': 53,\n"," '–¥—Ä': 54,\n"," '—Å–æ–±—ã—Ç–∏—è': 55,\n"," '—Å—Å—ã–ª–∫–µ': 56,\n"," '–∫': 57,\n"," '–±—ã—Ç—å': 58,\n"," '—Å–≤–æ–π': 59,\n"," 'üòª': 60,\n"," '–ª–∞–π—Ç': 61,\n"," '–ø–æ–¥–∞—Ä–æ–∫': 62,\n"," '–≤–∞—Å': 63,\n"," '–∫—Ç–æ': 64,\n"," 'app6408974': 65,\n"," '–æ—Ç–∑—ã–≤–æ–≤': 66,\n"," 'nan': 67,\n"," '–±—É–¥–µ—Ç': 68,\n"," '–æ': 69,\n"," '–∫—Ä–µ–≤–µ—Ç–∫–æ–π': 70,\n"," '—É—Å–ª–æ–≤–∏—è': 71,\n"," '–æ–Ω': 72,\n"," '–º–∏–Ω—É—Ç': 73,\n"," '30': 74,\n"," '–ø–æ–ª—É—á–∏—Ç—å': 75,\n"," '—Å–µ—Ç–∞': 76,\n"," '—Ñ–∏–ª–∞–¥–µ–ª—å—Ñ–∏—è': 77,\n"," '—Å–ø–∞—Å–∏–±–æ': 78,\n"," '–Ω–∞—Å': 79,\n"," '—Å–∫–æ—Ä–µ–µ': 80,\n"," '—Å–æ—Å—Ç–∞–≤': 81,\n"," '–∞–∫—Ü–∏—è': 82,\n"," '—Ç–æ': 83,\n"," '–º–æ–∂–µ—Ç': 84,\n"," '–Ω–∞–º': 85,\n"," '–∫–∞–∫': 86,\n"," '‚Ä¢': 87,\n"," '000': 88,\n"," '–∏–∑': 89,\n"," '—Å–ª–µ–¥—É—é—â–∏–π': 90,\n"," '—Å–∫–æ—Ä–æ': 91,\n"," '–∫—Ä—É–≥—É': 92,\n"," 'ü•≥': 93,\n"," '–ø—Ä–æ–º–æ–∫–æ–¥—É': 94,\n"," '–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫': 95,\n"," '–ø—Ä–∞–∑–¥–Ω–∏–∫': 96,\n"," '—ç—Ç–æ': 97,\n"," '—Ç–æ–ª—å–∫–æ': 98,\n"," '–º–Ω–æ–≥–æ': 99,\n"," '–±—ã–≤–∞–µ—Ç': 100,\n"," '–Ω–∞—à–∏–º': 101,\n"," '—É—á–∞—Å—Ç–∏—è': 102,\n"," '—Å–æ–æ–±—â–µ–Ω–∏—è': 103,\n"," '–≥—Ä—É–ø–ø—ã': 104,\n"," '–±–ª–∏–∑–∫–∏—Ö': 105,\n"," '–≤–∫—É—Å–Ω–æ–≥–æ': 106,\n"," '–ø–æ–∑–∞–±–æ—Ç–∏–º—Å—è': 107,\n"," '—É—á–∞—Å—Ç–Ω–∏–∫–∞–º': 108,\n"," '—Ä–æ–∂–¥–µ–Ω–∏—è': 109,\n"," '–æ—Ç–º–µ—Ç—å—Ç–µ': 110,\n"," '–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–π': 111,\n"," '—Ä–æ–¥—Å—Ç–≤–µ–Ω–Ω–∏–∫–æ–≤': 112,\n"," '–ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ': 113,\n"," '–∑–∞—Å—Ç–æ–ª—å—è': 114,\n"," '–º—ã‚ù§': 115,\n"," '–º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º': 116,\n"," '—Ä—É–±–ª–µ–πüéÅ': 117,\n"," 'üç±—á—Ç–æ': 118,\n"," '–≤—Ö–æ–¥–∏—Ç': 119,\n"," '–∂–∞—Ä–µ–Ω–Ω—ã–π': 120,\n"," '–∫—É—Ä–æ—á–∫–æ–π': 121,\n"," '–ø–æ–¥–≤–æ–¥–∏–º': 122,\n"," '–∫–æ–Ω–∫—É—Ä—Å': 123,\n"," '—Ä–∞—Å—Å—ã–ª–∫—É': 124,\n"," '–≤–∞–º': 125,\n"," '—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç': 126,\n"," '–±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö': 127,\n"," '–ø–µ—Ä–µ–Ω–æ—Å—è—Ç—Å—è': 128,\n"," '–ø–æ–∑–¥—Ä–∞–≤–ª—è–µ–º': 129,\n"," '40': 130,\n"," '‚ÇΩ': 131,\n"," '–Ω–µ–¥–µ–ª—é': 132,\n"," '1': 133,\n"," '¬´—Ç–æ–º–º–∏': 134,\n"," '—Ñ–∏—à¬ª': 135,\n"," '–Ω–æ': 136,\n"," '—É': 137,\n"," '–ø—Ä–∏–∑': 138,\n"," '–≤—ã–∏–≥—Ä—ã–≤–∞–µ—Ç': 139,\n"," '—á—Ç–æ–±—ã': 140,\n"," '—Å–¥–µ–ª–∞–π': 141,\n"," '–Ω—É–∂–Ω–æ': 142,\n"," '–æ—Ç–∑—ã–≤–æ–≤üì∏': 143,\n"," 'wall': 144,\n"," 'üëâ': 145,\n"," '215401732': 146,\n"," '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è': 147,\n"," '–º–µ—Å—Ç–æ': 148,\n"," '‚úÖ—Å–µ–∫—Ä–µ—Ç–Ω—ã–π': 149,\n"," 'üí¨–∞–∫—Ü–∏—è': 150,\n"," '–¥—Ä—É–∑—å—è': 151,\n"," '–∫–∞–∂–¥—ã–π': 152,\n"," '–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö': 153,\n"," '–ø–æ–±–µ–¥–∏—Ç–µ–ª—è': 154,\n"," '2': 155,\n"," '–ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—èüòâ': 156,\n"," '–ø–æ–¥—Ä–æ–±–Ω—ã–µ': 157,\n"," '–∫–æ–Ω–∫—É—Ä—Å–∞üëâ': 158,\n"," '–≤—Å–µ': 159,\n"," '–≤—Ä–µ–º—è': 160,\n"," '‚ö°Ô∏è—Å–∫–∞—á–∞–π': 161,\n"," '—Ñ—å—é–∂–Ω': 162,\n"," '—Ä–∞–∑': 163,\n"," '–µ—â–µ': 164,\n"," '—Ç–∞–∫': 165,\n"," '–¥–æ—Å—Ç–∞–≤–∫–∏': 166,\n"," '–∂–µ–ª–∞–µ–º': 167,\n"," '108565': 168,\n"," '—Ç–æ–º–º–∏': 169,\n"," '—Ñ–∏—à': 170,\n"," '–º–µ–Ω—é': 171,\n"," '–∑–∞–∫–∞–∂–∏': 172,\n"," '¬´–¥–ª—è': 173,\n"," '–¥–≤–æ–∏—Ö¬ª': 174,\n"," 'tommyfish': 175,\n"," '–æ—Ç–∑—ã–≤': 176,\n"," '4': 177,\n"," '5': 178,\n"," '–Ω–æ–≤—ã–π': 179,\n"," '–∂–¥–µ–º': 180,\n"," 'üòâ': 181,\n"," '‚Äì': 182,\n"," '–≤–∞—à': 183,\n"," '–Ω–æ—Ä–∏': 184,\n"," '‚ù§': 185,\n"," '32': 186,\n"," '–ø–æ–ª—É—á–∏–ª–∏': 187,\n"," '–Ω–∞—à–∏': 188,\n"," 'üòç': 189,\n"," '–Ω–∞—à–∞': 190,\n"," '–±—É–¥—É—Ç': 191,\n"," 'ru': 192,\n"," 'ü•∞': 193,\n"," '–ø–æ–ª—É—á–∞–µ—Ç–µ': 194,\n"," 'üî•': 195,\n"," '10': 196,\n"," '–æ—Ñ–æ—Ä–º–ª—è–π—Ç–µ': 197,\n"," '—Ä–∏—Å': 198,\n"," '–¥–µ–∫–∞–±—Ä—è': 199,\n"," '8': 200,\n"," '24': 201,\n"," '–º–µ–Ω–µ–µ': 202,\n"," '–Ω–µ–¥–µ–ª–∏': 203,\n"," '–º–µ–Ω—å—à–µ': 204,\n"," '–ø–æ–¥–∞—Ä–∫–æ–º': 205,\n"," '–∫–∞–∂–¥–æ–º—É': 206,\n"," '–Ω–æ–º–µ—Ä': 207,\n"," '–±—É–¥–µ–º': 208,\n"," '–≥–ª–∞–≤–Ω—ã–π': 209,\n"," '–ø–æ–±–µ–¥–∏—Ç–µ–ª—å–Ω–∏—Ü—É': 210,\n"," '–ø–æ–¥–∞—Ä–∫–∏': 211,\n"," '–Ω–∞–ø–æ–º–∏–Ω–∞–µ–º': 212,\n"," '–ª–æ—Å–æ—Å–µ–º': 213,\n"," 'üëâüèª': 214,\n"," '—Ä–æ–ª–ª–∞–º–∏': 215,\n"," '–≤–∞–∂–Ω–æ': 216,\n"," '‚úÖ': 217,\n"," 'üëâ–Ω–∞–ø–æ–º–∏–Ω–∞–µ–º': 218,\n"," '–ø—Ä–∏—Å–ª–∞—Ç—å': 219,\n"," '–ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å': 220,\n"," '–∑–∞–ø–∏—Å—å': 221,\n"," '–¥–æ–ª–∂–Ω–∞': 222,\n"," '–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∞': 223,\n"," '–ø—Ä–æ–¥—É–∫—Ü–∏—è': 224,\n"," '–ø–æ–¥–≤–æ–¥–∏—Ç—å—Å—è': 225,\n"," '—Ç–µ—á–µ–Ω–∏–µ': 226,\n"," '–Ω–∞–±—Ä–∞–ª–æ—Å—å': 227,\n"," '–Ω–∞–ø–∏—Å–∞–≤': 228,\n"," '–æ—Ç–∑—ã–≤—É': 229,\n"," '–ø—Ä–∏—Å–≤–æ–µ–Ω': 230,\n"," '–ø–æ—Ä—è–¥–∫–æ–≤—ã–π': 231,\n"," '–æ–ø—Ä–µ–¥–µ–ª—è—Ç—å': 232,\n"," '–ø–æ–º–æ—â–∏': 233,\n"," 'randstuff': 234,\n"," '¬´—Ñ—å—é–∂–Ω¬ª': 235,\n"," '—É–¥–∞—á–∏ü§û': 236,\n"," '–ø—Ä–∏—è—Ç–Ω–æ–≥–æ': 237,\n"," '–∞–ø–ø–µ—Ç–∏—Ç–∞': 238,\n"," '—Ç—É—Ç': 239,\n"," '—Å—ã—Ä': 240,\n"," '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏': 241,\n"," 'me': 242,\n"," '–ø–æ–º–æ—â—å—é': 243,\n"," '–ø—Ä–∏–∑—ã': 244,\n"," '–ø—Ä–æ—à–ª—É—é': 245,\n"," '—Å–æ–±—Ä–∞–ª–æ—Å—å': 246,\n"," 'üòî': 247,\n"," '–∏—Å—Ö–æ–¥—è': 248,\n"," '–ø—Ä–∞–≤–∏–ª': 249,\n"," '–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫‚ù§': 250,\n"," '–Ω–∞—à—É': 251,\n"," 's': 252,\n"," '‚ö°Ô∏è': 253,\n"," '—Å–ª–∏–≤–æ—á–Ω—ã–π': 254,\n"," '77': 255,\n"," '–¥–µ–ª–∞–π': 256,\n"," '31': 257,\n"," '00': 258,\n"," '60': 259,\n"," '–∏–ª–∏': 260,\n"," '—Ä–∞–Ω–¥–æ–º–∞–π–∑–µ—Ä': 261,\n"," '–æ—Ñ–æ—Ä–º–∏—Ç—å': 262,\n"," '—Ä–æ–ª–ª–∞': 263,\n"," '–∑–∞–∫–∞–∑—ã–≤–∞–π—Ç–µ': 264,\n"," '–Ω–∞—à–µ–π': 265,\n"," '–ø–æ–¥–ø–∏—Å–∞–Ω': 266,\n"," '—Ç–µ': 267,\n"," '–∑–∞–∫–∞–∑–∞–ºüôÇ': 268,\n"," '–ø–æ—Ä—Ü–∏—é': 269,\n"," '–∫–∞–∂–¥—É—é': 270,\n"," '–≤—Ö–æ–¥': 271,\n"," '–æ—Ç–∫—Ä—ã—Ç': 272,\n"," '–ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è': 273,\n"," 'app5898182': 274,\n"," '993592': 275,\n"," '–¥–æ–ª—å—à–µ': 276,\n"," '–≤—Å–µ–≥–æ': 277,\n"," '—Å–æ—É—Å': 278,\n"," '–Ω–∞—à': 279,\n"," '–µ—Å—Ç—å': 280,\n"," '—Å–º–æ–∂–µ—Ç–µ': 281,\n"," '–∫–∞–∫–∏–µ': 282,\n"," '¬´ozon¬ª': 283,\n"," '–æ—Ç–∑—ã–≤—ã': 284,\n"," 'http': 285,\n"," '–æ—á–µ–Ω—å': 286,\n"," '—Å–∫–∏–¥–∫—É': 287,\n"," '—Ä–∞–±–æ—Ç–∞–µ–º': 288,\n"," 'ü§§': 289,\n"," '–ø–æ—á–µ–º—É': 290,\n"," '–∂–¥–∞—Ç—å': 291,\n"," '–ø–µ—Ä–≤—ã–π': 292,\n"," '–∑–∞–∫–∞–∑–∞': 293,\n"," '—Å–≤–æ–∏': 294,\n"," '–±–µ—Å–ø–ª–∞—Ç–Ω–æ': 295,\n"," '—Å—É–º–º–∏—Ä—É–µ—Ç—Å—è': 296,\n"," '–¥—Ä—É–≥–∏–º–∏': 297,\n"," '–Ω–∞–ø–∏—à–∏—Ç–µ': 298,\n"," '–≥—Ä': 299,\n"," '–≤–∫—É—Å–Ω–æ': 300,\n"," '–æ–Ω–ª–∞–π–Ω': 301,\n"," '–∑–∞–∫–∞–∑—ã': 302,\n"," '—Å–∫–∞—á–∞–π': 303,\n"," '23': 304,\n"," '–ª–æ—Å–æ—Å—å': 305,\n"," 'ü§©': 306,\n"," '—Å–µ–±—è': 307,\n"," '–∑–∞–∫–∞–∑–æ–≤': 308,\n"," '–∞–∫—Ü–∏–∏': 309,\n"," '—Ç–µ—Ö': 310,\n"," '–≤–¥—Ä—É–≥': 311,\n"," 'üéÅ': 312,\n"," '–∏—Ö': 313,\n"," '–≤–µ—á–µ—Ä': 314,\n"," '–ª–∞–π–∫': 315,\n"," '—Ä–∞–±–æ—Ç—ã': 316,\n"," '–¥–µ–π—Å—Ç–≤–∏—è': 317,\n"," '—Ç–æ–∂–µ': 318,\n"," '—É–∂–∏–Ω': 319,\n"," '—É—Å—Ç–∞–Ω–æ–≤–∫—É': 320,\n"," '¬´–Ω–æ–≤—ã–π': 321,\n"," '—Ç–æ–≥–¥–∞': 322,\n"," '—Ç–æ—á–Ω–æ': 323,\n"," '35': 324,\n"," '–¥–≤–æ–∏—Ö': 325,\n"," 'üëá': 326,\n"," '–∑–∞–∫–∞–∂–∏—Ç–µ': 327,\n"," '–∫–æ—Ç–æ—Ä—ã–π': 328,\n"," '—Å–µ–±–µ': 329,\n"," '–µ–≥–æ': 330,\n"," '–ø–µ—Ä–µ—Ö–æ–¥–∏—Ç–µ': 331,\n"," '‚ú®': 332,\n"," '–Ω–æ–≤–æ–≥–æ': 333,\n"," '–ø–æ—á—Ç–∏': 334,\n"," '—Å—É—à–∏': 335,\n"," 'üòÅ': 336,\n"," '–∏–º–µ–Ω–Ω–æ': 337,\n"," '–æ–¥–∏–Ω': 338,\n"," '‚è±': 339,\n"," '–≤–∞—à–∏': 340,\n"," '—Ä–æ–∑—ã–≥—Ä—ã—à–∞': 341,\n"," '–º–µ—Å—è—Ü': 342,\n"," '–≤—Å–µ—Ö': 343,\n"," '—Å–ø–∞–π—Å–∏': 344,\n"," '–∑–∞–∫–∞–∑—ã–≤–∞–ª–∏': 345,\n"," '—Ñ–∏–ª–∞–¥–µ–ª—å—Ñ–∏—é': 346,\n"," '—Ä—É–±': 347,\n"," '—Ä–µ–ø–æ—Å—Ç': 348,\n"," '–∑–∞–±—Ä–∞—Ç—å': 349,\n"," '–ø–æ—Å–ª–µ–¥–Ω–∏–π': 350,\n"," '–ø—Ä–æ–º–æ–∫–æ–¥–∞': 351,\n"," '–∑–∞–≤—Ç—Ä–∞': 352,\n"," '–∑–∞–∫–∞–∑—ã–≤–∞—Ç—å': 353,\n"," '–≤–∫—É—Å–Ω—ã–π': 354,\n"," '—Å–∞–º–æ–≤—ã–≤–æ–∑': 355,\n"," '—Ç–µ–º': 356,\n"," '—Å–≤–æ–∏–º': 357,\n"," '–≤–∫—É—Å–Ω—ã–º': 358,\n"," '—Å–µ–∫—Ä–µ—Ç–Ω—ã–π': 359,\n"," '‚Äú–º299‚Äù': 360,\n"," '–∫—Ä–µ–≤–µ—Ç–∫–æ–π‚Äù': 361,\n"," '–º–∏–∫—Å¬ª': 362,\n"," '–æ–≥—É—Ä—á–∏–∫': 363,\n"," '–¥–æ—Å—Ç–∞–≤–∏–º': 364,\n"," '—Å–æ': 365,\n"," '–Ω–∞—à–∏—Ö': 366,\n"," '—Ö–æ—á–µ—Ç—Å—è': 367,\n"," 'üéÑ': 368,\n"," '7': 369,\n"," '–≥–æ–¥–∞': 370,\n"," '—Å–¥–µ–ª–∞—Ç—å': 371,\n"," '–∫—Å—Ç–∞—Ç–∏': 372,\n"," '—Å—Ç–æ–∏—Ç': 373,\n"," 'üëáüèª': 374,\n"," '–±–æ–ª—å—à–æ–µ': 375,\n"," '–≤—ã–±—Ä–∞–Ω—ã': 376,\n"," 'app4938347': 377,\n"," '—ç—Ç–æ–π': 378,\n"," '–≤–∫—É—Å–Ω—ã–µ': 379,\n"," '–ø—Ä–µ–¥–∑–∞–∫–∞–∑': 380,\n"," '–∑–∞–∫–∞–∑–∞—Ç—å': 381,\n"," '–ª—é–±–∏–º—ã–µ': 382,\n"," '–≤–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ': 383,\n"," '–∫—Ä–∞–±': 384,\n"," '1000‚ÇΩ': 385,\n"," '—Å–æ—Å—Ç–∞–≤–µ': 386,\n"," '—Ç—Ä–∏': 387,\n"," '–¥–∞–≤–∞–π—Ç–µ': 388,\n"," '500': 389,\n"," '–¥–æ—Å—Ç–∞–≤–∫—É': 390,\n"," '¬´—Å–ø–∞–π—Å–∏': 391,\n"," '—Ä–µ–±—è—Ç': 392,\n"," '800': 393,\n"," '—Ä—É–±üôÇ\\xa0': 394,\n"," '–¥–∞—ë—Ç': 395,\n"," '–∫–æ–µ': 396,\n"," '–ø–æ—Å—Ç–∞—Ä–∞–µ–º—Å—è': 397,\n"," '–ø—Ä–∏–¥—É–º–∞—Ç—åüòÄ\\xa0—Ç–∞–∫': 398,\n"," '–ø–ª—é—Å–µ': 399,\n"," '–∑–∞–∫–∞–∑—ã–≤–∞–µ—Ç': 400,\n"," '–¥–Ω—è—Ö': 401,\n"," '14': 402,\n"," '—Ç–æ—Ä—Ç–∏–ª—å—è': 403,\n"," '–Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ': 404,\n"," '–±—ã—Å—Ç—Ä–æ': 405,\n"," '–≥–æ–¥—É': 406,\n"," '18': 407,\n"," '11': 408,\n"," '–æ—Å—Ç–∞–ª–æ—Å—å': 409,\n"," '—Å–∞–º—ã–π': 410,\n"," '—Ç–∞–∫–∏–º': 411,\n"," '–µ—â—ë': 412,\n"," '–Ω–∞—à–µ': 413,\n"," '–∂–¥–µ—Ç–µ': 414,\n"," '–ø–æ—ç—Ç–æ–º—É': 415,\n"," '—Å–ø–∏—Å–æ–∫': 416,\n"," '1‚É£': 417,\n"," '2‚É£': 418,\n"," '3‚É£': 419,\n"," '—Å–ª—É—á–∞–µ': 420,\n"," '0': 421,\n"," '—Å—Ç–∞–≤—å—Ç–µ': 422,\n"," '–Ω–∞—à–∏–º–∏': 423,\n"," '–º–æ–±–∏–ª—å–Ω–æ–º': 424,\n"," '–∞–∫—Ü–∏—è–º–∏': 425,\n"," '‚Äú—Ñ–∏–ª–∞–¥–µ–ª—å—Ñ–∏—é': 426,\n"," '–º–æ–±–∏–ª—å–Ω–æ–≥–æ': 427,\n"," '–æ–Ω–∏': 428,\n"," '—Å–µ–π—á–∞—Å': 429,\n"," '–ø–æ–±–µ–¥–∏—Ç–µ–ª–µ–π': 430,\n"," '–ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å': 431,\n"," '—Ç–µ–º–ø—É—Ä–∞': 432,\n"," '–ø–µ—Ä–º—å': 433,\n"," '‚úÖ–ø–æ–±–µ–¥–∏—Ç–µ–ª–∏': 434,\n"," 'üß°': 435,\n"," '1200‚ÇΩ': 436,\n"," '50': 437,\n"," '–∫–≥': 438,\n"," '–ø—Ä–∏–Ω—è—Ç—å': 439,\n"," '–¥–æ—Å—Ç–∞–≤–∫–æ–π': 440,\n"," '—Å—ã—Ä–Ω—ã–π': 441,\n"," '–∫—Ä–µ–≤–µ—Ç–∫–∞': 442,\n"," '–ø–æ—Å—Ç–∞–≤–∏—Ç—å': 443,\n"," '–∑–∞–ø–∏—Å–∏': 444,\n"," '–Ω–∞–ø–∏—Å–∞—Ç—å': 445,\n"," '¬´–ª–µ—Ç–Ω–∏–π': 446,\n"," '—Ö–∏—Ç¬ª': 447,\n"," '¬´–±–∞–º–±–ª¬ª': 448,\n"," '–ø–æ–¥–≤–µ–¥–µ–º': 449,\n"," '—É–¥–∞—á–∏': 450,\n"," '–≤–∏–¥–µ': 451,\n"," '–Ω–∞—Ç–∞–ª—å—è': 452,\n"," '–∑–∞–ª–µ—Ç–∞–π—Ç–µ': 453,\n"," '–ø–æ–ª—É—á–∞–π—Ç–µ': 454,\n"," '–∑–∞–≥–ª—è–Ω–µ—Ç': 455,\n"," '–∑–∞–∫–∞–∑–æ–ºüòå': 456,\n"," '–∞–¥—Ä–µ—Å–∞': 457,\n"," '–±—É–ª—å–≤–∞—Ä': 458,\n"," '–≥–∞–≥–∞—Ä–∏–Ω–∞': 459,\n"," '55': 460,\n"," '—è–±–ª–æ—á–∫–æ–≤–∞': 461,\n"," '19': 462,\n"," '–¥–µ–∫–∞–±—Ä–∏—Å—Ç–æ–≤': 463,\n"," '70': 464,\n"," '—Ü–µ–ª–∏–Ω–Ω–∞—è': 465,\n"," '39': 466,\n"," '–ø–æ–¥–ª–µ—Å–Ω–∞—è': 467,\n"," '43–∞': 468,\n"," '–≤–∏–ª—å—è–º—Å–∞': 469,\n"," '—à–∏—à–∫–∏–Ω–∞': 470,\n"," '–æ–¥–µ–≤–∞–π—Ç–µ—Å—å': 471,\n"," '—Ç–µ–ø–ª–µ–π': 472,\n"," 'üç£\\xa0': 473,\n"," '–¥–æ—Å—Ç–∞–≤–∫–∞': 474,\n"," '–ø—Ä–æ—Å—Ç–æ': 475,\n"," '–±—ã–ª–æ': 476,\n"," '–¥–æ–ª–≥–æ': 477,\n"," '–ø—Ä–∏–¥–µ—Ç—Å—è': 478,\n"," '—Å—Ä–µ–¥–Ω–µ–µ': 479,\n"," '–±–æ–ª—å—à–µ': 480,\n"," '–Ω–∞—à–µ–º': 481,\n"," '—è–Ω–≤–∞—Ä—è': 482,\n"," '–¥–Ω–µ–π': 483,\n"," '–µ–ª–µ–Ω–∞': 484,\n"," '–æ–±—Ä–∞–∑–æ–º': 485,\n"," '—Ä–µ—Å–ø–µ–∫—Ç': 486,\n"," '–æ—Ç–∑—ã–≤–æ–º': 487,\n"," '–ø–æ–º–æ–≥–∞–µ—Ç': 488,\n"," '—Å—Ç–µ–Ω–µ': 489,\n"," '–º–µ—Å—Ç–∞': 490,\n"," 'c0vsiy': 491,\n"," '—Ç–µ–ø–µ—Ä—å': 492,\n"," '–º–æ–º–µ–Ω—Ç–∞': 493,\n"," '–ø—Ä–∏–Ω—è—Ç–∏—è': 494,\n"," '–ø—Ä–µ–≤—ã—Å–∏–ª–æ': 495,\n"," '–æ—Ç–ø—Ä–∞–≤–∏–º': 496,\n"," '—Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ': 497,\n"," '—Ç–∞–π–º–µ—Ä': 498,\n"," '–¥–Ω–∏': 499,\n"," '–º–æ–±–∏–ª—å–Ω–æ–µ': 500,\n"," '–ª—é–±–∏–º—ã–º': 501,\n"," '–∫–ª–∏–µ–Ω—Ç–∞–º': 502,\n"," '–ø—Ä–æ–≤–µ—Å—Ç–∏': 503,\n"," '–ø—Ä—è–º–æ': 504,\n"," '—Ü–µ–ª—ã–π': 505,\n"," '–ø–æ–±–µ–¥–∏—Ç–µ–ª—è–º': 506,\n"," '–æ—Ç–ø—Ä–∞–≤–∏–ª–∏': 507,\n"," '–ø—Ä–æ–º–æ–∫–æ–¥–∞–º–∏': 508,\n"," 'üëâ–µ—Å–ª–∏': 509,\n"," '–Ω–∞—à–ª–∏': 510,\n"," '—Å–ø–∏—Å–∫–µ': 511,\n"," '–∑–∞–ø–∞—Å': 512,\n"," '–∞–Ω–Ω–∞': 513,\n"," '–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—ÖüèÜ': 514,\n"," '–∑–∞–∫–∞–∑—ã–≤–∞–π': 515,\n"," '–º–∞—Ä—Ç–∞': 516,\n"," '–≤–æ—Ç': 517,\n"," '–¥–∞–∂–µ': 518,\n"," '—ç—Ç–æ–º': 519,\n"," '—Å–∫–æ–ª—å–∫–æ': 520,\n"," '–≤–∫—É—Å–Ω—ã–º–∏': 521,\n"," '—Å—É–±–±–æ—Ç—ã': 522,\n"," 'üö®': 523,\n"," '–±—É–¥—å—Ç–µ': 524,\n"," '–ø–æ–∂–∞–ª—É–π—Å—Ç–∞': 525,\n"," '—Å–Ω–µ–∂–Ω—ã–π': 526,\n"," '–∫–∞–Ω–∏': 527,\n"," '–ø–æ–±–µ–¥–∏—Ç–µ–ª—å': 528,\n"," '–ª—é–±–∏—Ç–µ–ª–∏': 529,\n"," '–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ': 530,\n"," '48': 531,\n"," '–∑–∞–ø–µ—á—ë–Ω–Ω—ã–π': 532,\n"," '6': 533,\n"," '—ç—Ç–∏': 534,\n"," '–≤–∞—à–µ': 535,\n"," '–≤—ã–±–æ—Ä': 536,\n"," '–ø—Ä–∏–≥–æ—Ç–æ–≤–∏–ª–∏': 537,\n"," '‚úî': 538,\n"," '–∫–ª–∏–µ–Ω—Ç—Å–∫–æ–π': 539,\n"," '–±–∞–∑–µ': 540,\n"," '–ø–æ–¥–∞—Ä–∫–æ–≤': 541,\n"," '—É—á–∞—Å—Ç–≤—É–π—Ç–µ': 542,\n"," '—É—á–∞—Å—Ç–∏–µ': 543,\n"," '–∫–æ–¥–æ–≤–æ–µ': 544,\n"," '—Å–ª–æ–≤–æ': 545,\n"," '–æ—Ç–¥–µ–ª—å–Ω–æ': 546,\n"," '—Ä–∞–∑—ã–≥—Ä–∞–µ–º': 547,\n"," '—Å—Ä–µ–¥–∏': 548,\n"," '—Å–¥–µ–ª–∞–µ—Ç': 549,\n"," '–ø–æ–¥–∞—Ä–∫–∞–º–∏': 550,\n"," '–º–∏–Ω': 551,\n"," '–∞–¥—Ä–µ—Å': 552,\n"," '600': 553,\n"," '¬´–º–Ω–æ–≥–æ': 554,\n"," '–Ω–µ–¥–æ—Ä–æ–≥–æ¬ª': 555,\n"," '–∑–∞–∫–∞–∑—É': 556,\n"," '—Ç—ã': 557,\n"," '–≥—Ä–∞—Ñ–∏–∫': 558,\n"," 'utm': 559,\n"," '–Ω–µ–≥–æ': 560,\n"," '–≤—Ö–æ–¥—è—Ç': 561,\n"," '–æ–≥—É—Ä—Ü–æ–º': 562,\n"," '–µ–¥–µ—Ç': 563,\n"," '—Å—ä–µ—à—å': 564,\n"," '—Ä–∞–∑–¥–µ–ª–∏': 565,\n"," '–Ω–∞—Å–ª–∞–¥–∏—Å—å': 566,\n"," '–±–ª–∏–∑–∫–∏–º–∏': 567,\n"," '–≤—Å–µ–π': 568,\n"," '–ø–æ—Ç–æ–º': 569,\n"," '–∫–∞–∫–æ–π': 570,\n"," '–∫–∞—Ä—Ç–∏–Ω–∫–µ': 571,\n"," '–≤–∫—É—Å': 572,\n"," '—Å–µ—Ç–æ–≤': 573,\n"," '–ø–æ–≤–∞—Ä–∞': 574,\n"," '222693987': 575,\n"," '–≤–∞—Å–∞–±–∏': 576,\n"," '–ø–æ—Ç–æ–º—É': 577,\n"," '–¥–æ–º–∞': 578,\n"," '—Ä–µ—à–∏–ª–∏': 579,\n"," '—á–µ–º': 580,\n"," '—Ä–∞–±–æ—Ç–∞—Ç—å': 581,\n"," '–≤—ã—Ö–æ–¥–Ω–æ–π': 582,\n"," '–ø—Ä–æ–º–æ–∫–æ–¥—ã': 583,\n"," '—Å—É–ø–µ—Ä': 584,\n"," '–Ω–∞—Ä–æ–¥': 585,\n"," '–∞–∫—Ç–∏–≤–Ω–æ': 586,\n"," '–±–ª–∞–≥–æ–¥–∞—Ä–∏–º': 587,\n"," '–¥–µ–ª–∞–µ—Ç': 588,\n"," '–æ–±–µ—â–∞–ª–∏': 589,\n"," '—Ä–∞–Ω–µ–µ': 590,\n"," '—Å–Ω–∏–∂–∞—Ç—å—Å—è': 591,\n"," '–æ–ø–æ–∑–¥–∞–µ–º': 592,\n"," 'ü§î': 593,\n"," '—Ä–∞–±–æ—Ç–∞–µ—Ç': 594,\n"," '–æ—Ñ–æ—Ä–º–ª—è–µ—Ç–µ': 595,\n"," '–∂–¥—ë—Ç–µ': 596,\n"," '–∫—É—Ä—å–µ—Ä–∞': 597,\n"," '–∫—Ä—É—Ç–æ': 598,\n"," '–ø—Ä–∞–≤–¥–∞': 599,\n"," '–∂–µ': 600,\n"," '—Å–æ—á–µ—Ç–∞–Ω–∏–µ': 601,\n"," '279': 602,\n"," '–ø—Ä–µ–¥–ª–∞–≥–∞–µ–º': 603,\n"," '–∫–ª—è—Ä': 604,\n"," '—Å–º–æ–∂–µ—Ç': 605,\n"," '—Ñ–∏–ª—å–º': 606,\n"," '–≤–µ–¥—å': 607,\n"," '000ü§©': 608,\n"," 'üíå–≤—Å–µ–º': 609,\n"," '–±—Ä–∞—Ç—å': 610,\n"," '—Ä–∞–∑–Ω—ã–µ': 611,\n"," '–æ—Ç–¥—ã—Ö–∞': 612,\n"," '–ª—É—á—à–µ': 613,\n"," '¬´—á–∏–∫–µ–Ω': 614,\n"," '–ª–∏—à—å': 615,\n"," '–≤–µ—á–µ—Ä–∞': 616,\n"," '–≤–∞—à–µ–≥–æ': 617,\n"," '–∑–∞—Ä–∞–Ω–µ–µ': 618,\n"," '–ø–æ–¥': 619,\n"," '–≤—Å–µ–≥–¥–∞': 620,\n"," '1000': 621,\n"," '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è': 622,\n"," '–∫—Ä–∞–±–æ–≤–∞—è': 623,\n"," '–ø–∞–ª–æ—á–∫–∞': 624,\n"," '—É–ø—É—Å—Ç–∏—Ç–µ': 625,\n"," '–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å': 626,\n"," '–∑–∞–ø–µ—á–µ–Ω–Ω—ã–µ': 627,\n"," '—Ç—Ä—Ü': 628,\n"," 'id152372762': 629,\n"," '–æ—Ç–≤–µ—Ç—ã': 630,\n"," '–Ω–∏—Ö': 631,\n"," '–≤–∞—à–∏—Ö': 632,\n"," '—Å–∫–∞—á–∞—Ç—å': 633,\n"," '–¥–æ–º': 634,\n"," '–º–æ–≥–ª–∏': 635,\n"," '–Ω–æ—è–±—Ä—è': 636,\n"," '—á–∏–∫–µ–Ω': 637,\n"," '–æ–¥–Ω–æ—Ä–∞–∑–æ–≤—ã–π': 638,\n"," '¬´–ª–∞–π–∫¬ª': 639,\n"," '¬´—Å—ä–µ—à—å': 640,\n"," '–º–µ–Ω—è¬ª': 641,\n"," '—Å–æ–≤–µ—Ç—É–µ–º': 642,\n"," '—Å–µ–Ω—Ç—è–±—Ä—è': 643,\n"," '–±—ã': 644,\n"," '–ø—Ä–µ–¥–∑–∞–∫–∞–∑–æ–≤': 645,\n"," '–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ': 646,\n"," '–±–µ–∑': 647,\n"," '–ø—Ä–∏–∑–∞': 648,\n"," '–ø—É—Å—Ç—å': 649,\n"," '–º–Ω–æ–∂–µ—Å—Ç–≤–æ': 650,\n"," '–≤–∫—É—Å–Ω—ã—Ö': 651,\n"," '–∫–æ–Ω–∫—É—Ä—Å–µ': 652,\n"," '–≤—ã–∏–≥—Ä—ã–≤–∞–π—Ç–µ': 653,\n"," '503': 654,\n"," '–ø—Ä–∏–∑–æ–≤—ã—Ö': 655,\n"," '–æ–±—â–∏–π': 656,\n"," '–≤–µ—Å': 657,\n"," '–ø—Ä–∏–∑–æ–≤': 658,\n"," '–∫–∏–ª–æ–≥—Ä–∞–º–º': 659,\n"," '¬´–ª–∞—Å–∫–æ–≤—ã–π': 660,\n"," '–º–∞–π¬ª': 661,\n"," '—Å—É–º–º—É': 662,\n"," '15': 663,\n"," '9': 664,\n"," 'ü•¢': 665,\n"," '—Å—ã—Ä–æ–º': 666,\n"," '127': 667,\n"," '–ª–∞–≤–∞': 668,\n"," '–ø–æ–¥–±–æ—Ä–∫–∏': 669,\n"," '–∂–∏–∑–Ω—å': 670,\n"," '–±–µ–∫–æ–Ω¬ªüî•–∫–∞–∫': 671,\n"," '–Ω—É–∂–µ–Ω': 672,\n"," '–∫–æ–º–∞–Ω–¥—É': 673,\n"," '–≥–æ—Ä–æ–¥–∞': 674,\n"," 'www': 675,\n"," 'avito': 676,\n"," 'sharing': 677,\n"," '–ª—é–±–∏—Ç–µ': 678,\n"," '–∫–æ–ø—á–µ–Ω—ã–π': 679,\n"," '—Å—è–∫–µ': 680,\n"," '369': 681,\n"," '–¥–æ—Å—Ç–∞–≤–∫–µ': 682,\n"," '–Ω–∞—Ä–æ–¥–Ω–∞—è': 683,\n"," '–º—É–¥—Ä–æ—Å—Ç—å': 684,\n"," '‚òùüèª': 685,\n"," '–∑–∞–≤—Ç—Ä–∞–∫': 686,\n"," '—Å–∞–º': 687,\n"," '–æ–±–µ–¥': 688,\n"," '–¥—Ä—É–≥–æ–º': 689,\n"," '–≤—Ä–µ–º–µ–Ω–µ–º': 690,\n"," '—Å–≤–æ–∏–º–∏': 691,\n"," '–æ—Ç–¥—ã—Ö–∞–ª–∏': 692,\n"," '‚ù§üç£': 693,\n"," '–≤—á–µ—Ä–∞': 694,\n"," '–≤–∫—É—Å–Ω–µ–π—à–∏–µ': 695,\n"," 'ü§≠': 696,\n"," '–Ω–µ–º–Ω–æ–≥–æ': 697,\n"," 'üòÇ': 698,\n"," '–∫–æ–≥–æ': 699,\n"," '—Ä–∞–¥–æ–≤–∞—Ç—å': 700,\n"," '–∑–∞–∫–∞–∑–∞–ª–∏': 701,\n"," '—Å–µ—Ç–æ–º': 702,\n"," '—Å—É–±–±–æ—Ç–∞': 703,\n"," 'üí´': 704,\n"," '—Å–æ–µ–≤—ã–π': 705,\n"," '–∏–º–±–∏—Ä—å': 706,\n"," '–ª–∏–±–æ': 707,\n"," '–º–æ–∂–µ—Ç–µ': 708,\n"," '–∏–¥–µ–∞–ª—å–Ω—ã–π': 709,\n"," '–ª–µ–≥–∫–æ–≥–æ': 710,\n"," '–±—É–¥–µ—Ç–µ': 711,\n"," '–ø—Ä–∞–∑–¥–Ω–∏–∫–∏': 712,\n"," '–Ω–æ–≤–æ–º—É': 713,\n"," '—Å–∞–º–æ–µ': 714,\n"," 'üìÜ': 715,\n"," '–æ–±—ã—á–Ω–æ–º': 716,\n"," '—Ä–µ–∂–∏–º–µ': 717,\n"," '17': 718,\n"," '–¥–æ—Å—Ç–∞–≤–ª—è–µ–º': 719,\n"," '–ø—Ä–∏—ë–º': 720,\n"," '–∑–∞–∫—Ä—ã—Ç': 721,\n"," '–¥–≤–∞': 722,\n"," '—Å–ø–æ—Å–æ–±–Ω—ã': 723,\n"," '–≥–æ–ª–æ–¥': 724,\n"," '¬´–º–µ—Ö–∏–∫–æ¬ª': 725,\n"," '–æ—Å—Ç–∞–≤–ª—è–µ—Ç': 726,\n"," '‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî': 727,\n"," '–ø—Ä–∏—è—Ç–Ω–∞—è': 728,\n"," '–ø—Ä–∏—á–∏–Ω–∞üôÇ': 729,\n"," '–¥–∞': 730,\n"," '–æ–¥–Ω–æ–π': 731,\n"," '—Å—Ç–æ—Ä–æ–Ω—ã': 732,\n"," '–∫–ª–∏–µ–Ω—Ç': 733,\n"," '–≤—ã—Ä–∞–∂–∞–µ—Ç': 734,\n"," '–ø–æ–≤–∞—Ä–∞–º': 735,\n"," '–ø–µ—Ä–µ–¥–∞—ë—Ç': 736,\n"," '–ø—Ä–∏–≤–µ—Ç': 737,\n"," '–∫–æ–º–∞–Ω–¥–µ': 738,\n"," '–¥—Ä—É–≥–∏–º': 739,\n"," '–ø–æ–¥–ø–∏—Å—á–∏–∫–∞–º': 740,\n"," '–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å—Å—è': 741,\n"," '–≤—ã–±–æ—Ä–æ–º': 742,\n"," '—Ç–µ–∫—Å—Ç–æ–≤–æ–º': 743,\n"," '—Ñ–æ—Ä–º–∞—Ç–µ': 744,\n"," '–∑–∞–º–æ—Ä–∞—á–∏–≤–∞–µ—Ç—Å—è': 745,\n"," '–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ': 746,\n"," '–æ—Å—Ç–∞–≤–ª—è–µ—Ç–µ': 747,\n"," '—Ü–µ–∑–∞—Ä—åüòçüî•': 748,\n"," '–≤–∑–∞–∏–º–Ω—ã–π': 749,\n"," '–¥–æ–≤–æ–ª—å–Ω—ã': 750,\n"," '–ø–æ–ª—å–∑—É–π—Ç–µ—Å—å': 751,\n"," '–º–æ–º–µ–Ω—Ç–æ–º': 752,\n"," '—Å–≤–∏–¥–∞–Ω–∏–µ': 753,\n"," '–º–æ–≥—É—Ç': 754,\n"," '–¥–æ–≤–æ–ª—å–Ω–æ': 755,\n"," '–Ω–µ–º': 756,\n"," '–Ω–µ—Ç': 757,\n"," '–Ω–∞–º–∏': 758,\n"," '—Å–æ–≥–ª–∞—Å–∏—Ç–µ—Å—å': 759,\n"," '–ø–∞—Ä–∞': 760,\n"," '—Å—Ç–æ–ª–µ': 761,\n"," '–ª—É—á—à–∏–π': 762,\n"," '–∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–º–∏': 763,\n"," '–∫–∞—á–µ—Å—Ç–≤–æ': 764,\n"," '—Å–≤–µ—Ç–ª–∞–Ω–∞': 765,\n"," '‚ö†': 766,\n"," '–∑–∞–º–µ—Ç–∏–ª–∏': 767,\n"," '–ø–æ—Ä–∞–¥—É–µ—Ç': 768,\n"," '–≤–∫—É—Å–æ–≤—ã–µ': 769,\n"," '—Å–æ—Å–æ—á–∫–∏': 770,\n"," '–Ω–æ–≤–æ–≥–æ–¥–Ω–∏–µ': 771,\n"," '‚ùÑ': 772,\n"," '—Ç–æ–º': 773,\n"," '—Å–æ–≤—Å–µ–º': 774,\n"," '–≤–æ–ª—à–µ–±–Ω—ã–π': 775,\n"," '–æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è': 776,\n"," '–æ–∂–∏–¥–∞–Ω–∏–µ': 777,\n"," '–∑–∞–ø–∞—Å–∞': 778,\n"," '—Ç–∞—Ç—å—è–Ω–∞': 779,\n"," '–∫—Å–µ–Ω–∏—è': 780,\n"," '–∂–¥–∞–Ω–æ–≤–∞': 781,\n"," '–ø–æ–±–µ–¥–∏—Ç–µ–ª–∏': 782,\n"," '–∏–¥–µ–∞–ª—å–Ω–æ–µ': 783,\n"," '–ª–æ—Å–æ—Å—è': 784,\n"," '–æ–≥—É—Ä–µ—Ü': 785,\n"," '—Ö–æ—Ç–∏—Ç–µ': 786,\n"," '—É–Ω–∞–≥–∏': 787,\n"," '—É–≥–æ—Ä—å': 788,\n"," '–ø–∞–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ': 789,\n"," '—Å—É—Ö–∞—Ä–∏': 790,\n"," '–∫—É–Ω–∂—É—Ç': 791,\n"," '–ø—Ä–æ–≤–µ–¥–∏—Ç–µ': 792,\n"," '–ø–æ–¥–≥–æ—Ç–æ–≤–∏–ª–∏': 793,\n"," '–∫–æ—Ç–æ—Ä—ã–µ': 794,\n"," '–ø—Ä–∞–∑–¥–Ω–∏—á–Ω–æ–µ': 795,\n"," '—Ä–æ–∂–¥–µ—Å—Ç–≤–∞': 796,\n"," '–≥—Ä–∏–Ω—á': 797,\n"," '—Ä–æ–∂–¥–µ—Å—Ç–≤–æ': 798,\n"," '–Ω–∞—Å–ª–∞–∂–¥–∞–π—Ç–µ—Å—å': 799,\n"," '–≤–º–µ—Å—Ç–µ': 800,\n"," '–¥—Ä—É–∑—å—è–º–∏': 801,\n"," '–ø—Ä–∞–∑–¥–Ω–∏—á–Ω—ã–π': 802,\n"," 'üòã': 803,\n"," '–Ω–∞—Å–ª–∞–¥–∏—Ç—å—Å—è': 804,\n"," '–¥–µ–π—Å—Ç–≤—É—é—Ç': 805,\n"," '–∞–ª–µ–∫—Å–µ–π': 806,\n"," '–∫—Ä–∏—Å—Ç–∏–Ω–∞': 807,\n"," '–ø—Ä–∏—Ä–æ–¥—É': 808,\n"," '–≤—ã–ª–∞–∑–∫–∏': 809,\n"," 'üôÇ': 810,\n"," '–æ–±—ã—á–Ω–æ': 811,\n"," '—Ç–∞–∫–∏–µ': 812,\n"," '—à–∞—à–ª—ã–∫–∏': 813,\n"," 'üëâ—à–∞—à–ª—ã–∫–∏': 814,\n"," '—Å—Ç–∞–ª–∏': 815,\n"," '—Å—Ç–æ–ª': 816,\n"," '–≥–æ–¥': 817,\n"," '—Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫–µ': 818,\n"," '—É–¥–æ–±–Ω–æ': 819,\n"," '–Ω–∞–¥–æ–ª–≥–æ': 820,\n"," '—Ä–∞–Ω—á¬ªüòç': 821,\n"," '–Ω–∞–π–¥–µ—Ç–µ': 822,\n"," '–æ—Ç–ª–∏—á–Ω–æ': 823,\n"," '‚ùÑÔ∏è': 824,\n"," '–¥–µ–ª–∏—Ç—å—Å—è': 825,\n"," '–≤—ã–ø–æ–ª–Ω–∏—Ç—å': 826,\n"," '–¥–µ–ª–∏—Ç–µ—Å—å': 827,\n"," '–æ—Ç–¥–æ—Ö–Ω—É—Ç—å': 828,\n"," '–¥–∏–≤–∞–Ω–µ': 829,\n"," '–ª—É—á—à–∏–µ': 830,\n"," '–¥–æ—Å—Ç–∞–≤–∏—Ç—å': 831,\n"," '–æ–±—ä–µ–¥–∏–Ω—è–µ—Ç': 832,\n"," '–º–µ–Ω—éüëâ': 833,\n"," '–æ—Ç–≤–µ—Ç': 834,\n"," '–≤–µ—Å—å': 835,\n"," '–≤–∞–∂–Ω–∞—è': 836,\n"," '–∫–æ–º–ø–∞–Ω–∏–∏': 837,\n"," '–º–æ—à–µ–Ω–Ω–∏–∫–∏': 838,\n"," '–ø–æ–º–æ–≥–∏—Ç–µ': 839,\n"," '—Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏': 840,\n"," '—Å—Ä–µ–¥–Ω–µ–º': 841,\n"," '–∫—É—Ä—å–µ—Ä': 842,\n"," '–æ—Ç–ª–∏—á–Ω—ã–π': 843,\n"," '–ø–æ–≤–æ–¥': 844,\n"," '–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ': 845,\n"," '–∞–±—Å–æ–ª—é—Ç–Ω–æ': 846,\n"," '¬´413¬ª': 847,\n"," '¬´813¬ª': 848,\n"," '—Å–∞–º–æ–º': 849,\n"," '—Ä–∞–∑–≥–∞—Ä–µ': 850,\n"," '–∑–¥–µ—Å—å': 851,\n"," '–æ—Å—Ç—Ä–µ–Ω—å–∫–æ–≥–æ': 852,\n"," '–ø–æ–Ω—Ä–∞–≤—è—Ç—Å—è': 853,\n"," '–≥—É—Ä–º–∞–Ω–∞–º': 854,\n"," '–µ–¥—É—Ç': 855,\n"," '¬´—Å—ã—Ä–Ω—ã–π': 856,\n"," '–∏–º–∏': 857,\n"," '–ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å': 858,\n"," '–ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å': 859,\n"," '—Å—Ç–∞–≤—å': 860,\n"," '–ø—É–Ω–∫—Ç': 861,\n"," '–ø—Ä–æ–≤–µ—Ä–∏–º': 862,\n"," '–∑–Ω–∞–Ω–∏—èü§î': 863,\n"," '–¥–æ–≥–∞–¥–∞—Ç—å—Å—è': 864,\n"," '–≤–∞—Ä–∏–∞–Ω—Ç—ã': 865,\n"," '—Å–æ–æ–±—â–∏–º': 866,\n"," '–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ': 867,\n"," '—Å–µ—Ç—ã': 868,\n"," '–ø—Ä–æ–≥–æ–ª–æ—Å—É–π—Ç–µ': 869,\n"," '–æ–ø—Ä–æ—Å–µ': 870,\n"," '–Ω–∏–º–∏': 871,\n"," '–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å': 872,\n"," '—Ä–∞–¥–æ—Å—Ç—å': 873,\n"," '–º–∏—Å—Å–∏—è': 874,\n"," 'üòé': 875,\n"," '—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º': 876,\n"," '–æ—Ç–¥–∞—Ç—å': 877,\n"," '—Ö–ª–æ–ø–æ—Ç—ã': 878,\n"," '–ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ–º': 879,\n"," '–µ–¥—ã': 880,\n"," '—Å–µ–º—å–∏': 881,\n"," '‚ù§\\u200düëç': 882,\n"," '—Å–≤–æ–µ–π': 883,\n"," '—Ç–∞–∫–æ–≥–æ': 884,\n"," '–ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–º–∏': 885,\n"," '—è—Ä–∫–∏–º–∏': 886,\n"," '–Ω–∞—Å—ã—â–µ–Ω–Ω—ã—Ö': 887,\n"," '–Ω–∞–¥–µ–∂–¥–∞': 888,\n"," '–ª–∞—Ä–∏—Å–∞': 889,\n"," '–∞–Ω–∞—Å—Ç–∞—Å–∏—è': 890,\n"," '–µ–∫–∞—Ç–µ—Ä–∏–Ω–∞': 891,\n"," '–ø–æ—Ä–∞–¥–æ–≤–∞—Ç—å': 892,\n"," '–ø—Ä–æ–≤–µ—Ä–∏—Ç—å': 893,\n"," '—Ñ–∞–∫—Ç': 894,\n"," '–Ω–∞—à–µ–≥–æ': 895,\n"," '1Ô∏è‚É£': 896,\n"," 'üí™': 897,\n"," '—Ç–∞–∫–æ–µ': 898,\n"," '–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ': 899,\n"," '–¥–æ–≤–µ—Ä–∏–µ': 900,\n"," '–ø–æ–ª—É—á–∏—Ç–µ': 901,\n"," '–º–∏–∫—Å': 902,\n"," '276150': 903,\n"," 'üòÖ': 904,\n"," '–≤–∫': 905,\n"," '¬´–∞—Ä—Ä–∏–≤–∞¬ªü§åüèº–∫–∞–∫': 906,\n"," '—Ä–æ–ª–ª–µ': 907,\n"," '—Ç–∏–≥—Ä–æ–≤–∞—è': 908,\n"," '–∑–∞–ø–µ—á–µ–Ω–Ω—ã–π': 909,\n"," '–ª–∏—à–Ω–µ–≥–æ': 910,\n"," '–¥–æ–±–∞–≤–∏—Ç—å': 911,\n"," '–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞': 912,\n"," '—É–∫—Ä–∞—Å–∏—Ç—å': 913,\n"," '–Ω–æ–≤–æ–≥–æ–¥–Ω–∏–π': 914,\n"," 'cciqmv': 915,\n"," '–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ': 916,\n"," '—Ç–∞–∫–∂–µ': 917,\n"," '–æ–Ω–æ': 918,\n"," '–≤—ã–±–∏—Ä–∞—é—Ç': 919,\n"," '–∫–ª–∏–µ–Ω—Ç–æ–≤': 920,\n"," '—Ä–æ–ª–ª–∞—Ö': 921,\n"," '–Ω–∞–ø—Ä–∏–º–µ—Ä': 922,\n"," '–±—ã—Å—Ç—Ä—É—é': 923,\n"," '–ø—Ä–æ—à–ª–æ–≥–æ': 924,\n"," 'üòº': 925,\n"," '–≤–∞–ª–µ—Ä–∞': 926,\n"," '¬´–ø–ª–∞–Ω–µ—Ç–∞¬ª': 927,\n"," '—Å–ø–µ—à–∏—Ç–µ': 928,\n"," '—è–ø–æ–Ω—Å–∫–æ–π': 929,\n"," '–∫—É—Ö–Ω–∏': 930,\n"," '–ª—é–±–æ–≤—å': 931,\n"," '–ø–æ–º–æ–∂–µ–º': 932,\n"," '–≤–∫—É—Å–Ω–æ–π': 933,\n"," '128': 934,\n"," '–Ω–∏–±—É–¥—å': 935,\n"," '—Ä–∏—Å–æ–º': 936,\n"," '–Ω–∞—Å—Ç—É–ø–∞—é—â–∏–º': 937,\n"," '—Å—á–∞—Å—Ç–ª–∏–≤—ã': 938,\n"," '–∑–¥–æ—Ä–æ–≤—ã': 939,\n"," '—É–ª—ã–±–∞–π—Ç–µ—Å—å': 940,\n"," '—Ä–∞–¥—É–π—Ç–µ—Å—å': 941,\n"," '–º–µ–ª–æ—á–∞–º': 942,\n"," '—Å–∞–º—ã–º': 943,\n"," '–±–æ–ª—å—à–∏–º': 944,\n"," '–æ–≥–æ—Ä—á–µ–Ω–∏–µ–º': 945,\n"," '–Ω–æ–≤–æ–º': 946,\n"," '2024': 947,\n"," '–æ–ø–æ–∑–¥–∞–ª–∏': 948,\n"," '–ª–µ—Ç–æ': 949,\n"," '–¥–µ–ª–∞—Ç—å': 950,\n"," '–¥–µ–ª–∞–π—Ç–µ': 951,\n"," '‚òÄ': 952,\n"," '20': 953,\n"," '–æ–∫—Ç—è–±—Ä—è': 954,\n"," '125': 955,\n"," '–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ': 956,\n"," 'ozon': 957,\n"," 'üòá': 958,\n"," '–∫–∞–Ω–∏¬ª': 959,\n"," '‚ù§Ô∏è': 960,\n"," '–ª—é–±–∏–º—É—é': 961,\n"," '–ª—é–∫—Å': 962,\n"," '—Å—é–¥–∞': 963,\n"," '–≤—Å–ø–æ–º–Ω–∏—Ç—å': 964,\n"," '–∞–Ω—Ç–æ–Ω': 965,\n"," '299': 966,\n"," '–±–µ–∫–æ–Ω': 967,\n"," '–≤–Ω–∏–º–∞–Ω–∏–µ': 968,\n"," '–ø–æ–≤–∞—Ä': 969,\n"," '–æ–±—É—á–µ–Ω–∏—è': 970,\n"," '—á–∞—Å': 971,\n"," '–∫–æ–Ω—Å—Ç–∞–Ω—Ç–∏–Ω': 972,\n"," '–≤–∫–ª—é—á–∞–π—Ç–µ': 973,\n"," '—Å–µ—Ä–∏–∞–ª': 974,\n"," 'üé¨': 975,\n"," '—Ç—Ä–∏–≥–≥–µ—Ä': 976,\n"," '–∫–æ—à–∫–∞': 977,\n"," '–≤–∞–º–ø–∏—Ä—ã': 978,\n"," '—Å—Ä–µ–¥–Ω–µ–π': 979,\n"," '–ø–æ–ª–æ—Å—ã': 980,\n"," '–∂–∏—Ç—å': 981,\n"," '–±–µ—Å–ø—Ä–∏–Ω—Ü–∏–ø–Ω—ã–µ': 982,\n"," 'id162688142': 983,\n"," '–ø—Ä–æ–∫–æ—Ñ—å–µ–≤–∞': 984,\n"," 'üíØüëáüèª': 985,\n"," '–∏—â–µ–º': 986,\n"," '–∫—É—Ä—å–µ—Ä–æ–≤': 987,\n"," '–æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤': 988,\n"," '–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–æ–≤': 989,\n"," '—Å–º–µ–Ω—ã': 990,\n"," '—Å—É—à–∏—Å—Ç–æ–≤': 991,\n"," '—Ñ–∏–ª–∏–∞–ª—ã': 992,\n"," '–ø–µ—Ä–º–∏': 993,\n"," '—Ñ–∏–ª–∏–∞–ª': 994,\n"," '–∫–∞–∂–¥–æ–º': 995,\n"," '—Ä–∞–π–æ–Ω–µ': 996,\n"," '–≥–∏–±–∫–∏–π': 997,\n"," '—Ä–∞–±–æ—Ç–∞': 998,\n"," '—Ä—è–¥–æ–º': 999,\n"," '–¥–æ–º–æ–º': 1000,\n"," ...}"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ–ø–∏—Å–∞–Ω–∏—è –≤ —á–∏—Å–ª–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ"],"metadata":{"id":"8DCJ3To8D5PC"}},{"cell_type":"code","source":["sequences = tokenizer.texts_to_sequences(descrs)\n"],"metadata":{"id":"SxFnbO0YEAO5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["index = 1\n","#print(descrs[index])\n","#print(sequences[index])\n","descrs"],"metadata":{"id":"I5Hoe_W7F0DN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711866952233,"user_tz":-300,"elapsed":75,"user":{"displayName":"Andrey Sokolov","userId":"00506076773780018870"}},"outputId":"86c6b74a-5f52-479b-dda3-015e200ee1f0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0      –í–∫—É—Å–Ω–æ...#–æ—Ç–∑—ã–≤—ã@tommyfish\\n–ë–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö —Ä–æ–ª–ª–æ–≤ ...\n","1      –î—Ä—É–∑—å—è, –∑–∞ –ø—Ä–æ—à–ª—É—é –Ω–µ–¥–µ–ª—é —Å–æ–±—Ä–∞–ª–æ—Å—å –º–µ–Ω–µ–µ 30 —Ñ...\n","2      –ó–∞–∫–∞–∑—ã–≤–∞–π—Ç–µ –¥–æ—Å—Ç–∞–≤–∫—É –∏ –≤–∫–ª—é—á–∞–π—Ç–µ —Å–µ—Ä–∏–∞–ª –∏–∑ –Ω–∞—à...\n","3      –°–∫–æ—Ä–æ –î–µ–Ω—å —Ä–æ–∂–¥–µ–Ω–∏—è? \\n–ê –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–Ω —É–∂–µ —Å–µ–≥...\n","4      –ü–æ–¥–≤–æ–¥–∏–º –∏—Ç–æ–≥–∏ –∫–æ–Ω–∫—É—Ä—Å–∞ —Ñ–æ—Ç–æ-–æ—Ç–∑—ã–≤–æ–≤üì∏ –ü–æ–∑–¥—Ä–∞–≤–ª...\n","                             ...                        \n","196    nan\\n- –û—Ç—á–µ—Ç –∑–∞ 01.05.2023 ‚úÖ\\n\\n–ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –ø—Ä...\n","197                                                  nan\n","198    –î—Ä—É–∑—å—è, –∑–∞ –ø—Ä–æ—à–ª—É—é –Ω–µ–¥–µ–ª—é —Å–æ–±—Ä–∞–ª–æ—Å—å –º–µ–Ω–µ–µ 30 —Ñ...\n","199    –í–Ω–∏–º–∞–Ω–∏–µ, –ª—é–±–∏—Ç–µ–ª–∏ —Å—É—à–∏!\\n\\n–î–∞–≤–∞–π—Ç–µ –≤—ã–±–µ—Ä–µ–º –ª—é...\n","200    –ë–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö —Ä–æ–ª–ª–æ–≤ –º–Ω–æ–≥–æ –Ω–µ –±—ã–≤–∞–µ—Ç!\\n–î–∞—Ä–∏–º –Ω–∞—à–∏...\n","Name: –¢–µ–∫—Å—Ç, Length: 201, dtype: object"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["tokenizer.word_index['–¥–∞—Ä–∏–º']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IL-_HFAYT6SW","executionInfo":{"status":"ok","timestamp":1711866952738,"user_tz":-300,"elapsed":514,"user":{"displayName":"Andrey Sokolov","userId":"00506076773780018870"}},"outputId":"e3c1aa23-34fc-4add-a258-70fcb1dca2d2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["23"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["#–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–∞–∫—Å –¥–ª–∏–Ω—ã\n","max1 = 0\n","for i in sequences:\n","  if len(i) > max1: max1 = len(i)\n","max1"],"metadata":{"id":"L_Jgsf4YN8Mr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711866952738,"user_tz":-300,"elapsed":28,"user":{"displayName":"Andrey Sokolov","userId":"00506076773780018870"}},"outputId":"7d070551-2a76-45c8-c68f-3e7e24046888"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["300"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["for i in range(0, len(sequences)):\n","  print(i, '\\t', sequences[i])"],"metadata":{"id":"fhFv4RhrP4J4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711866952738,"user_tz":-300,"elapsed":26,"user":{"displayName":"Andrey Sokolov","userId":"00506076773780018870"}},"outputId":"88fee7ea-673d-466f-c27d-aa01d45a5401"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0 \t [300, 284, 175, 127, 30, 99, 9, 100, 23, 101, 53, 38, 6, 17, 29, 11, 48, 43, 149, 15, 54, 150, 40, 12, 13, 27, 2, 12, 13, 44, 55, 32, 161, 33, 16, 56, 2, 141, 59, 24, 5, 18, 26]\n","1 \t [151, 22, 245, 132, 246, 202, 74, 10, 66, 247, 248, 89, 249, 31, 46, 10, 66, 128, 3, 90, 250, 218, 71, 102, 8, 35, 102, 142, 219, 59, 10, 176, 49, 220, 221, 8, 3, 10, 222, 58, 223, 190, 224, 8, 31, 191, 225, 152, 95, 8, 39, 1, 226, 203, 227, 204, 74, 66, 83, 31, 128, 3, 90, 95, 8, 15, 4, 205, 34, 75, 228, 1, 103, 104, 8, 206, 10, 229, 1, 153, 68, 230, 231, 207, 8, 232, 154, 208, 17, 233, 234, 192, 14, 209, 138, 46, 6, 235, 167, 21, 236, 2, 237, 238, 193, 91, 50, 109, 19, 84, 58, 72, 45, 52, 60, 110, 47, 111, 96, 1, 92, 112, 2, 105, 19, 69, 113, 106, 114, 107, 115, 23, 21, 53, 38, 6, 17, 116, 29, 11, 48, 117, 118, 119, 1, 81, 76, 77, 61, 4, 70, 120, 36, 4, 121, 15, 54, 82, 40, 12, 13, 27, 2, 12, 13, 44, 55]\n","2 \t [264, 390, 2, 973, 974, 89, 265, 669, 975, 133, 976, 155, 977, 12, 978, 979, 980, 177, 981, 670, 178, 982]\n","3 \t [91, 50, 109, 19, 84, 58, 72, 45, 52, 60, 110, 47, 111, 96, 1, 92, 112, 2, 105, 19, 69, 113, 106, 114, 107, 115, 23, 21, 53, 38, 6, 17, 116, 29, 11, 48, 117, 118, 119, 1, 81, 76, 77, 61, 4, 70, 120, 36, 4, 121, 15, 54, 82, 40, 12, 13, 27, 2, 12, 13, 44, 55]\n","4 \t [122, 31, 46, 10, 143, 129, 210, 93, 14, 983, 452, 984, 139, 6, 162, 78, 21, 108, 123, 156, 157, 71, 158, 7, 5, 20, 144, 51, 168, 64, 266, 3, 251, 124, 267, 45, 187, 15, 3, 179, 62, 57, 268, 1, 47, 163, 16, 94, 34, 75, 269, 30, 391, 671, 3, 10, 39, 37, 164, 9, 194, 270, 132, 211, 11, 134, 135, 8, 271, 1, 124, 272, 180, 8, 273, 34, 239, 7, 5, 20, 274, 51, 252, 275]\n","5 \t [557, 672, 85, 1, 673, 985, 986, 987, 988, 989, 990, 2, 991, 1, 992, 169, 170, 1, 993, 994, 1, 995, 996, 674, 997, 558, 316, 998, 999, 4, 1000, 2, 1001, 1002, 1003, 1004, 1005, 16, 56, 2, 25, 1006, 4, 1007, 7, 675, 676, 192, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 559, 1016, 1017, 559, 1018, 1019, 1020, 1021, 559, 1022, 1023, 677]\n","6 \t [91, 50, 109, 19, 84, 58, 72, 45, 52, 60, 110, 47, 111, 96, 1, 92, 112, 2, 105, 19, 69, 113, 106, 114, 107, 115, 23, 21, 53, 38, 6, 17, 116, 29, 11, 48, 117, 118, 119, 1, 81, 76, 77, 61, 4, 70, 120, 36, 4, 121, 15, 54, 82, 40, 12, 13, 27, 2, 12, 13, 44, 55]\n","7 \t [1024, 57, 1025, 1026]\n","8 \t [165, 392, 52, 350, 50, 317, 351, 3, 301, 302, 11, 393, 394, 72, 395, 1, 62, 42, 391, 1027, 136, 352, 25, 35, 63, 318, 396, 41, 397, 398, 41, 1, 399, 159, 2, 64, 400, 52, 2, 64, 68, 353, 3, 401, 171, 285, 5, 20, 65, 51, 91, 50, 109, 19, 84, 58, 72, 45, 52, 60, 110, 47, 111, 96, 1, 92, 112, 2, 105, 19, 69, 113, 106, 114, 107, 115, 23, 21, 53, 38, 6, 17, 116, 29, 11, 48, 117, 118, 119, 1, 81, 76, 77, 61, 4, 70, 120, 36, 4, 121, 15, 54, 82, 40, 12, 13, 27, 2, 12, 13, 44, 55]\n","9 \t [78, 22, 1028, 2, 286, 354, 319, 284, 175, 127, 30, 99, 9, 100, 23, 101, 53, 38, 6, 17, 29, 11, 48, 43, 149, 15, 54, 150, 40, 12, 13, 27, 2, 12, 13, 44, 55, 32, 253, 303, 33, 16, 56, 2, 141, 59, 24, 5, 18, 26]\n","10 \t [453, 57, 85, 2, 454, 287, 195, 151, 212, 41, 137, 79, 40, 82, 22, 355, 145, 23, 287, 196, 21, 356, 64, 455, 57, 85, 22, 357, 358, 456, 188, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 402, 470, 304, 471, 472, 2, 197, 24, 25, 63, 180, 181, 182, 7, 5, 20, 65, 146]\n","11 \t [23, 42, 22, 320, 147, 32, 86, 75, 59, 62, 253, 303, 33, 16, 56, 5, 18, 26, 253, 1029, 359, 15, 360, 253, 1030, 42, 1031, 61, 4, 361, 1, 62, 21, 30, 195]\n","12 \t [91, 50, 109, 19, 84, 58, 72, 45, 52, 60, 110, 47, 111, 96, 1, 92, 112, 2, 105, 19, 69, 113, 106, 114, 107, 115, 23, 21, 53, 38, 6, 17, 116, 29, 11, 48, 117, 118, 119, 1, 81, 76, 77, 61, 4, 70, 120, 36, 4, 121, 15, 54, 82, 40, 12, 13, 27, 2, 12, 13, 44, 55]\n","13 \t [151, 22, 245, 132, 246, 202, 74, 10, 66, 247, 248, 89, 249, 31, 46, 10, 66, 128, 3, 90, 250, 218, 71, 102, 8, 35, 102, 142, 219, 59, 10, 176, 49, 220, 221, 8, 3, 10, 222, 58, 223, 190, 224, 8, 31, 191, 225, 152, 95, 8, 39, 1, 226, 203, 227, 204, 74, 66, 83, 31, 128, 3, 90, 95, 8, 15, 4, 205, 34, 75, 228, 1, 103, 104, 8, 206, 10, 229, 1, 153, 68, 230, 231, 207, 8, 232, 154, 208, 17, 233, 234, 192, 14, 209, 138, 46, 6, 321, 362, 167, 21, 236, 2, 237, 238, 193]\n","14 \t [678, 679, 305, 322, 36, 403, 680, 4, 1032, 213, 323, 35, 63, 1, 560, 561, 473, 403, 473, 254, 240, 473, 363, 473, 305, 679, 80, 264, 1, 241, 22, 681, 43, 7, 5, 18, 26]\n","15 \t [1033, 1034, 1035, 38, 36, 4, 213, 2, 562, 364, 1036, 22, 324, 73, 212, 41, 17, 682, 276, 255, 73, 4, 79, 6, 35, 325, 1, 62, 172, 49, 33, 22, 1037, 43, 7, 5, 18, 26, 41, 68, 39, 474, 563, 276, 255, 73, 159, 475, 4, 79, 6, 35, 325, 1, 62, 189, 80, 256, 24, 49, 33, 7, 5, 18, 26]\n","16 \t [683, 684, 685, 686, 564, 687, 688, 565, 4, 689, 19, 3, 319, 172, 42, 2, 566, 690, 365, 691, 567, 25, 288, 140, 37, 692, 693, 694, 1038, 1039, 1040, 1, 1041, 568, 1042, 19, 569, 1043, 2, 1044, 1045, 3, 695, 42, 78, 169, 170]\n","17 \t [1046, 570, 89, 1047, 4, 1048, 1049, 571, 1050, 696, 326, 41, 68, 39, 474, 563, 276, 255, 73, 159, 475, 4, 79, 6, 35, 325, 1, 62, 189, 80, 256, 24, 49, 33, 7, 5, 18, 26]\n","18 \t [30, 99, 9, 100, 327, 1, 169, 170, 35, 1051, 1052, 5, 18, 26, 253, 91, 50, 109, 19, 84, 58, 72, 45, 52, 60, 110, 47, 111, 96, 1, 92, 112, 2, 105, 19, 69, 113, 106, 114, 107, 115, 23, 21, 53, 38, 6, 17, 116, 29, 11, 48, 117, 118, 119, 1, 81, 76, 77, 61, 4, 70, 120, 36, 4, 121, 15, 54, 82, 40, 12, 13, 27, 2, 12, 13, 44, 55]\n","19 \t [697, 1053, 1, 1054, 1055, 698, 1056, 137, 699, 476, 165, 151, 22, 245, 132, 246, 202, 74, 10, 66, 247, 248, 89, 249, 31, 46, 10, 66, 128, 3, 90, 250, 218, 71, 102, 8, 35, 102, 142, 219, 59, 10, 176, 49, 220, 221, 8, 3, 10, 222, 58, 223, 190, 224, 8, 31, 191, 225, 152, 95, 8, 39, 1, 226, 203, 227, 204, 74, 66, 83, 31, 128, 3, 90, 95, 8, 15, 4, 205, 34, 75, 228, 1, 103, 104, 8, 206, 10, 229, 1, 153, 68, 230, 231, 207, 8, 232, 154, 208, 17, 233, 234, 192, 14, 209, 138, 46, 6, 235, 167, 21, 236, 2, 237, 238, 193, 159, 25, 1057, 697, 86, 1058, 698]\n","20 \t [122, 31, 46, 10, 143, 129, 210, 93, 14, 1059, 1060, 1061, 139, 6, 162, 78, 21, 108, 123, 156, 157, 71, 158, 7, 5, 20, 144, 51, 168, 572, 328, 1062, 700, 63, 1, 47, 95, 97, 572, 366, 573, 289, 98, 1063, 37, 701, 329, 183, 1064, 6, 152, 1065, 1066, 1067, 19, 11, 330, 1068, 1069, 1070, 1071, 404, 306, 1072, 307, 52, 358, 702, 16, 56, 214, 7, 5, 20, 65, 146]\n","21 \t [703, 1073, 160, 140, 1074, 307, 1075, 215, 32, 188, 574, 1076, 159, 1077, 140, 1078, 183, 50, 1, 1079, 1080, 1081, 19, 1082, 1083, 165, 405, 41, 37, 2, 9, 1084, 1085, 1086, 704, 197, 24, 277, 22, 177, 1087, 1088, 5, 20, 65, 575]\n","22 \t [290, 705, 278, 706, 2, 576, 9, 561, 1, 81, 573, 2, 30, 14, 577, 41, 25, 9, 1089, 140, 37, 1090, 9, 152, 1091, 576, 707, 706, 19, 137, 699, 83, 578, 84, 1092, 705, 278, 2, 1093, 9, 367, 353, 330, 181, 356, 9, 202, 1094, 1095, 37, 708, 1096, 137, 1097, 579, 580, 1098, 52, 1099, 331, 16, 56, 2, 197, 24, 214, 7, 5, 20, 65, 146]\n","23 \t [363, 198, 2, 184, 189, 38, 36, 4, 562, 709, 35, 710, 1100, 291, 477, 9, 478, 479, 160, 166, 308, 324, 73, 172, 49, 147, 22, 1101, 43, 7, 5, 18, 26]\n","24 \t [67]\n","25 \t [19, 37, 711, 581, 1, 712, 368, 580, 1102, 57, 713, 406, 356, 480, 1103, 1104, 1105, 1, 103, 332, 714, 160, 1106, 69, 481, 1107, 316, 715, 87, 74, 199, 288, 1, 716, 717, 87, 257, 199, 288, 27, 718, 258, 719, 302, 27, 407, 74, 87, 133, 482, 582, 720, 308, 721, 87, 365, 155, 16, 369, 482, 288, 4, 408, 258, 27, 304, 258, 216, 257, 199, 583, 2, 309, 581, 9, 191, 701, 1, 292, 163, 159, 584, 1108, 405, 2, 286, 286, 300, 27, 333, 370, 409, 178, 483, 1109]\n","26 \t [334, 722, 1110, 335, 1111, 97, 279, 6, 1112, 1113, 1114, 1115, 30, 723, 1116, 410, 1117, 724, 336, 253, 303, 33, 16, 56, 2, 172, 22, 1118, 43, 5, 18, 26]\n","27 \t [151, 22, 245, 132, 246, 202, 74, 10, 66, 247, 248, 89, 249, 31, 46, 10, 66, 128, 3, 90, 250, 218, 71, 102, 8, 35, 102, 142, 219, 59, 10, 176, 49, 220, 221, 8, 3, 10, 222, 58, 223, 190, 224, 8, 31, 191, 225, 152, 95, 8, 39, 1, 226, 203, 227, 204, 74, 66, 83, 31, 128, 3, 90, 95, 8, 15, 4, 205, 34, 75, 228, 1, 103, 104, 8, 206, 10, 229, 1, 153, 68, 230, 231, 207, 8, 232, 154, 208, 17, 233, 234, 192, 14, 209, 138, 46, 6, 235, 167, 21, 236, 2, 237, 238, 193, 64, 266, 3, 251, 124, 267, 45, 187, 15, 3, 179, 62, 57, 268, 1, 47, 163, 16, 94, 34, 75, 269, 30, 725, 1119, 3, 10, 39, 37, 164, 9, 194, 270, 132, 211, 11, 134, 135, 8, 271, 1, 124, 272, 180, 8, 273, 34, 239, 7, 5, 20, 274, 51, 252, 275]\n","28 \t [122, 31, 46, 10, 143, 129, 154, 93, 14, 1120, 484, 1121, 139, 6, 162, 78, 21, 108, 123, 156, 157, 71, 158, 7, 5, 20, 144, 51, 168, 290, 585, 165, 586, 726, 284, 727, 280, 728, 729, 730, 4, 731, 732, 733, 411, 485, 734, 486, 735, 736, 737, 568, 738, 412, 357, 487, 488, 739, 740, 741, 4, 742, 136, 97, 34, 371, 2, 1, 743, 744, 136, 64, 745, 337, 10, 487, 310, 25, 746, 587, 39, 37, 44, 293, 747, 10, 176, 3, 489, 25, 23, 15, 3, 36, 748, 411, 485, 749, 486, 2, 159, 750, 64, 588, 24, 751, 752]\n","29 \t [280, 338, 1122, 39, 1123, 1124, 3, 753, 83, 327, 1125, 42, 2, 322, 753, 1126, 1127, 336, 1128, 24, 49, 413, 33, 7, 5, 18, 26, 181]\n","30 \t [91, 50, 109, 19, 84, 58, 72, 45, 52, 60, 110, 47, 111, 96, 1, 92, 112, 2, 105, 19, 69, 113, 106, 114, 107, 115, 23, 21, 53, 38, 6, 17, 116, 29, 11, 48, 117, 118, 119, 1, 81, 76, 77, 61, 4, 70, 120, 36, 4, 121, 15, 54, 82, 40, 12, 13, 27, 2, 12, 13, 44, 55]\n","31 \t [42, 89, 169, 170, 9, 754, 9, 700, 336, 80, 172, 329, 16, 56, 7, 5, 18, 26, 372, 39, 37, 414, 294, 42, 276, 255, 73, 4, 79, 62, 195]\n","32 \t [212, 1129, 251, 287, 196, 3, 355, 80, 264, 49, 33, 7, 5, 18, 26]\n","33 \t [122, 31, 46, 10, 143, 129, 210, 93, 14, 1130, 1131, 1132, 139, 6, 162, 78, 21, 108, 123, 156, 157, 71, 158, 7, 5, 20, 144, 51, 168, 95, 2, 165, 755, 1133, 50, 415, 1134, 1135, 1, 756, 490, 757, 1136, 1137, 25, 165, 1138, 1139, 2, 37, 4, 758, 759, 165, 41, 1140, 47, 1141, 416, 1142, 1143, 2, 80, 1144, 182, 1145, 19, 25, 107, 69, 1146, 760, 1147, 2, 319, 137, 63, 3, 761, 1148, 285, 5, 18, 491]\n","34 \t [364, 22, 259, 73, 339, 260, 6, 30, 295, 14, 86, 2, 589, 590, 160, 166, 1, 309, 68, 591, 492, 97, 259, 73, 185, 39, 311, 25, 592, 83, 63, 68, 291, 6, 30, 1, 62, 312, 593, 86, 594, 417, 595, 24, 418, 596, 597, 419, 194, 59, 24, 217, 1, 420, 39, 160, 166, 4, 493, 494, 293, 495, 259, 73, 3, 90, 50, 25, 496, 125, 15, 3, 6, 173, 174, 22, 421, 43, 328, 37, 281, 75, 497, 295, 60, 598, 599, 331, 1, 171, 7, 5, 18, 26, 197, 24, 2, 422, 498, 339]\n","35 \t [1149, 127, 30, 99, 9, 100, 23, 101, 53, 38, 6, 17, 29, 11, 48, 43, 149, 15, 54, 150, 40, 12, 13, 27, 2, 12, 13, 44, 55, 32, 161, 33, 16, 56, 2, 141, 59, 24, 5, 18, 26]\n","36 \t [91, 50, 109, 19, 84, 58, 72, 45, 52, 60, 110, 47, 111, 96, 1, 92, 112, 2, 105, 19, 69, 113, 106, 114, 107, 115, 23, 21, 53, 38, 6, 17, 116, 29, 11, 48, 117, 118, 119, 1, 81, 76, 77, 61, 4, 70, 120, 36, 4, 121, 15, 54, 82, 40, 12, 13, 27, 2, 12, 13, 44, 55]\n","37 \t [67]\n","38 \t [151, 22, 245, 132, 246, 202, 74, 10, 66, 247, 248, 89, 249, 31, 46, 10, 66, 128, 3, 90, 250, 218, 71, 102, 8, 35, 102, 142, 219, 59, 10, 176, 49, 220, 221, 8, 3, 10, 222, 58, 223, 190, 224, 8, 31, 191, 225, 152, 95, 8, 39, 1, 226, 203, 227, 204, 74, 66, 83, 31, 128, 3, 90, 95, 8, 15, 4, 205, 34, 75, 228, 1, 103, 104, 8, 206, 10, 229, 1, 153, 68, 230, 231, 207, 8, 232, 154, 208, 17, 233, 234, 192, 14, 209, 138, 46, 6, 235, 167, 21, 236, 2, 237, 238, 193]\n","39 \t [64, 266, 3, 251, 124, 267, 187, 15, 3, 179, 62, 57, 268, 1, 47, 163, 16, 94, 34, 75, 269, 30, 1150, 86, 3, 10, 39, 37, 164, 9, 194, 270, 132, 211, 11, 134, 135, 8, 271, 1, 124, 272, 180, 8, 273, 34, 239, 7, 5, 20, 274, 51, 252, 275, 67]\n","40 \t [67]\n","41 \t [165, 392, 52, 350, 50, 317, 351, 3, 301, 302, 11, 393, 394, 72, 395, 1, 62, 42, 391, 1151, 136, 352, 25, 35, 63, 318, 396, 41, 397, 398, 41, 1, 399, 159, 2, 64, 400, 52, 2, 64, 68, 353, 3, 401, 171, 285, 5, 20, 65, 51, 410, 762, 50, 182, 52, 93, 19, 1152, 499, 142, 1153, 9, 98, 1154, 1155, 136, 2, 423, 763, 215, 1156, 1157, 572, 2, 764]\n","42 \t [122, 31, 46, 10, 143, 129, 210, 93, 14, 1158, 765, 1159, 139, 6, 162, 78, 21, 108, 123, 156, 157, 71, 158, 7, 5, 20, 144, 51, 168, 1160, 474, 17, 682, 276, 255, 73, 4, 79, 6, 35, 325, 1, 62, 189, 80, 256, 24, 49, 33, 7, 5, 18, 26]\n","43 \t [6, 30, 1, 62, 39, 9, 364, 22, 259, 73, 1161, 217, 1, 420, 39, 160, 166, 4, 493, 494, 293, 495, 259, 73, 3, 90, 50, 25, 496, 125, 15, 3, 6, 173, 174, 22, 421, 43, 328, 37, 281, 75, 497, 295, 60, 766, 216, 40, 98, 1, 424, 241, 9, 296, 4, 297, 425, 331, 1, 500, 33, 197, 24, 2, 422, 498, 1162]\n","44 \t [1163, 1164, 1165, 488, 1166, 1167, 1168, 1169, 2, 1170, 1171, 181, 64, 1172, 99, 30, 8, 767, 1173]\n","45 \t [86, 600, 1174, 1175]\n","46 \t [122, 31, 46, 10, 143, 129, 210, 93, 14, 1176, 1177, 1178, 139, 6, 162, 78, 21, 108, 123, 156, 157, 71, 158, 7, 5, 20, 144, 51, 168]\n","47 \t [678, 305, 322, 279, 6, 1179, 1180, 768, 340, 769, 770, 336, 1181, 130, 30, 4, 1182, 1183, 213, 306, 1184, 195]\n","48 \t [127, 30, 99, 9, 100, 23, 101, 53, 38, 6, 17, 29, 11, 48, 43, 149, 15, 54, 150, 40, 12, 13, 27, 2, 12, 13, 44, 55, 32, 161, 33, 16, 56, 2, 141, 59, 24, 5, 18, 26, 78, 22, 1185, 584]\n","49 \t [41, 1186, 3, 319, 335, 260, 42, 327, 49, 33, 7, 5, 18, 26]\n","50 \t [122, 31, 46, 10, 143, 129, 210, 93, 14, 1187, 452, 1188, 139, 6, 162, 78, 21, 108, 123, 156, 157, 71, 158, 7, 5, 20, 144, 51, 168, 127, 30, 99, 9, 100, 23, 101, 501, 502, 426, 61, 4, 361, 22, 320, 427, 147, 32, 359, 15, 360, 253, 303, 33, 16, 56, 5, 18, 26]\n","51 \t [67]\n","52 \t [67, 453, 57, 85, 2, 454, 287, 195, 151, 212, 41, 137, 79, 40, 82, 22, 355, 145, 23, 287, 196, 21, 356, 64, 455, 57, 85, 22, 357, 358, 456, 188, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 402, 470, 304, 471, 472, 2, 197, 24, 25, 63, 180, 181, 182, 7, 5, 20, 65, 146]\n","53 \t [558, 316, 715, 3, 771, 712, 772, 37, 1189, 97, 1190, 1191, 1192, 2, 1193, 1194, 428, 1195, 85, 69, 773, 41, 774, 91, 1196, 410, 775, 50, 1, 406, 19, 22, 1197, 2, 1198, 1199, 189, 1200, 503, 313, 4, 423, 215, 322, 125, 373, 776, 4, 1201, 265, 316, 374, 87, 74, 199, 288, 1, 716, 717, 87, 257, 199, 288, 27, 718, 258, 719, 302, 27, 407, 74, 87, 133, 482, 582, 720, 308, 721, 87, 365, 155, 16, 369, 482, 288, 16, 1202, 4, 408, 258, 27, 304, 258, 766, 216, 257, 199, 583, 2, 309, 581, 9, 191, 19, 140, 777, 1203, 9, 411, 1204, 264, 42, 504, 429, 145, 7, 5, 20, 65, 575, 122, 31, 341, 778, 30, 3, 505, 342, 93, 375, 78, 21, 108, 185, 1, 47, 163, 63, 334, 155, 1205, 1206, 21, 506, 507, 124, 4, 508, 509, 311, 37, 510, 307, 1, 511, 136, 9, 187, 15, 298, 85, 1, 103, 104, 7, 5, 242, 175, 14, 1207, 779, 1208, 512, 30, 3, 342, 14, 1209, 780, 1210, 6, 321, 362, 186, 28, 14, 1211, 1212, 781, 6, 321, 362, 186, 28, 14, 1213, 513, 1214, 6, 321, 362, 186, 28, 416, 343, 430, 1, 514, 782, 376, 4, 243, 147, 261, 7, 5, 20, 377]\n","54 \t [1215, 63, 4, 423, 215, 680, 1216, 783, 601, 1217, 784, 1218, 1219, 184, 2, 1220, 344, 327, 49, 33, 22, 1221, 43, 7, 5, 18, 26]\n","55 \t [67, 98, 1222, 785, 198, 2, 184, 189, 97, 279, 38, 36, 4, 70, 2, 562, 709, 1223, 35, 310, 64, 1224, 1225, 215, 306, 172, 49, 33, 22, 602, 43, 7, 5, 18, 26]\n","56 \t [786, 1226, 1227, 603, 125, 431, 279, 36, 787, 432, 1228, 81, 32, 254, 240, 32, 788, 32, 363, 32, 604, 32, 789, 790, 32, 198, 32, 184, 32, 278, 1229, 32, 791, 80, 264, 1, 241, 22, 681, 43, 7, 5, 18, 26]\n","57 \t [792, 1230, 1, 92, 1231, 1232, 306, 279, 6, 1233, 323, 605, 1234, 340, 1235, 14, 291, 477, 9, 478, 479, 160, 166, 169, 170, 324, 73, 80, 515, 49, 33, 7, 5, 18, 26]\n","58 \t [368, 1, 1236, 333, 370, 25, 793, 35, 63, 1237, 1238, 1239, 1240, 794, 1241, 125, 1242, 795, 404, 374, 417, 338, 578, 1243, 1244, 69, 1245, 1246, 328, 1247, 338, 578, 1248, 160, 796, 2, 1249, 4, 1250, 418, 797, 1251, 796, 1252, 1253, 69, 1254, 1255, 1256, 1257, 1258, 798, 137, 1259, 674, 64, 1260, 419, 1261, 1262, 1263, 606, 69, 1264, 328, 1265, 1, 1266, 1267, 3, 1268, 140, 1269, 1, 798, 1270, 606, 89, 378, 669, 327, 379, 42, 2, 799, 1271, 1272, 800, 4, 567, 2, 801, 60]\n","59 \t [97, 1273, 183, 96, 1274, 1275, 183, 802, 319, 803, 200, 516, 45, 352, 25, 323, 1276, 41, 37, 786, 140, 47, 50, 1277, 1278, 1279, 1, 560, 1280, 1281, 1282, 2, 695, 42, 11, 134, 135, 32, 19, 140, 58, 1283, 1, 773, 41, 337, 183, 24, 1284, 323, 57, 1285, 145, 603, 125, 262, 380, 607, 804, 215, 1, 47, 50, 1286, 1287, 433, 60, 212, 41, 262, 380, 34, 98, 49, 413, 500, 33, 1288, 24, 34, 504, 1, 241, 1289, 1290, 1291, 1292, 1293, 260, 1294, 217, 216, 200, 516, 583, 1295, 2, 1296, 309, 9, 805, 1297]\n","60 \t [127, 30, 99, 9, 100, 23, 101, 53, 38, 6, 17, 29, 11, 48, 43, 149, 15, 54, 150, 40, 12, 13, 27, 2, 12, 13, 44, 55, 32, 161, 33, 16, 56, 2, 141, 59, 24, 5, 18, 26]\n","61 \t [122, 31, 341, 778, 30, 3, 505, 342, 93, 375, 78, 21, 108, 185, 1, 47, 163, 63, 334, 155, 608, 609, 506, 507, 124, 4, 508, 509, 311, 37, 510, 307, 1, 511, 136, 9, 187, 15, 298, 85, 1, 103, 104, 7, 5, 242, 175, 14, 1298, 1299, 1300, 512, 30, 3, 342, 14, 1301, 806, 1302, 6, 173, 174, 201, 28, 14, 1303, 1304, 1305, 6, 173, 174, 201, 28, 14, 1306, 807, 1307, 6, 173, 174, 201, 28, 416, 343, 430, 1, 514, 434, 376, 4, 243, 147, 261, 7, 5, 20, 377, 127, 30, 99, 9, 100, 23, 101, 53, 38, 6, 17, 29, 11, 48, 43, 149, 15, 54, 150, 40, 12, 13, 27, 2, 12, 13, 44, 55, 32, 161, 33, 16, 56, 2, 141, 59, 24, 5, 18, 26]\n","62 \t [122, 31, 46, 10, 143, 129, 154, 93, 14, 1308, 1309, 1310, 139, 6, 162, 78, 21, 108, 123, 156, 157, 71, 158, 7, 5, 20, 144, 51, 168, 286, 379, 42, 1311, 11, 1312, 187, 412, 354, 62, 1313, 57, 292, 163, 345, 127, 30, 99, 9, 100, 23, 101, 501, 502, 426, 61, 4, 361, 22, 320, 427, 147, 32, 359, 15, 360, 253, 303, 33, 16, 56, 5, 18, 26]\n","63 \t [67]\n","64 \t [290, 3, 1314, 3, 808, 373, 610, 42, 1315, 429, 767, 585, 1316, 586, 1317, 3, 808, 1318, 1319, 35, 1320, 1321, 611, 809, 810, 2, 811, 3, 812, 809, 1322, 610, 813, 136, 280, 760, 1323, 814, 45, 9, 1324, 1325, 1326, 86, 1, 1327, 428, 475, 815, 755, 1328, 1329, 1330, 89, 22, 1331, 1332, 480, 16, 1333, 814, 142, 1334, 19, 517, 97, 1335, 1336, 1337, 1338, 1339, 816, 97, 1340, 41, 1341, 1342, 2, 34, 21, 1343, 1344, 19, 1345, 57, 1346, 813, 428, 86, 1347, 3, 179, 817, 1, 818, 1348, 45, 795, 404, 136, 1349, 1350, 415, 25, 518, 35, 1351, 612, 1352, 42, 1353, 1354, 1355, 1356, 2, 1357, 819, 2, 1358, 1359, 39, 1360, 820, 83, 1361, 318, 1362, 9, 68, 136, 35, 1363, 1364, 3, 50, 42, 714, 83, 8, 613, 277, 610, 239, 5, 18, 491]\n","65 \t [67, 165, 392, 52, 350, 50, 317, 351, 3, 301, 302, 11, 393, 394, 72, 395, 1, 62, 42, 614, 821, 136, 352, 25, 35, 63, 318, 396, 41, 397, 398, 41, 1, 399, 159, 2, 64, 400, 52, 2, 64, 68, 353, 3, 401, 171, 285, 5, 20, 65, 51]\n","66 \t [164, 9, 345, 137, 79, 23, 346, 61, 4, 213, 3, 183, 292, 24, 80, 256, 24, 49, 33, 7, 5, 18, 26, 286, 300, 2, 584, 405, 284, 175]\n","67 \t [127, 30, 99, 9, 100, 23, 101, 53, 38, 6, 17, 29, 11, 48, 43, 149, 15, 54, 150, 40, 12, 13, 27, 2, 12, 13, 44, 55, 32, 161, 33, 16, 56, 2, 141, 59, 24, 5, 18, 26]\n","68 \t [67, 1365, 1366, 86, 503, 1367, 165, 140, 1368, 1369, 820, 145, 125, 142, 371, 615, 1370, 182, 381, 390, 30, 2, 1371, 89, 169, 170, 332, 98, 239, 37, 822, 83, 41, 823, 1372, 1, 1373, 1374, 2, 1375, 616, 1376, 1, 519, 1377, 331, 16, 56, 1378, 1379, 1380, 2, 197, 24, 16, 56, 182, 7, 5, 20, 65, 146]\n","69 \t [1381, 22, 1382, 67, 27, 333, 370, 409, 369, 483, 824]\n","70 \t [37, 1383, 163, 1384, 381, 294, 382, 42, 2, 1385, 4, 1386, 9, 825, 19, 84, 373, 826, 1387, 1388, 1389, 52, 181]\n","71 \t [281, 1390, 159, 1391, 1392, 520, 1393, 827, 1, 153, 374]\n","72 \t [1394, 9, 373, 188, 42, 1395, 63, 1396, 2, 521, 80, 264, 49, 33, 7, 5, 18, 26]\n","73 \t [37, 414, 314, 522, 140, 828, 4, 521, 215, 3, 829, 25, 180, 314, 522, 140, 371, 830, 42, 35, 617, 612, 2, 831, 313, 86, 34, 80, 181, 25, 611, 136, 79, 832, 169, 170, 435, 833, 7, 5, 20, 65, 146]\n","74 \t [67, 1397, 404, 1398, 1399, 4, 215, 11, 169, 170, 195, 80, 264, 49, 33, 7, 5, 18, 26]\n","75 \t [164, 9, 345, 137, 79, 23, 346, 61, 4, 213, 3, 183, 292, 24, 80, 256, 24, 49, 33, 7, 5, 18, 26]\n","76 \t [290, 188, 42, 812, 379, 834, 1400, 25, 9, 1401, 313, 618, 2, 9, 1402, 1, 818, 25, 1403, 1404, 24, 98, 44, 330, 1405, 415, 37, 194, 42, 1406, 89, 619, 1407, 19, 164, 137, 79, 620, 1408, 1409, 2, 1410, 1411, 517, 2, 835, 1412, 165, 392, 52, 350, 50, 317, 351, 3, 301, 302, 11, 621, 394, 72, 395, 1, 62, 42, 614, 821, 136, 352, 25, 35, 63, 318, 396, 41, 397, 398, 41, 1, 399, 159, 2, 64, 400, 52, 2, 64, 68, 353, 3, 401, 171, 285, 5, 20, 65, 51]\n","77 \t [523, 836, 622, 429, 1413, 41, 1414, 1415, 1416, 1417, 1418, 1419, 265, 837, 2, 1420, 75, 138, 524, 1421, 97, 838, 25, 620, 1422, 4, 1423, 98, 49, 1424, 103, 104, 7, 5, 242, 175, 525, 524, 1425, 2, 9, 1426, 3, 1427, 4, 1428, 69, 63, 169, 170, 185, 127, 30, 99, 9, 100, 23, 101, 53, 38, 6, 17, 29, 11, 48, 43, 149, 15, 54, 150, 40, 12, 13, 27, 2, 12, 13, 44, 55, 32, 161, 33, 16, 56, 2, 141, 59, 24, 5, 18, 26]\n","78 \t [1429, 839, 1430, 1431, 520, 1432, 30, 1433, 3, 840]\n","79 \t [67, 11, 293, 27, 166, 1, 841, 324, 73, 279, 842, 45, 1434, 57, 125, 80, 264, 49, 33, 7, 5, 18, 26]\n","80 \t [27, 333, 370, 155, 13, 777, 3, 1435, 302, 1436, 1437, 1438, 3, 257, 199, 824]\n","81 \t [364, 22, 259, 73, 339, 260, 6, 30, 295, 14, 86, 2, 589, 590, 160, 166, 1, 309, 68, 591, 492, 97, 259, 73, 185, 39, 311, 25, 592, 83, 63, 68, 291, 6, 30, 1, 62, 312, 593, 86, 594, 417, 595, 24, 418, 596, 597, 419, 194, 59, 24, 217, 1, 420, 39, 160, 166, 4, 493, 494, 293, 495, 259, 73, 3, 90, 50, 25, 496, 125, 15, 3, 6, 173, 174, 22, 421, 43, 328, 37, 281, 75, 497, 295, 60, 598, 599, 331, 1, 171, 7, 5, 18, 26, 197, 24, 2, 422, 498, 339, 127, 30, 99, 9, 100, 23, 101, 501, 502, 426, 61, 4, 361, 22, 320, 427, 147, 32, 359, 15, 360, 253, 303, 33, 16, 56, 5, 18, 26]\n","82 \t [1439, 329, 775, 582, 4, 243, 333, 1440, 368, 145, 98, 27, 1441, 199, 383, 1442, 24, 1, 481, 424, 241, 25, 1443, 125, 36, 354, 384, 60, 217, 17, 29, 11, 385, 177, 28, 16, 94, 1444, 11, 436, 182, 200, 28, 16, 94, 1445, 32, 1, 386, 263, 254, 240, 526, 384, 623, 624, 363, 791, 198, 184, 289, 216, 82, 40, 98, 1, 424, 241, 9, 296, 4, 297, 425]\n","83 \t [91, 50, 109, 19, 84, 58, 72, 45, 52, 60, 110, 47, 111, 96, 1, 92, 112, 2, 105, 19, 69, 113, 106, 114, 107, 115, 23, 21, 53, 38, 6, 17, 116, 29, 11, 48, 117, 118, 119, 1, 81, 76, 77, 61, 4, 70, 120, 36, 4, 121, 15, 54, 82, 40, 12, 13, 27, 2, 12, 13, 44, 55]\n","84 \t [67]\n","85 \t [1446, 196, 3, 355, 1, 169, 170, 843, 844, 1447, 181, 141, 24, 49, 413, 33, 2, 1448, 1, 1449, 335, 1450, 7, 5, 18, 26]\n","86 \t [41, 68, 39, 474, 563, 276, 255, 73, 159, 475, 4, 79, 6, 35, 325, 1, 62, 189, 80, 256, 24, 49, 33, 7, 5, 18, 26, 1451]\n","87 \t [810]\n","88 \t [183, 62, 334, 1452, 195, 415, 98, 52, 1453, 75, 845, 1454, 36, 725, 846, 295, 17, 29, 145, 17, 29, 11, 393, 347, 177, 28, 16, 94, 847, 145, 17, 29, 11, 621, 347, 200, 28, 16, 94, 848, 9, 625, 626, 2, 197, 24, 145, 7, 5, 20, 65, 146, 1455, 1, 849, 850, 3, 1456, 1457, 42, 1458, 1459, 19, 165, 100, 338, 722, 387, 520, 313, 851, 1460, 839, 85, 1461]\n","89 \t [67, 164, 9, 345, 137, 79, 23, 346, 61, 4, 213, 3, 183, 292, 24, 80, 256, 24, 49, 33, 7, 5, 18, 26, 122, 31, 46, 10, 143, 129, 210, 93, 14, 1462, 1463, 1464, 139, 6, 162, 78, 21, 108, 123, 156, 157, 71, 158, 7, 5, 20, 144, 51, 168]\n","90 \t [367, 852, 188, 627, 42, 344, 527, 853, 518, 854, 80, 172, 277, 22, 602, 43, 7, 5, 18, 26, 372, 39, 42, 855, 276, 255, 73, 4, 79, 6, 35, 325, 1, 62]\n","91 \t [528, 341, 3, 437, 88, 43, 1, 628, 1465, 1466, 59, 138, 1467, 282, 1468, 1469, 1, 1470, 181, 1471, 252, 9, 1472, 1473, 629, 1474, 1475, 1, 153, 326]\n","92 \t [122, 31, 46, 10, 143, 129, 154, 93, 14, 1476, 1477, 1478, 139, 6, 162, 78, 21, 108, 123, 156, 157, 71, 158, 7, 5, 20, 144, 51, 168, 64, 266, 3, 251, 124, 267, 45, 187, 15, 3, 179, 62, 57, 268, 1, 47, 163, 16, 94, 34, 75, 269, 30, 856, 1479, 86, 3, 10, 39, 37, 164, 9, 194, 270, 132, 211, 11, 134, 135, 8, 271, 1, 124, 272, 180, 8, 273, 34, 239, 7, 5, 20, 274, 51, 252, 275]\n","93 \t [67, 42, 89, 169, 170, 1480, 379, 41, 857, 9, 367, 825, 336, 80, 172, 329, 16, 56, 7, 5, 18, 26, 372, 39, 37, 414, 294, 42, 276, 255, 73, 4, 79, 62, 195]\n","94 \t [127, 30, 99, 9, 100, 23, 101, 53, 38, 6, 17, 29, 11, 48, 43, 149, 15, 54, 150, 40, 12, 13, 27, 2, 12, 13, 44, 55, 32, 161, 33, 16, 56, 2, 141, 59, 24, 5, 18, 26]\n","95 \t [367, 852, 188, 627, 42, 344, 527, 853, 518, 854, 80, 172, 277, 22, 602, 43, 7, 5, 18, 26, 372, 39, 42, 855, 276, 255, 73, 4, 79, 6, 35, 325, 1, 62]\n","96 \t [27, 333, 370, 409, 277, 407, 483, 93, 1481, 277, 142, 1482, 368, 1483, 1484, 1485, 858, 771, 1486, 312, 859, 211, 1487, 2, 1488, 32, 2, 1489, 381, 294, 382, 42, 860, 315, 39, 45, 1490, 1491, 861, 89, 1492, 1493]\n","97 \t [32, 529, 335, 388, 862, 340, 863, 281, 864, 282, 42, 1494, 3, 571, 298, 294, 865, 1, 153, 2, 25, 866, 125, 867, 630, 14, 122, 31, 46, 10, 143, 129, 210, 93, 14, 1495, 1496, 1497, 139, 6, 162, 78, 21, 108, 123, 156, 157, 71, 158, 7, 5, 20, 144, 51, 168]\n","98 \t [1498, 1499, 336, 9, 1500, 329, 1, 1501, 172, 6, 16, 56, 7, 5, 18, 26, 372, 39, 37, 414, 294, 42, 276, 255, 73, 4, 79, 62, 195]\n","99 \t [67, 127, 30, 99, 9, 100, 23, 101, 53, 38, 6, 17, 29, 11, 48, 43, 149, 15, 54, 150, 40, 12, 13, 27, 2, 12, 13, 44, 55, 32, 161, 33, 16, 56, 2, 141, 59, 24, 5, 18, 26]\n","100 \t [67]\n","101 \t [1502, 1503, 21, 1504, 185, 1505, 851, 214, 285, 5, 20, 65, 51]\n","102 \t [52, 25, 1506, 171, 2, 1507, 868, 1508, 1509, 89, 171, 1510, 530, 19, 16, 1511, 89, 631, 37, 711, 1512, 14, 869, 1, 870, 97, 286, 216, 35, 1513, 1514, 1515, 25, 1516, 1517, 1518, 260, 371, 1519, 1520, 1521, 1522, 632, 1523, 573, 185, 1524, 1525, 1526, 1527, 759, 1, 1528, 1529, 774, 9, 367, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 2, 381, 1539, 1540, 289, 1541, 1542, 63, 1543, 1544, 1545, 25, 793, 1546, 1547, 30, 794, 34, 431, 504, 429, 189, 388, 4, 871, 1548, 326, 7, 5, 20, 65, 51, 14, 6, 1549, 1550, 1551, 531, 28, 1552, 299, 7, 5, 20, 65, 51, 14, 6, 1553, 1554, 130, 28, 1555, 299, 7, 5, 20, 65, 51, 14, 6, 1556, 1557, 130, 28, 1558, 299, 7, 5, 20, 65, 51, 14, 6, 1559, 1560, 186, 28, 1561, 299, 7, 5, 20, 65, 51, 14, 6, 321, 362, 186, 28, 1562, 299, 7, 5, 20, 65, 51, 14, 6, 1563, 1564, 130, 28, 1565, 299, 7, 5, 20, 65, 51, 14, 6, 1566, 1567, 130, 28, 1568, 299, 7, 5, 20, 65, 51, 14, 6, 1569, 130, 28, 1570, 299, 7, 5, 20, 65, 51, 14, 6, 1571, 531, 28, 1572, 299, 152, 89, 631, 1573, 2, 1574, 1575, 1576, 1577, 1578, 530, 570, 6, 1579, 1580, 306, 868, 45, 1581, 35, 1582, 4, 1583, 208, 291, 632, 66, 435, 453, 57, 85, 2, 454, 287, 195, 151, 212, 41, 137, 79, 40, 82, 22, 355, 145, 23, 287, 196, 21, 356, 64, 455, 57, 85, 22, 357, 358, 456, 188, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 402, 470, 304, 471, 472, 2, 197, 24, 25, 63, 180, 181, 182, 7, 5, 20, 65, 146]\n","103 \t [1584]\n","104 \t [64, 266, 3, 251, 124, 267, 45, 187, 15, 3, 179, 62, 57, 268, 1, 47, 163, 16, 94, 34, 75, 269, 30, 391, 671, 3, 10, 39, 37, 164, 9, 194, 270, 132, 211, 11, 134, 135, 8, 271, 1, 124, 272, 180, 8, 273, 34, 239, 7, 5, 20, 274, 51, 252, 275]\n","105 \t [290, 373, 633, 413, 33, 86, 1585, 577, 41, 25, 23, 426, 61, 4, 361, 22, 330, 320, 32, 1586, 359, 1587, 360, 253, 303, 33, 16, 56, 5, 18, 26]\n","106 \t [364, 22, 259, 73, 339, 260, 6, 30, 295, 14, 86, 2, 589, 590, 160, 166, 1, 309, 68, 591, 492, 97, 259, 73, 185, 39, 311, 25, 592, 83, 63, 68, 291, 6, 30, 1, 62, 312, 593, 86, 594, 417, 595, 24, 418, 596, 597, 419, 194, 59, 24, 217, 1, 420, 39, 160, 166, 4, 493, 494, 293, 495, 259, 73, 3, 90, 50, 25, 496, 125, 15, 3, 6, 173, 174, 22, 421, 43, 328, 37, 281, 75, 497, 295, 60, 598, 599, 331, 1, 171, 7, 5, 18, 26, 197, 24, 2, 422, 498, 339]\n","107 \t [872, 873, 1, 152, 634, 8, 190, 874, 875, 25, 876, 877, 159, 878, 4, 879, 880, 85, 140, 37, 635, 503, 354, 314, 1, 92, 881, 882]\n","108 \t [52, 1588, 404, 189, 1589, 1590, 365, 883, 1591, 2, 804, 358, 702, 173, 174, 607, 72, 1592, 337, 35, 884, 1593, 616, 1594, 1595, 14, 1596, 1597, 432, 1598, 197, 24, 16, 56, 214, 7, 5, 20, 65, 146]\n","109 \t [67, 151, 22, 245, 132, 246, 202, 74, 10, 66, 247, 248, 89, 249, 31, 46, 10, 66, 128, 3, 90, 250, 218, 71, 102, 8, 35, 102, 142, 219, 59, 10, 176, 49, 220, 221, 8, 3, 10, 222, 58, 223, 190, 224, 8, 31, 191, 225, 152, 95, 8, 39, 1, 226, 203, 227, 204, 74, 66, 83, 31, 128, 3, 90, 95, 8, 15, 4, 205, 34, 75, 228, 1, 103, 104, 8, 206, 10, 229, 1, 153, 68, 230, 231, 207, 8, 232, 154, 208, 17, 233, 234, 192, 14, 209, 138, 46, 6, 235, 167, 21, 236, 2, 237, 238, 193, 91, 50, 109, 19, 84, 58, 72, 45, 52, 60, 110, 47, 111, 96, 1, 92, 112, 2, 105, 19, 69, 113, 106, 114, 107, 115, 23, 21, 53, 38, 6, 17, 116, 29, 11, 48, 117, 118, 119, 1, 81, 76, 77, 61, 4, 70, 120, 36, 4, 121, 15, 54, 82, 40, 12, 13, 27, 2, 12, 13, 44, 55]\n","110 \t [91, 50, 109, 19, 84, 58, 72, 45, 52, 60, 110, 47, 111, 96, 1, 92, 112, 2, 105, 19, 69, 113, 106, 114, 107, 115, 23, 21, 53, 38, 6, 17, 116, 29, 11, 48, 117, 118, 119, 1, 81, 76, 77, 61, 4, 70, 120, 36, 4, 121, 15, 54, 82, 40, 12, 13, 27, 2, 12, 13, 44, 55]\n","111 \t [1599, 670, 885, 1600, 2, 886, 1601, 2, 423, 763, 215, 32, 171, 7, 5, 18, 26]\n","112 \t [35, 887, 1602, 9, 1603, 615, 106, 263, 1, 62, 312, 145, 98, 27, 304, 636, 383, 23, 125, 532, 36, 637, 1604, 60, 217, 17, 29, 11, 385, 177, 28, 16, 94, 1605, 11, 436, 182, 200, 28, 16, 94, 1606, 32, 1, 386, 263, 254, 240, 1607, 1608, 1609, 278, 198, 184, 289, 216, 15, 638, 40, 98, 17, 301, 29, 2, 9, 296, 4, 297, 425]\n","113 \t [165, 392, 52, 350, 50, 317, 351, 3, 301, 302, 11, 393, 394, 72, 395, 1, 62, 42, 1610, 136, 352, 25, 35, 63, 318, 396, 41, 397, 398, 41, 1, 399, 159, 2, 64, 400, 52, 2, 64, 68, 353, 3, 401, 171, 285, 5, 20, 65, 51, 122, 31, 341, 1611, 438, 30, 93, 375, 78, 21, 108, 185, 1, 47, 163, 63, 334, 533, 608, 609, 506, 507, 124, 4, 508, 509, 311, 37, 510, 307, 1, 511, 136, 9, 187, 15, 298, 85, 1, 103, 104, 7, 5, 242, 175, 14, 1612, 484, 1613, 6, 173, 174, 201, 28, 14, 1614, 888, 1615, 6, 173, 174, 201, 28, 14, 1616, 1617, 1618, 6, 173, 174, 201, 28, 14, 1619, 1620, 797, 6, 1621, 531, 28, 14, 1622, 889, 1623, 6, 639, 186, 28, 14, 1624, 1625, 1626, 6, 639, 186, 28, 14, 1627, 890, 1628, 6, 639, 186, 28, 14, 1629, 1630, 1631, 6, 640, 641, 130, 28, 14, 1632, 891, 1633, 6, 640, 641, 130, 28, 14, 1634, 1635, 1636, 6, 640, 641, 130, 28, 14, 1637, 1638, 1639, 6, 235, 186, 28, 14, 1640, 1641, 1642, 6, 235, 186, 28, 14, 1643, 1644, 1645, 6, 235, 186, 28, 416, 343, 430, 1, 514, 434, 376, 4, 243, 147, 261, 7, 5, 20, 377]\n","114 \t [67, 127, 30, 99, 9, 100, 23, 101, 53, 38, 6, 17, 29, 11, 48, 43, 149, 15, 54, 150, 40, 12, 13, 27, 2, 12, 13, 44, 55, 32, 161, 33, 16, 56, 2, 141, 59, 24, 5, 18, 26]\n","115 \t [1646, 42, 4, 70, 1647, 64, 605, 1648, 871, 1649, 534, 42, 723, 892, 518, 1650, 1651, 1652, 189, 313, 34, 1653, 86, 783, 601, 1654, 1655, 2, 887, 1656, 9, 1657, 322, 642, 893, 47, 894, 1658, 1659, 534, 42, 89, 895, 171, 182, 7, 5, 20, 65, 146]\n","116 \t [67, 91, 50, 109, 19, 84, 58, 72, 45, 52, 60, 110, 47, 111, 96, 1, 92, 112, 2, 105, 19, 69, 113, 106, 114, 107, 115, 23, 21, 53, 38, 6, 17, 116, 29, 11, 48, 117, 118, 119, 1, 81, 76, 77, 61, 4, 70, 120, 36, 4, 121, 15, 54, 82, 40, 12, 13, 27, 2, 12, 13, 44, 55]\n","117 \t [127, 30, 99, 9, 100, 23, 101, 53, 38, 6, 17, 29, 11, 48, 43, 149, 15, 54, 150, 40, 12, 13, 27, 2, 12, 13, 44, 55, 32, 161, 33, 16, 56, 2, 141, 59, 24, 5, 18, 26]\n","118 \t [67, 823, 1660, 1, 1661, 1662, 1663, 896, 1664, 1665, 43, 1666, 1667, 22, 896, 643, 897, 308, 1668, 3, 1669, 1670, 9, 1671, 439, 21, 1672, 1673, 260, 313, 24, 1674, 898, 318, 476, 298, 525, 85, 25, 899, 1675, 2, 1676, 535, 900, 1677, 125, 78, 22, 536, 2, 900, 57, 85, 193]\n","119 \t [1678, 3, 52, 1679, 1680, 1681, 312, 217, 98, 27, 533, 1682, 17, 29, 11, 1683, 177, 28, 16, 94, 847, 11, 385, 182, 200, 28, 16, 94, 848, 37, 901, 845, 354, 36, 1684, 846, 295, 60, 9, 625, 626, 2, 197, 24, 145, 7, 5, 20, 65, 146]\n","120 \t [164, 9, 345, 137, 79, 23, 346, 61, 4, 213, 3, 183, 292, 24, 80, 256, 24, 49, 33, 7, 5, 18, 26]\n","121 \t [122, 31, 46, 10, 143, 129, 154, 93, 14, 1685, 484, 1686, 139, 6, 179, 902, 78, 21, 108, 123, 156, 157, 71, 158, 7, 5, 20, 144, 51, 903, 1687, 1688, 97, 3, 179, 817, 1689, 904, 136, 1690, 291, 257, 199, 39, 34, 1691, 521, 215, 2, 27, 1692]\n","122 \t [188, 1693, 323, 373, 431, 3, 536, 32, 4, 1694, 32, 4, 213, 32, 4, 1695, 70, 570, 1696, 557, 1697, 1, 905, 365, 1698, 80, 264, 1, 241, 7, 5, 18, 26]\n","123 \t [37, 414, 314, 522, 140, 828, 4, 521, 215, 3, 829, 25, 180, 314, 522, 140, 371, 830, 42, 35, 617, 612, 2, 831, 313, 86, 34, 80, 181, 25, 611, 136, 79, 832, 169, 170, 435, 833, 7, 5, 20, 65, 146]\n","124 \t [64, 266, 3, 251, 124, 267, 45, 187, 15, 3, 179, 62, 57, 268, 1, 47, 163, 16, 94, 34, 75, 269, 30, 906, 3, 10, 39, 37, 164, 9, 194, 270, 132, 211, 11, 134, 135, 8, 271, 1, 124, 272, 180, 8, 273, 34, 239, 7, 5, 20, 274, 51, 252, 275, 122, 31, 46, 10, 143, 129, 210, 93, 14, 1699, 1700, 1701, 139, 6, 162, 78, 21, 108, 123, 156, 157, 71, 158, 7, 5, 20, 144, 51, 168, 91, 50, 109, 19, 84, 58, 72, 45, 52, 60, 110, 47, 111, 96, 1, 92, 112, 2, 105, 19, 69, 113, 106, 114, 107, 115, 23, 21, 53, 38, 6, 17, 116, 29, 11, 48, 117, 118, 119, 1, 81, 76, 77, 61, 4, 70, 120, 36, 4, 121, 15, 54, 82, 40, 12, 13, 27, 2, 12, 13, 44, 55]\n","125 \t [127, 30, 99, 9, 100, 23, 101, 53, 38, 6, 17, 29, 11, 48, 43, 149, 15, 54, 150, 40, 12, 13, 27, 2, 12, 13, 44, 55, 32, 161, 33, 16, 56, 2, 141, 59, 24, 5, 18, 26]\n","126 \t [67, 405, 2, 300, 195, 534, 1702, 34, 1703, 1, 1704, 1705, 4, 265, 440, 25, 152, 50, 288, 35, 63, 2, 1706, 613, 4, 1707, 1708, 185]\n","127 \t [32, 529, 335, 388, 862, 340, 863, 281, 864, 282, 42, 1709, 3, 1710, 417, 1, 519, 907, 280, 441, 278, 2, 908, 442, 19, 164, 72, 909, 195, 418, 1711, 37, 822, 254, 240, 788, 305, 2, 604, 419, 98, 305, 2, 198, 1712, 910, 97, 1713, 41, 1714, 39, 57, 1715, 387, 911, 254, 240, 2, 184, 298, 294, 865, 1, 153, 2, 25, 866, 125, 867, 630, 14]\n","128 \t [44, 30, 169, 170, 16, 1716, 2, 58, 9, 84, 181, 80, 515, 49, 33, 7, 5, 18, 26]\n","129 \t [27, 333, 370, 409, 277, 533, 483, 368, 86, 1717, 912, 9, 1718, 41, 83, 1719, 25, 537, 1720, 1721, 89, 177, 1722, 1723, 1724, 140, 37, 1725, 307, 326, 538, 913, 634, 2, 443, 1726, 538, 859, 211, 35, 105, 538, 858, 1727, 644, 338, 914, 606, 538, 1728, 69, 1729, 761, 350, 861, 410, 1730, 415, 212, 41, 1731, 645, 3, 257, 199, 1732, 9, 625, 1733, 1734, 57, 713, 406, 618, 2, 1735, 1736, 1737, 1, 1738, 499, 332, 262, 380, 34, 49, 500, 33, 3, 1739, 1740, 160, 27, 407, 258, 185, 145, 633, 33, 34, 16, 56, 7, 5, 18, 915, 216, 646, 1741, 645, 916, 257, 199, 309, 917, 9, 805, 1742]\n","130 \t [388, 1743, 41, 37, 1, 292, 163, 1744, 576, 1745, 282, 1746, 918, 137, 63, 1747, 1748, 122, 31, 46, 10, 143, 129, 210, 93, 14, 1749, 1750, 1751, 139, 6, 162, 78, 21, 108, 123, 156, 157, 71, 158, 7, 5, 20, 144, 51, 168, 290, 919, 1752, 170, 1753, 25, 1754, 137, 366, 1755, 920, 290, 428, 919, 337, 79, 2, 517, 282, 630, 25, 1756, 1757, 79, 22, 764, 1758, 1759, 240, 2, 305, 1760, 895, 784, 2, 330, 646, 1, 921, 1761, 536, 1762, 30, 1763, 329, 1764, 2, 9, 754, 647, 631, 922, 36, 1765, 77, 4, 1766, 619, 1767, 1768, 1769, 1770, 1771, 923, 390, 2, 1772, 366, 1773]\n","131 \t [122, 31, 46, 10, 143, 129, 210, 93, 14, 1774, 891, 1775, 139, 6, 162, 78, 21, 108, 123, 156, 157, 71, 158, 7, 5, 20, 144, 51, 168, 1776, 1777, 648, 7, 5, 20, 144, 51, 1778, 924, 46, 57, 1779, 209, 528, 924, 46, 9, 1780, 1, 265, 539, 540, 25, 579, 1781, 126, 925, 179, 528, 326, 629, 926, 1782, 139, 126, 3, 437, 88, 131, 1, 628, 927, 523, 629, 926, 525, 298, 85, 59, 207, 1783, 1, 103, 104, 140, 25, 635, 893, 1784, 1, 539, 540, 7, 5, 242, 175, 1785, 216, 1, 420, 39, 528, 1786, 648, 9, 1787, 1788, 1789, 1, 539, 540, 83, 138, 68, 1790, 1791, 165, 41, 9, 928, 1792, 1793, 181, 434, 376, 4, 243, 147, 261, 7, 5, 20, 377, 164, 9, 345, 137, 79, 23, 346, 61, 4, 213, 3, 183, 292, 24, 80, 256, 24, 49, 33, 7, 5, 18, 26]\n","132 \t [1794, 151, 22, 245, 132, 246, 202, 74, 10, 66, 247, 248, 89, 249, 31, 46, 10, 66, 128, 3, 90, 250, 218, 71, 102, 8, 35, 102, 142, 219, 59, 10, 176, 49, 220, 221, 8, 3, 10, 222, 58, 223, 190, 224, 8, 31, 191, 225, 152, 95, 8, 39, 1, 226, 203, 227, 204, 74, 66, 83, 31, 128, 3, 90, 95, 8, 15, 4, 205, 34, 75, 228, 1, 103, 104, 8, 206, 10, 229, 1, 153, 68, 230, 231, 207, 8, 232, 154, 208, 17, 233, 234, 192, 14, 209, 138, 46, 6, 235, 167, 21, 236, 2, 237, 238, 193]\n","133 \t [39, 644, 1, 1795, 929, 930, 1796, 59, 1797, 83, 1798, 89, 201, 1799, 1800, 644, 101, 1801, 1802]\n","134 \t [67]\n","135 \t [372, 25, 1803, 1804, 1805, 3, 1806, 64, 1807, 696]\n","136 \t [159, 1808, 200, 516, 1809, 1810, 382, 1811, 129, 63, 4, 200, 516, 649, 63, 1812, 1813, 1814, 2, 1815, 931, 19, 152, 50, 68, 86, 96, 1816]\n","137 \t [188, 42, 379, 97, 894, 136, 1817, 8, 188, 42, 1818, 1, 519, 835, 1819, 181, 1820, 2, 1821, 931, 19, 25, 932, 185]\n","138 \t [67, 127, 30, 99, 9, 100, 23, 101, 53, 38, 6, 17, 29, 11, 48, 43, 149, 15, 54, 150, 40, 12, 13, 27, 2, 12, 13, 44, 55, 32, 161, 33, 16, 56, 2, 141, 59, 24, 5, 18, 26]\n","139 \t [140, 9, 476, 1822, 499, 2, 616, 142, 1823, 886, 1824, 2, 933, 1825, 1826, 19, 25, 4, 1827, 125, 1, 519, 932, 185]\n","140 \t [904, 91, 50, 109, 19, 84, 58, 72, 45, 52, 60, 110, 47, 111, 96, 1, 92, 112, 2, 105, 19, 69, 113, 106, 114, 107, 115, 23, 21, 53, 38, 6, 17, 116, 29, 11, 48, 117, 118, 119, 1, 81, 76, 77, 61, 4, 70, 120, 36, 4, 121, 15, 54, 82, 40, 12, 13, 27, 2, 12, 13, 44, 55]\n","141 \t [433, 23, 934, 438, 30, 22, 315, 25, 537, 35, 63, 650, 651, 541, 542, 1, 652, 2, 653, 654, 655, 490, 656, 657, 343, 658, 934, 659, 86, 439, 543, 8, 443, 315, 57, 378, 444, 8, 445, 544, 545, 123, 1, 103, 104, 7, 5, 242, 175, 282, 244, 87, 133, 148, 6, 660, 661, 130, 28, 87, 155, 177, 148, 6, 446, 447, 130, 28, 87, 178, 369, 148, 6, 1828, 130, 28, 87, 200, 196, 148, 6, 448, 186, 28, 87, 408, 389, 148, 532, 36, 1829, 546, 547, 387, 76, 173, 174, 201, 28, 548, 310, 64, 549, 348, 31, 449, 408, 1830, 4, 243, 147, 261, 167, 21, 450, 138, 9, 296, 4, 297, 550, 138, 1, 451, 76, 34, 349, 98, 440, 17, 29, 3, 551, 662, 35, 166, 3, 183, 552, 244, 1, 451, 263, 34, 349, 17, 29, 11, 553, 43]\n","142 \t [1831, 1832, 1833, 935, 1834, 322, 642, 431, 188, 627, 1835, 1836, 432, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 4, 936, 184, 1844, 1845, 2, 159, 97, 619, 101, 344, 1846, 1847, 80, 515, 49, 33, 7, 5, 18, 26]\n","143 \t [151, 129, 4, 937, 524, 938, 2, 939, 940, 941, 942, 649, 943, 944, 945, 45, 1, 946, 947, 406, 35, 63, 68, 83, 41, 25, 948, 57, 125, 3, 663, 73, 151, 129, 4, 937, 524, 938, 2, 939, 940, 941, 942, 649, 943, 944, 945, 45, 1, 946, 947, 406, 35, 63, 68, 83, 41, 25, 948, 57, 125, 3, 663, 73]\n","144 \t [1848, 535, 949, 1849, 1850, 1851, 2, 37, 9, 1852, 950, 534, 499, 412, 1853, 4, 423, 215, 193, 80, 951, 24, 2, 799, 1854, 1855, 952, 7, 5, 18, 1856, 179, 123, 169, 170, 1857, 437, 88, 43, 3, 1858, 1859, 439, 543, 8, 256, 348, 378, 444, 8, 1860, 41, 1861, 86, 1862, 1863, 1864, 1, 1865, 8, 1866, 1, 265, 539, 540, 3, 1867, 1868, 341, 282, 244, 87, 133, 148, 126, 3, 437, 88, 131, 1, 628, 927, 87, 155, 953, 148, 126, 3, 133, 88, 131, 1, 134, 135, 31, 449, 402, 1869, 4, 243, 147, 261, 167, 21, 450]\n","145 \t [145, 212, 41, 98, 27, 178, 954, 383, 23, 125, 1870, 42, 441, 527, 60, 217, 17, 29, 11, 385, 177, 28, 16, 94, 1871, 11, 436, 182, 200, 28, 16, 94, 1872, 32, 1, 386, 263, 254, 240, 526, 384, 623, 624, 441, 278, 198, 184, 289, 262, 24, 34, 16, 56, 214, 7, 5, 20, 65, 146, 433, 23, 955, 438, 30, 22, 315, 25, 537, 35, 63, 650, 651, 541, 542, 1, 652, 2, 653, 654, 655, 490, 656, 657, 343, 658, 955, 659, 86, 439, 543, 8, 443, 315, 57, 378, 444, 8, 445, 544, 545, 123, 1, 103, 104, 7, 5, 242, 175, 282, 244, 87, 133, 12, 148, 6, 1873, 1874, 155, 1875, 87, 177, 533, 148, 6, 1876, 1877, 87, 369, 664, 148, 6, 1878, 9, 1879, 87, 196, 389, 148, 36, 1880, 1881, 546, 547, 387, 76, 1882, 201, 28, 548, 310, 64, 549, 348, 31, 449, 953, 954, 4, 243, 147, 261, 167, 21, 450, 138, 9, 296, 4, 297, 550, 244, 4, 133, 16, 664, 148, 19, 917, 244, 22, 348, 34, 349, 440, 17, 29, 11, 551, 1883, 35, 166, 3, 1884, 552, 244, 4, 196, 16, 389, 148, 17, 29, 11, 1885, 43]\n","146 \t [956, 2, 909, 36, 1, 62, 1886, 1887, 189, 145, 98, 27, 1888, 636, 383, 23, 125, 532, 36, 441, 527, 60, 217, 17, 29, 11, 385, 177, 28, 16, 94, 1889, 11, 436, 182, 200, 28, 16, 94, 1890, 32, 1, 386, 263, 254, 240, 526, 384, 623, 624, 441, 278, 198, 184, 289, 216, 15, 638, 40, 98, 17, 301, 29, 2, 9, 296, 4, 297, 425, 262, 24, 34, 16, 56, 214, 7, 5, 20, 65, 146]\n","147 \t [85, 216, 535, 1891, 607, 337, 340, 284, 1892, 85, 1893, 613, 1894, 840, 1895, 308, 1, 1896, 1897, 2, 25, 4, 1898, 313, 1899, 19, 39, 25, 1900, 1901, 707, 1902, 9, 928, 1903, 1904, 176, 298, 85, 25, 899, 1905, 1906, 2, 1907, 1908]\n","148 \t [453, 57, 85, 2, 454, 287, 195, 151, 212, 41, 137, 79, 40, 82, 22, 355, 145, 23, 287, 196, 21, 356, 64, 455, 57, 85, 22, 357, 358, 456, 188, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 402, 470, 304, 471, 472, 2, 197, 24, 25, 63, 180, 181, 182, 7, 5, 20, 65, 146, 151, 22, 245, 132, 246, 202, 74, 10, 66, 247, 248, 89, 249, 31, 46, 10, 66, 128, 3, 90, 250, 218, 71, 102, 8, 35, 102, 142, 219, 59, 10, 176, 49, 220, 221, 8, 3, 10, 222, 58, 223, 190, 224, 8, 31, 191, 225, 152, 95, 8, 39, 1, 226, 203, 227, 204, 74, 66, 83, 31, 128, 3, 90, 95, 8, 15, 4, 205, 34, 75, 228, 1, 103, 104, 8, 206, 10, 229, 1, 153, 68, 230, 231, 207, 8, 232, 154, 208, 17, 233, 234, 192, 14, 209, 138, 46, 6, 235, 167, 21, 236, 2, 237, 238, 193, 433, 23, 512, 30, 3, 505, 342, 35, 102, 142, 826, 277, 155, 317, 8, 443, 315, 57, 378, 444, 8, 445, 544, 545, 123, 1, 103, 104, 7, 5, 242, 175, 282, 244, 87, 133, 148, 512, 30, 3, 342, 87, 155, 389, 148, 36, 1909, 1910, 4, 1911, 546, 547, 387, 76, 173, 174, 201, 28, 548, 310, 64, 549, 348, 31, 449, 663, 643, 4, 243, 147, 261, 167, 21, 450, 138, 9, 296, 4, 297, 550, 244, 22, 348, 34, 349, 98, 440, 17, 29, 3, 551, 662, 35, 166, 3, 183, 552, 244, 1, 451, 263, 34, 349, 17, 29, 11, 553, 43]\n","149 \t [122, 31, 46, 10, 143, 129, 154, 93, 14, 1912, 888, 1913, 139, 6, 179, 902, 78, 21, 108, 123, 156, 157, 71, 158, 7, 5, 20, 144, 51, 903, 1914, 1915, 45, 1916, 1917, 1918, 3, 1919, 132, 1920, 9, 1921, 59, 724, 950, 41, 83, 3, 1922, 1923, 1924, 1925, 1926, 1927, 1, 413, 171, 2, 327, 382, 42, 32]\n","150 \t [1928, 782, 45, 1929, 294, 1930, 1, 957, 958, 1931, 57, 1932, 1, 153, 332, 122, 31, 46, 10, 143, 129, 210, 93, 14, 1933, 1934, 1935, 139, 6, 162, 78, 21, 108, 123, 156, 157, 71, 158, 7, 5, 20, 144, 51, 168]\n","151 \t [35, 63, 280, 62, 312, 32, 901, 36, 391, 959, 1, 62, 145, 17, 29, 11, 621, 347, 177, 28, 16, 94, 1936, 145, 17, 29, 11, 48, 347, 200, 28, 16, 94, 1937, 80, 951, 24, 1, 241, 7, 5, 18, 26]\n","152 \t [78, 375, 22, 379, 42, 9, 1938, 165, 405, 75, 24, 284, 175, 142, 58, 1939, 42, 286, 1940, 1941, 564, 338, 1942, 1943, 1944, 569, 1945, 1, 1946, 1947, 1948, 1949, 2, 1950, 1951, 262, 24, 87, 1, 241, 905, 7, 5, 18, 1952, 87, 3, 481, 1953, 1954, 1955]\n","153 \t [1, 1956, 50, 166, 335, 1957, 9, 892, 307, 215, 89, 169, 170, 336, 35, 1958, 1959, 137, 79, 280, 6, 1960, 1961, 172, 2, 566, 338, 260, 565, 4, 801, 14]\n","154 \t [1962, 1963, 1964, 2, 9, 1965, 329, 910, 1966, 19, 41, 39, 25, 1967, 41, 137, 79, 1, 171, 280, 9, 202, 1968, 1969, 1970, 323, 9, 1971, 1972, 1973, 530, 332, 322, 1974, 279, 36, 403, 1975, 277, 22, 1976, 195, 665, 81, 403, 254, 240, 363, 1977, 1978, 1979, 300, 2, 1980, 1, 883, 1981, 1982, 800, 4, 169, 170, 435, 214, 7, 5, 20, 65, 146]\n","155 \t [39, 1983, 860, 960, 1984, 1985, 188, 38, 42, 1986, 1987, 38, 36, 4, 70, 2, 666, 1988, 442, 1989, 1990, 666, 936, 2, 1991, 960, 172, 49, 33, 22, 1992, 43, 7, 5, 18, 26, 364, 1, 841, 22, 324, 73, 433, 23, 667, 438, 30, 22, 315, 25, 537, 35, 63, 650, 651, 541, 542, 1, 652, 2, 653, 654, 655, 490, 656, 657, 343, 658, 667, 659, 86, 439, 543, 8, 443, 315, 57, 378, 444, 8, 445, 544, 545, 123, 1, 103, 104, 7, 5, 242, 175, 282, 244, 87, 133, 148, 6, 660, 661, 130, 28, 87, 155, 177, 148, 6, 446, 447, 130, 28, 87, 178, 369, 148, 6, 554, 2, 555, 130, 28, 87, 200, 196, 148, 6, 448, 186, 28, 87, 408, 389, 148, 36, 1993, 959, 57, 556, 11, 553, 43, 546, 547, 387, 76, 173, 174, 201, 28, 548, 310, 64, 549, 348, 31, 449, 1994, 1995, 4, 243, 147, 261, 167, 21, 450, 138, 9, 296, 4, 297, 550, 138, 1, 451, 76, 34, 349, 98, 440, 17, 29, 3, 551, 662, 35, 166, 3, 183, 552, 244, 1, 451, 263, 34, 349, 17, 29, 11, 553, 43]\n","156 \t [91, 50, 109, 19, 84, 58, 72, 45, 52, 60, 110, 47, 111, 96, 1, 92, 112, 2, 105, 19, 69, 113, 106, 114, 107, 115, 23, 21, 53, 38, 6, 17, 116, 29, 11, 48, 117, 118, 119, 1, 81, 76, 77, 61, 4, 70, 120, 36, 4, 121, 15, 54, 82, 40, 12, 13, 27, 2, 12, 13, 44, 55]\n","157 \t [159, 41, 142, 1, 47, 314, 165, 97, 1996, 1997, 1998, 603, 1999, 183, 319, 2, 381, 961, 2000, 346, 962, 803, 37, 708, 381, 86, 2001, 165, 2, 505, 36, 181, 704, 2002, 98, 3, 330, 81, 367, 2003, 504, 429, 254, 240, 305, 198, 184, 22, 2004, 962, 125, 963, 214, 7, 5, 20, 65, 146]\n","158 \t [91, 50, 109, 19, 84, 58, 72, 45, 52, 60, 110, 47, 111, 96, 1, 92, 112, 2, 105, 19, 69, 113, 106, 114, 107, 115, 23, 21, 53, 38, 6, 17, 116, 29, 11, 48, 117, 118, 119, 1, 81, 76, 77, 61, 4, 70, 120, 36, 4, 121, 15, 54, 82, 40, 12, 13, 27, 2, 12, 13, 44, 55]\n","159 \t [91, 50, 109, 19, 84, 58, 72, 45, 52, 60, 110, 47, 111, 96, 1, 92, 112, 2, 105, 19, 69, 113, 106, 114, 107, 115, 23, 21, 53, 38, 6, 17, 116, 29, 11, 48, 117, 118, 119, 1, 81, 76, 77, 61, 4, 70, 120, 36, 4, 121, 15, 54, 82, 40, 12, 13, 27, 2, 12, 13, 44, 55]\n","160 \t [792, 47, 314, 4, 933, 2005, 70, 1, 907, 2006, 432, 668, 32, 337, 898, 601, 2007, 588, 2008, 2009, 286, 2010, 374, 665, 81, 254, 240, 908, 442, 2011, 2012, 363, 604, 789, 790, 198, 184, 278, 668, 2013, 911, 47, 36, 1, 2014, 37, 281, 2015, 330, 1, 171, 16, 56, 214, 7, 5, 20, 65, 146]\n","161 \t [91, 50, 109, 19, 84, 58, 72, 45, 52, 60, 110, 47, 111, 96, 1, 92, 112, 2, 105, 19, 69, 113, 106, 114, 107, 115, 23, 21, 53, 38, 6, 17, 116, 29, 11, 48, 117, 118, 119, 1, 81, 76, 77, 61, 4, 70, 120, 36, 4, 121, 15, 54, 82, 40, 12, 13, 27, 2, 12, 13, 44, 55, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 37, 2025, 2026, 2027]\n","162 \t [122, 31, 46, 10, 143, 129, 210, 93, 14, 2028, 890, 2029, 139, 6, 162, 78, 21, 108, 123, 156, 157, 71, 158, 7, 5, 20, 144, 51, 168, 64, 266, 3, 251, 124, 267, 45, 187, 15, 3, 179, 62, 57, 268, 1, 47, 163, 16, 94, 34, 75, 269, 30, 614, 2030, 3, 10, 39, 37, 164, 9, 194, 270, 132, 211, 11, 134, 135, 8, 271, 1, 124, 272, 180, 8, 273, 34, 239, 7, 5, 20, 274, 51, 252, 275]\n","163 \t [91, 50, 109, 19, 84, 58, 72, 45, 52, 60, 110, 47, 111, 96, 1, 92, 112, 2, 105, 19, 69, 113, 106, 114, 107, 115, 23, 21, 53, 38, 6, 17, 116, 29, 11, 48, 117, 118, 119, 1, 81, 76, 77, 61, 4, 70, 120, 36, 4, 121, 15, 54, 82, 40, 12, 13, 27, 2, 12, 13, 44, 55, 64, 266, 3, 251, 124, 267, 45, 187, 15, 3, 179, 62, 57, 268, 1, 47, 163, 16, 94, 34, 75, 269, 30, 856, 2031, 3, 10, 39, 37, 164, 9, 194, 270, 132, 211, 11, 134, 135, 8, 271, 1, 124, 272, 180, 8, 273, 34, 239, 7, 5, 20, 274, 51, 252, 275]\n","164 \t [127, 30, 99, 9, 100, 23, 101, 501, 502, 426, 61, 4, 361, 22, 320, 427, 147, 32, 359, 15, 360, 253, 303, 33, 16, 56, 5, 18, 26]\n","165 \t [949, 45, 2032, 2033, 136, 918, 2034, 2035, 1, 2036, 964, 2037, 2038, 605, 2039, 279, 6, 2040, 2041, 952, 665, 1, 756, 130, 2042, 30, 637, 2043, 2044, 344, 77, 61, 4, 70, 403, 637, 432, 2045, 787, 964, 2046, 2047, 34, 2048, 16, 56, 214, 7, 5, 20, 65, 146, 67]\n","166 \t [67, 164, 9, 345, 137, 79, 23, 346, 61, 4, 213, 3, 183, 292, 24, 80, 256, 24, 49, 33, 7, 5, 18, 26]\n","167 \t [2049, 160, 2050, 31, 341, 196, 2051, 3, 437, 88, 43, 1, 957, 93, 375, 78, 21, 108, 185, 22, 155, 203, 25, 2052, 334, 155, 389, 2053, 2, 480, 155, 2054, 632, 2055, 306, 2056, 2057, 2058, 2059, 2060, 126, 3, 178, 88, 131, 1, 283, 2061, 2062, 2063, 126, 3, 178, 88, 131, 1, 283, 2064, 965, 2065, 126, 3, 178, 88, 131, 1, 283, 2066, 2067, 2068, 126, 3, 178, 88, 131, 1, 283, 2069, 889, 2070, 126, 3, 178, 88, 131, 1, 283, 2071, 513, 2072, 126, 3, 178, 88, 131, 1, 283, 2073, 513, 2074, 126, 3, 178, 88, 131, 1, 283, 2075, 2076, 2077, 126, 3, 178, 88, 131, 1, 283, 2078, 2079, 2080, 126, 3, 178, 88, 131, 1, 283, 2081, 779, 2082, 126, 3, 178, 88, 131, 1, 283, 2083, 2084, 2085, 126, 3, 133, 88, 131, 1, 134, 135, 2086, 2087, 2088, 126, 3, 133, 88, 131, 1, 134, 135, 2089, 2090, 2091, 126, 3, 133, 88, 131, 1, 134, 135, 2092, 2093, 2094, 126, 3, 133, 88, 131, 1, 134, 135, 2095, 2096, 2097, 126, 3, 133, 88, 131, 1, 134, 135, 2098, 452, 2099, 126, 3, 133, 88, 131, 1, 134, 135, 2100, 2101, 781, 126, 3, 133, 88, 131, 1, 134, 135, 2102, 807, 2103, 126, 3, 133, 88, 131, 1, 134, 135, 2104, 765, 2105, 126, 3, 133, 88, 131, 1, 134, 135, 2106, 484, 2107, 126, 3, 133, 88, 131, 1, 134, 135, 523, 430, 2108, 445, 85, 1, 103, 104, 35, 2109, 648, 7, 5, 242, 175, 434, 376, 4, 243, 147, 261, 7, 5, 20, 377, 872, 873, 1, 152, 634, 8, 190, 874, 875, 25, 876, 877, 159, 878, 4, 879, 880, 85, 140, 37, 635, 503, 354, 314, 1, 92, 881, 882]\n","168 \t [127, 30, 99, 9, 100, 23, 101, 53, 38, 6, 17, 29, 11, 48, 43, 149, 15, 54, 150, 40, 12, 13, 27, 2, 12, 13, 44, 55, 32, 161, 33, 16, 56, 2, 141, 59, 24, 5, 18, 26]\n","169 \t [67, 442, 198, 2, 184, 189, 97, 279, 38, 36, 4, 70, 35, 2110, 710, 2111, 306, 291, 477, 9, 478, 479, 160, 166, 308, 324, 73, 172, 49, 147, 22, 966, 43, 7, 5, 18, 26, 530, 19, 37, 2112, 2113, 925, 542, 1, 870, 326]\n","170 \t [91, 50, 109, 19, 84, 58, 72, 45, 52, 60, 110, 47, 111, 96, 1, 92, 112, 2, 105, 19, 69, 113, 106, 114, 107, 115, 23, 21, 53, 38, 6, 17, 116, 29, 11, 48, 117, 118, 119, 1, 81, 76, 77, 61, 4, 70, 120, 36, 4, 121, 15, 54, 82, 40, 12, 13, 27, 2, 12, 13, 44, 55]\n","171 \t [67]\n","172 \t [2, 2114, 127, 30, 99, 9, 100, 23, 101, 53, 38, 6, 17, 29, 11, 48, 43, 149, 15, 54, 150, 40, 12, 13, 27, 2, 12, 13, 44, 55, 32, 161, 33, 16, 56, 2, 141, 59, 24, 5, 18, 26]\n","173 \t [27, 2115, 263, 2116, 169, 45, 2117, 2118, 834, 1, 153, 374]\n","174 \t [281, 2119, 520, 2120, 3, 571, 827, 2121, 1, 153, 374, 151, 22, 245, 132, 246, 202, 74, 10, 66, 247, 248, 89, 249, 31, 46, 10, 66, 128, 3, 90, 250, 218, 71, 102, 8, 35, 102, 142, 219, 59, 10, 176, 49, 220, 221, 8, 3, 10, 222, 58, 223, 190, 224, 8, 31, 191, 225, 152, 95, 8, 39, 1, 226, 203, 227, 204, 74, 66, 83, 31, 128, 3, 90, 95, 8, 15, 4, 205, 34, 75, 228, 1, 103, 104, 8, 206, 10, 229, 1, 153, 68, 230, 231, 207, 8, 232, 154, 208, 17, 233, 234, 192, 14, 209, 138, 46, 6, 235, 167, 21, 236, 2, 237, 238, 193]\n","175 \t [27, 2122, 317, 351, 409, 204, 531, 2123, 339, 145, 212, 98, 27, 664, 636, 383, 23, 125, 532, 36, 344, 967, 60, 217, 17, 29, 11, 385, 177, 28, 16, 94, 2124, 11, 436, 182, 200, 28, 16, 94, 2125, 32, 1, 386, 263, 198, 184, 967, 344, 278, 289, 216, 15, 638, 40, 98, 17, 301, 29, 2, 9, 296, 4, 297, 425, 262, 24, 34, 16, 56, 214, 7, 5, 20, 65, 575]\n","176 \t [290, 585, 165, 586, 726, 284, 727, 280, 728, 729, 730, 4, 731, 732, 733, 411, 485, 734, 486, 735, 736, 737, 568, 738, 412, 357, 487, 488, 739, 740, 741, 4, 742, 136, 97, 34, 371, 2, 1, 743, 744, 136, 64, 745, 337, 10, 487, 310, 25, 746, 587, 39, 37, 44, 293, 747, 10, 176, 3, 489, 25, 23, 15, 3, 36, 748, 411, 485, 749, 486, 2, 159, 750, 64, 588, 24, 751, 752, 2126, 2127, 41, 2128, 52, 517, 25, 2129, 86, 2, 811, 300, 63, 2130, 37, 600, 9, 2131, 2132, 703, 182, 97, 843, 844, 300, 2133, 382, 2134, 260, 431, 41, 935, 2135, 289, 171, 285, 5, 18, 491]\n","177 \t [122, 31, 46, 10, 143, 129, 154, 93, 14, 2136, 806, 2137, 139, 6, 162, 78, 21, 108, 123, 156, 157, 71, 158, 7, 5, 20, 144, 51, 168]\n","178 \t [137, 63, 412, 280, 626, 262, 380, 257, 199, 368, 212, 41, 646, 645, 916, 2, 262, 380, 34, 98, 1, 481, 424, 241, 2138, 290, 1, 241, 577, 41, 97, 819, 60, 922, 39, 311, 2139, 160, 166, 68, 2140, 83, 33, 2141, 125, 2142, 2143, 2144, 160, 217, 19, 45, 257, 199, 37, 281, 2145, 22, 2146, 2147, 2148, 617, 293, 2, 2149, 86, 91, 57, 125, 2150, 842, 332, 633, 33, 2, 262, 380, 3, 257, 199, 34, 16, 56, 7, 5, 18, 915, 52, 2151, 50, 574, 929, 930, 32, 388, 2152, 310, 64, 2153, 2154, 3, 2155, 2, 2156, 79, 885, 2157, 189, 410, 762, 2158, 35, 2159, 97, 340, 284, 958, 327, 294, 382, 42, 260, 6, 2160, 857, 19, 569, 2161, 574, 2162, 176, 3, 489, 265, 2163]\n","179 \t [2164, 1, 849, 850, 772, 2165, 2166, 580, 913, 914, 816, 85, 2167, 41, 188, 42, 2168, 2169, 332, 39, 2170, 422, 185]\n","180 \t [38, 36, 4, 213, 2, 666, 768, 340, 769, 770, 291, 477, 9, 478, 479, 160, 166, 308, 324, 73, 172, 49, 33, 22, 2171, 43, 7, 5, 18, 26]\n","181 \t [151, 869, 525, 85, 216, 535, 2172, 435, 64, 266, 3, 251, 124, 267, 45, 187, 15, 3, 179, 62, 57, 268, 1, 47, 163, 16, 94, 34, 75, 269, 30, 906, 3, 10, 39, 37, 164, 9, 194, 270, 132, 211, 11, 134, 135, 8, 271, 1, 124, 272, 180, 8, 273, 34, 239, 7, 5, 20, 274, 51, 252, 275, 91, 189]\n","182 \t [127, 30, 99, 9, 100, 23, 101, 53, 38, 6, 17, 29, 11, 48, 43, 149, 15, 54, 150, 40, 12, 13, 27, 2, 12, 13, 44, 55, 32, 161, 33, 16, 56, 2, 141, 59, 24, 5, 18, 26, 836, 622, 523, 108, 46, 815, 2173, 103, 4, 2174, 2175, 2176, 1, 2177, 2, 2178, 3, 2179, 2180, 4, 2181, 2182, 2183, 2184, 2, 2185, 265, 837, 2186, 63, 41, 97, 838, 9, 2187, 2188, 57, 85, 2189, 9, 331, 16, 2190, 9, 2191, 2192, 2193, 97, 2194, 2195, 622, 69, 2196, 2, 2197, 2198, 2199, 758, 2200, 1, 2201, 265, 104, 7, 5, 242, 175, 2202, 2203, 25, 9, 2204, 1, 2205, 2206, 2207, 79, 757, 587, 2208, 2, 2209, 920, 22, 2210, 2211, 69, 2212, 2213]\n","183 \t [2214, 912, 57, 2215, 2216, 2217, 4, 293, 30, 140, 9, 2218, 1, 2219, 642, 262, 390, 618, 2220, 171, 285, 5, 18, 491]\n","184 \t [67, 91, 50, 109, 19, 84, 58, 72, 45, 52, 60, 110, 47, 111, 96, 1, 92, 112, 2, 105, 19, 69, 113, 106, 114, 107, 115, 23, 21, 53, 38, 6, 17, 116, 29, 11, 48, 117, 118, 119, 1, 81, 76, 77, 61, 4, 70, 120, 36, 4, 121, 15, 54, 82, 40, 12, 13, 27, 2, 12, 13, 44, 55, 541, 480, 9, 68, 2221, 277, 615, 2222, 4, 133, 2223, 212, 41, 137, 79, 280, 2224, 309, 32, 36, 77, 61, 4, 213, 57, 2225, 556, 32, 36, 2226, 61, 4, 2227, 1, 62, 22, 320, 427, 147, 32, 62, 3, 536, 57, 556, 11, 2228, 43, 32, 38, 6, 57, 556, 206, 2229, 32, 211, 22, 2230, 3, 489, 265, 104, 2231, 34, 776, 16, 56, 145, 7, 5, 20, 65, 146]\n","185 \t [122, 31, 46, 10, 143, 129, 154, 93, 14, 2232, 2233, 2234, 139, 6, 162, 151, 25, 579, 2235, 209, 138, 46, 10, 66, 492, 1, 2236, 2237, 68, 2238, 6, 321, 2239, 218, 71, 102, 8, 35, 102, 142, 219, 59, 10, 176, 49, 220, 221, 8, 3, 10, 222, 58, 223, 190, 224, 8, 31, 191, 225, 152, 95, 8, 39, 1, 226, 203, 227, 204, 74, 66, 83, 31, 128, 3, 90, 95, 8, 15, 4, 205, 34, 75, 228, 1, 103, 104, 8, 206, 10, 229, 1, 153, 68, 230, 231, 207, 8, 232, 154, 208, 17, 233, 234, 192, 167, 21, 236, 2, 237, 238, 193, 91, 50, 109, 19, 84, 58, 72, 45, 52, 60, 110, 47, 111, 96, 1, 92, 112, 2, 105, 19, 69, 113, 106, 114, 107, 115, 23, 21, 53, 38, 6, 17, 116, 29, 11, 48, 117, 118, 119, 1, 81, 76, 77, 61, 4, 70, 120, 36, 4, 121, 15, 54, 82, 40, 12, 13, 27, 2, 12, 13, 44, 55]\n","186 \t [67]\n","187 \t [86, 620, 286, 300, 2240, 78, 22, 923, 390, 284, 2241, 23, 42, 140, 956, 2242, 3, 2243, 312, 214, 212, 98, 27, 2244, 643, 383, 23, 36, 2245, 2246, 16, 94, 2247, 17, 29, 11, 48, 43, 1, 424, 241, 60, 32, 1, 386, 263, 526, 384, 785, 278, 668, 198, 184, 289, 22, 205, 963, 214, 7, 2248, 2249, 2250, 20, 2251, 2252, 127, 30, 99, 9, 100, 23, 101, 53, 38, 6, 17, 29, 11, 48, 43, 149, 15, 54, 150, 40, 12, 13, 27, 2, 12, 13, 44, 55, 32, 161, 33, 16, 56, 2, 141, 59, 24, 5, 18, 26]\n","188 \t [683, 684, 685, 686, 564, 687, 688, 565, 4, 689, 19, 3, 319, 172, 42, 2, 566, 690, 365, 691, 567, 25, 288, 140, 37, 692, 693]\n","189 \t [3, 366, 2253, 2254, 2255, 2256, 2257, 897, 2, 39, 37, 27, 2258, 2259, 2260, 2261, 41, 600, 884, 2262, 84, 58, 1, 921, 83, 37, 323, 9, 2263, 366, 2264, 2265, 2266, 2267, 264, 428, 45, 2268, 63]\n","190 \t [968, 529, 2269, 2270, 35, 63, 25, 2271, 6, 2272, 38, 1, 560, 561, 32, 77, 32, 77, 4, 2273, 32, 77, 4, 70, 80, 515, 49, 500, 33, 7, 5, 18, 26, 151, 22, 245, 132, 246, 202, 74, 10, 66, 247, 248, 89, 249, 31, 46, 10, 66, 128, 3, 90, 250, 218, 71, 102, 8, 35, 102, 142, 219, 59, 10, 176, 49, 220, 221, 8, 3, 10, 222, 58, 223, 190, 224, 8, 31, 191, 225, 152, 95, 8, 39, 1, 226, 203, 227, 204, 74, 66, 83, 31, 128, 3, 90, 95, 8, 15, 4, 205, 34, 75, 228, 1, 103, 104, 8, 206, 10, 229, 1, 153, 68, 230, 231, 207, 8, 232, 154, 208, 17, 233, 234, 192, 14, 209, 138, 46, 6, 235, 167, 21, 236, 2, 237, 238, 193]\n","191 \t [2274, 557, 672, 85, 1, 673, 326, 2275, 2276, 3, 2277, 2278, 2279, 969, 2280, 4, 2281, 316, 2282, 11, 74, 88, 27, 2283, 88, 347, 969, 647, 2284, 316, 4, 2285, 133, 342, 2286, 3, 160, 970, 2287, 347, 971, 16, 2288, 970, 2, 2289, 2290, 2291, 2292, 347, 971, 558, 316, 178, 155, 155, 155, 177, 155, 2293, 3, 2294, 196, 258, 2295, 74, 2296, 2297, 196, 258, 304, 74, 2298, 2299, 2300, 16, 2301, 2302, 4, 196, 258, 27, 407, 258, 2303, 25, 3, 2304, 7, 675, 676, 192, 2305, 2306, 2307, 2308, 677]\n","192 \t [64, 266, 3, 251, 124, 267, 45, 187, 15, 3, 179, 62, 57, 268, 1, 47, 163, 16, 94, 34, 75, 269, 30, 2309, 3, 10, 39, 37, 164, 9, 194, 270, 132, 211, 11, 134, 135, 8, 271, 1, 124, 272, 180, 8, 273, 34, 239, 7, 5, 20, 274, 51, 252, 275]\n","193 \t [67, 122, 31, 341, 667, 438, 30, 93, 375, 78, 21, 108, 185, 1, 47, 163, 63, 334, 177, 608, 609, 506, 507, 124, 4, 508, 509, 311, 37, 510, 307, 1, 511, 136, 9, 187, 15, 298, 85, 1, 103, 104, 7, 5, 242, 175, 14, 2310, 2311, 2312, 6, 173, 174, 201, 28, 14, 2313, 2314, 2315, 6, 173, 174, 201, 28, 14, 2316, 2317, 2318, 6, 173, 174, 201, 28, 14, 2319, 513, 2320, 6, 660, 661, 130, 28, 14, 2321, 965, 2322, 6, 446, 447, 130, 28, 14, 2323, 780, 2324, 6, 446, 447, 130, 28, 14, 2325, 972, 2326, 6, 446, 447, 130, 28, 14, 2327, 2328, 2329, 6, 554, 2, 555, 130, 28, 14, 2330, 2331, 2332, 6, 554, 2, 555, 130, 28, 14, 2333, 2334, 2335, 6, 554, 2, 555, 130, 28, 14, 2336, 2337, 2338, 6, 448, 186, 28, 14, 2339, 452, 2340, 6, 448, 186, 28, 14, 2341, 972, 2342, 6, 448, 186, 28, 416, 343, 430, 1, 514, 434, 376, 4, 243, 147, 261, 7, 5, 20, 377]\n","194 \t [67]\n","195 \t [91, 50, 109, 19, 84, 58, 72, 45, 52, 60, 110, 47, 111, 96, 1, 92, 112, 2, 105, 19, 69, 113, 106, 114, 107, 115, 23, 21, 53, 38, 6, 17, 116, 29, 11, 48, 117, 118, 119, 1, 81, 76, 77, 61, 4, 70, 120, 36, 4, 121, 15, 54, 82, 40, 12, 13, 27, 2, 12, 13, 44, 55]\n","196 \t [67, 2343, 22, 2344, 2345, 2346, 217, 2347, 3, 802, 50, 694, 476, 421, 308, 4, 2348, 2349, 255, 73, 60, 305, 198, 2, 184, 189, 97, 279, 38, 36, 4, 213, 488, 2350, 647, 2351, 306, 172, 49, 147, 22, 966, 43, 7, 5, 18, 26]\n","197 \t [67]\n","198 \t [151, 22, 245, 132, 246, 202, 74, 10, 66, 247, 248, 89, 249, 31, 46, 10, 66, 128, 3, 90, 250, 218, 71, 102, 8, 35, 102, 142, 219, 59, 10, 176, 49, 220, 221, 8, 3, 10, 222, 58, 223, 190, 224, 8, 31, 191, 225, 152, 95, 8, 39, 1, 226, 203, 227, 204, 74, 66, 83, 31, 128, 3, 90, 95, 8, 15, 4, 205, 34, 75, 228, 1, 103, 104, 8, 206, 10, 229, 1, 153, 68, 230, 231, 207, 8, 232, 154, 208, 17, 233, 234, 192, 14, 209, 138, 46, 6, 321, 362, 167, 21, 236, 2, 237, 238, 193, 91, 50, 109, 19, 84, 58, 72, 45, 52, 60, 110, 47, 111, 96, 1, 92, 112, 2, 105, 19, 69, 113, 106, 114, 107, 115, 23, 21, 53, 38, 6, 17, 116, 29, 11, 48, 117, 118, 119, 1, 81, 76, 77, 61, 4, 70, 120, 36, 4, 121, 15, 54, 82, 40, 12, 13, 27, 2, 12, 13, 44, 55]\n","199 \t [968, 529, 335, 388, 2352, 961, 2353, 35, 366, 2354, 30, 384, 260, 442, 172, 49, 33, 7, 5, 18, 26]\n","200 \t [127, 30, 99, 9, 100, 23, 101, 53, 38, 6, 17, 29, 11, 48, 43, 149, 15, 54, 150, 40, 12, 13, 27, 2, 12, 13, 44, 55, 32, 161, 33, 16, 56, 2, 141, 59, 24, 5, 18, 26]\n"]}]},{"cell_type":"markdown","source":["–ü—Ä–∏–≤–æ–¥–∏–º –¥–ª–∏–Ω—ã –Ω–∞–∑–≤–∞–Ω–∏–π –∫ –æ–¥–Ω–æ–º—É –∑–Ω–∞—á–µ–Ω–∏—é"],"metadata":{"id":"vTvuXJWEecip"}},{"cell_type":"code","source":["x_train = pad_sequences(sequences, maxlen = max1)\n","x_train"],"metadata":{"id":"onNAjxJLeZL_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711866952738,"user_tz":-300,"elapsed":22,"user":{"displayName":"Andrey Sokolov","userId":"00506076773780018870"}},"outputId":"b7059ed9-a138-4e25-89bb-ed18697b13dc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[  0,   0,   0, ...,   5,  18,  26],\n","       [  0,   0,   0, ...,  13,  44,  55],\n","       [  0,   0,   0, ..., 670, 178, 982],\n","       ...,\n","       [  0,   0,   0, ...,  13,  44,  55],\n","       [  0,   0,   0, ...,   5,  18,  26],\n","       [  0,   0,   0, ...,   5,  18,  26]], dtype=int32)"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["%tensorflow_version 2.x\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, MaxPooling1D, Conv1D, GlobalMaxPooling1D, Dropout, LSTM, GRU\n","from tensorflow.keras import utils\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras import utils\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A5X47LQlf1wv","executionInfo":{"status":"ok","timestamp":1711866952739,"user_tz":-300,"elapsed":7,"user":{"displayName":"Andrey Sokolov","userId":"00506076773780018870"}},"outputId":"a3b99649-f396-4c4d-adc1-9a9edf859822"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"]}]},{"cell_type":"markdown","source":["# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ —Å –ø–æ–º–æ—â—å—é Keras Tuner"],"metadata":{"id":"HBhFCu9oGXck"}},{"cell_type":"code","source":["!pip install -U keras-tuner"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uv_mrsLBGU8J","executionInfo":{"status":"ok","timestamp":1711866960379,"user_tz":-300,"elapsed":7644,"user":{"displayName":"Andrey Sokolov","userId":"00506076773780018870"}},"outputId":"39d6323b-a53a-4bb5-a8fa-491f102e0417"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting keras-tuner\n","  Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n","\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.4/129.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n","Collecting kt-legacy (from keras-tuner)\n","  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.2.2)\n","Installing collected packages: kt-legacy, keras-tuner\n","Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"]}]},{"cell_type":"code","source":["%tensorflow_version 2.x\n","from tensorflow.keras.datasets import fashion_mnist\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras import utils\n","from google.colab import files\n","from kerastuner.tuners import RandomSearch, Hyperband, BayesianOptimization\n","import numpy as np"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zox1BO2KH6qx","executionInfo":{"status":"ok","timestamp":1711866960379,"user_tz":-300,"elapsed":9,"user":{"displayName":"Andrey Sokolov","userId":"00506076773780018870"}},"outputId":"939a04cb-cefb-47a0-cb1d-ca7738ad8682"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-24-b7a017acafd2>:7: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n","  from kerastuner.tuners import RandomSearch, Hyperband, BayesianOptimization\n"]}]},{"cell_type":"markdown","source":["# –¢—é–Ω–µ—Ä –¥–ª—è —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏"],"metadata":{"id":"Ji6XLiKK8hOU"}},{"cell_type":"code","source":["#–±–∏–ª–¥–µ—Ä –º–æ–¥–µ–ª–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏\n","def build_model_cnn(hp):\n","    activation_choice = hp.Choice('activation', values=['relu', 'sigmoid', 'tanh', 'elu', 'selu'])\n","    model = Sequential()\n","\n","    model.add(Embedding(output_dim=hp.Int('output_dim',    # –≠–º–±–µ–¥–¥–∏–Ω–≥ - —Å–ª–æ–π —Å —Ä–∞–∑–Ω—ã–º–∏ –¥–ª–∏–Ω–∞–º–∏ –ø–ª–æ—Ç–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞\n","                                   min_value=8,\n","                                   max_value=32,\n","                                   step=4),\n","                    input_dim=num_words,\n","                    input_length=max1))\n","\n","    model.add(Conv1D(filters= hp.Int('filters',    # –°–≤—ë—Ä—Ç–æ—á–Ω—ã–π —Å–ª–æ–π —Å —Ä–∞–∑–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ñ–∏–ª—å—Ç—Ä–æ–≤\n","                                   min_value=100,\n","                                   max_value=400,\n","                                   step=50),\n","                    kernel_size = hp.Int('kernel_size',    # –æ–Ω –∂–µ, —Å —Ä–∞–∑–Ω—ã–º —Ä–∞–∑–º–µ—Ä–æ–º —è–¥—Ä–∞ —Å–≤—ë—Ä—Ç–∫–∏\n","                                   min_value=3,\n","                                   max_value=8,\n","                                   step=1),\n","                    padding = 'valid',\n","                    activation = activation_choice))\n","\n","    model.add(GlobalMaxPooling1D())\n","\n","    model.add(Dense(units=hp.Int('units_hidden',\n","                                   min_value=64,\n","                                   max_value=512,\n","                                   step=32),\n","                    activation=activation_choice))\n","\n","    model.add(Dense(5, activation='softmax'))\n","\n","    model.compile(\n","        optimizer=hp.Choice('optimizer', values=['adam','rmsprop','SGD']),\n","        loss='categorical_crossentropy',\n","        metrics=['accuracy'])\n","    return model"],"metadata":{"id":"4NtUwQtTJo3W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#–±–∏–ª–¥–µ—Ä –º–æ–¥–µ–ª–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ LSTM\n","def build_model_lstm(hp):\n","    activation_choice = hp.Choice('activation', values=['relu', 'sigmoid', 'tanh', 'elu', 'selu'])\n","    recurrent_activation_choice = hp.Choice('recurrent_activation', values=['relu', 'sigmoid', 'tanh', 'elu', 'selu'])\n","    model = Sequential()\n","\n","    model.add(Embedding(output_dim=hp.Int('output_dim',    # –≠–º–±–µ–¥–¥–∏–Ω–≥ - —Å–ª–æ–π —Å —Ä–∞–∑–Ω—ã–º–∏ –¥–ª–∏–Ω–∞–º–∏ –ø–ª–æ—Ç–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞\n","                                   min_value=8,\n","                                   max_value=32,\n","                                   step=4),\n","                    input_dim=num_words,\n","                    input_length=max1))\n","\n","    model.add(LSTM(units= hp.Int('units',    # –°–≤—ë—Ä—Ç–æ—á–Ω—ã–π —Å–ª–æ–π —Å —Ä–∞–∑–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ñ–∏–ª—å—Ç—Ä–æ–≤\n","                                   min_value=8,\n","                                   max_value=32,\n","                                   step=4),\n","                    recurrent_activation = recurrent_activation_choice,\n","                    activation = activation_choice))\n","\n","    model.add(Dense(units=hp.Int('units_hidden',\n","                                   min_value=64,\n","                                   max_value=512,\n","                                   step=32),\n","                    activation=activation_choice))\n","\n","    model.add(Dense(5, activation='softmax'))\n","\n","    model.compile(\n","        optimizer=hp.Choice('optimizer', values=['adam','rmsprop','SGD']),\n","        loss='categorical_crossentropy',\n","        metrics=['accuracy'])\n","    return model"],"metadata":{"id":"t2sYI0F4Coh1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#–±–∏–ª–¥–µ—Ä –º–æ–¥–µ–ª–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ GRU\n","def build_model_gru(hp):\n","    activation_choice = hp.Choice('activation', values=['relu', 'sigmoid', 'tanh', 'elu', 'selu'])\n","    recurrent_activation_choice = hp.Choice('recurrent_activation', values=['relu', 'sigmoid', 'tanh', 'elu', 'selu'])\n","    model = Sequential()\n","\n","    model.add(Embedding(output_dim=hp.Int('output_dim',    # –≠–º–±–µ–¥–¥–∏–Ω–≥ - —Å–ª–æ–π —Å —Ä–∞–∑–Ω—ã–º–∏ –¥–ª–∏–Ω–∞–º–∏ –ø–ª–æ—Ç–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞\n","                                   min_value=8,\n","                                   max_value=32,\n","                                   step=4),\n","                    input_dim=num_words,\n","                    input_length=max1))\n","\n","    model.add(GRU(units= hp.Int('units',    # –°–ª–æ–π GRU —Å —Ä–∞–∑–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —è—á–µ–µ–∫\n","                                   min_value=8,\n","                                   max_value=32,\n","                                   step=4),\n","                    recurrent_activation = recurrent_activation_choice,\n","                    activation = activation_choice))\n","\n","    model.add(Dense(units=hp.Int('units_hidden',\n","                                   min_value=64,\n","                                   max_value=512,\n","                                   step=32),\n","                    activation=activation_choice))\n","\n","    model.add(Dense(5, activation='softmax'))\n","\n","    model.compile(\n","        optimizer=hp.Choice('optimizer', values=['adam','rmsprop','SGD']),\n","        loss='categorical_crossentropy',\n","        metrics=['accuracy'])\n","    return model"],"metadata":{"id":"A7APbM8SMMiA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#–°–æ–∑–¥–∞–Ω–∏–µ —Ç—é–Ω–µ—Ä–∞ —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–π —Å–µ—Ç–∏\n","tuner_cnn = BayesianOptimization(\n","    build_model_cnn,                 # —Ñ—É–Ω–∫—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏\n","    objective='val_accuracy',    # –º–µ—Ç—Ä–∏–∫–∞, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å -\n","                                 # –¥–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –ø—Ä–æ–≤–µ—Ä–æ—á–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö\n","    max_trials=500,               # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—É—Å–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è\n","    directory='test_directory_cnn' ,   # –∫–∞—Ç–∞–ª–æ–≥, –∫—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –æ–±—É—á–µ–Ω–Ω—ã–µ —Å–µ—Ç–∏\n","    overwrite=False\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QtP-0i9s4bYs","executionInfo":{"status":"ok","timestamp":1711867071153,"user_tz":-300,"elapsed":110221,"user":{"displayName":"Andrey Sokolov","userId":"00506076773780018870"}},"outputId":"2c995a4f-4341-4f19-be4f-12fc8d450c52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reloading Tuner from test_directory_cnn/untitled_project/tuner0.json\n"]}]},{"cell_type":"code","source":["#–°–æ–∑–¥–∞–Ω–∏–µ —Ç—é–Ω–µ—Ä–∞ LSTM - —Å–µ—Ç–∏\n","# tuner_lstm = BayesianOptimization(\n","#     build_model_lstm,                 # —Ñ—É–Ω–∫—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏\n","#     objective='val_accuracy',    # –º–µ—Ç—Ä–∏–∫–∞, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å -\n","#                                  # –¥–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –ø—Ä–æ–≤–µ—Ä–æ—á–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö\n","#     max_trials=500,               # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—É—Å–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è\n","#     directory='test_directory',   # –∫–∞—Ç–∞–ª–æ–≥, –∫—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –æ–±—É—á–µ–Ω–Ω—ã–µ —Å–µ—Ç–∏\n","#     overwrite=False\n","#     )"],"metadata":{"id":"T4R0XUhfKub2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["–°–æ–∑–¥–∞–Ω–∏–µ —Ç—é–Ω–µ—Ä–∞ GRU - —Å–µ—Ç–∏\n","tuner_gru = BayesianOptimization(\n","    build_model_gru,                 # —Ñ—É–Ω–∫—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏\n","    objective='val_accuracy',    # –º–µ—Ç—Ä–∏–∫–∞, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å -\n","                                 # –¥–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –ø—Ä–æ–≤–µ—Ä–æ—á–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö\n","    max_trials=500,               # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—É—Å–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è\n","    directory='test_directory',   # –∫–∞—Ç–∞–ª–æ–≥, –∫—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –æ–±—É—á–µ–Ω–Ω—ã–µ —Å–µ—Ç–∏\n","    overwrite=True\n","    )"],"metadata":{"id":"43ILRKJUOjXk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# –ó–∞–ø—É—Å–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤"],"metadata":{"id":"hdBmkUoqO-4v"}},{"cell_type":"code","source":["#–¢—é–Ω–µ—Ä —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–π —Å–µ—Ç–∏\n","tuner_cnn.search(x_train,                  # –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n","             y_train,                  # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã\n","             batch_size=128,           # –†–∞–∑–º–µ—Ä –º–∏–Ω–∏-–≤—ã–±–æ—Ä–∫–∏\n","             epochs=15,                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è\n","             validation_split=0.1,     # –ß–∞—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏\n","             )"],"metadata":{"id":"XH2jUl_h44na"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#–¢—é–Ω–µ—Ä LSTM\n","# tuner_lstm.search(x_train,                  # –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n","#              y_train,                  # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã\n","#              batch_size=128,           # –†–∞–∑–º–µ—Ä –º–∏–Ω–∏-–≤—ã–±–æ—Ä–∫–∏\n","#              epochs=15,                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è\n","#              validation_split=0.1,     # –ß–∞—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏\n","#              )"],"metadata":{"id":"pmC9fxPhPNbs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tuner_cnn.results_summary(60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FS2IH5Jz0ciA","executionInfo":{"status":"ok","timestamp":1711867448587,"user_tz":-300,"elapsed":309,"user":{"displayName":"Andrey Sokolov","userId":"00506076773780018870"}},"outputId":"56fef095-41d8-4b17-92fc-1926a1a1a2e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Results summary\n","Results in test_directory_cnn/untitled_project\n","Showing 60 best trials\n","Objective(name=\"val_accuracy\", direction=\"max\")\n","\n","Trial 036 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 350\n","kernel_size: 6\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 040 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 300\n","kernel_size: 7\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 042 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 448\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 048 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 7\n","units_hidden: 448\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 065 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 28\n","filters: 400\n","kernel_size: 8\n","units_hidden: 416\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 074 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 090 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 480\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 100 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 6\n","units_hidden: 448\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 101 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 7\n","units_hidden: 448\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 097 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 098 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 110 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 7\n","units_hidden: 448\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 103 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 7\n","units_hidden: 448\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 123 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 7\n","units_hidden: 448\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 127 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 480\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 138 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 141 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 151 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 350\n","kernel_size: 5\n","units_hidden: 480\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 154 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 350\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 157 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 160 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 173 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 174 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 350\n","kernel_size: 7\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 182 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 191 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 193 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 202 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 208 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 209 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 219 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 220 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 230 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 232 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 239 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 241 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 7\n","units_hidden: 448\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 252 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 7\n","units_hidden: 448\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 124 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 480\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 299 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 305 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 145 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 146 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 5\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 147 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 169 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 203 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 260 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 204 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 257 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 212 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 7\n","units_hidden: 448\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 216 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 217 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 350\n","kernel_size: 7\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 243 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 350\n","kernel_size: 7\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 221 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 233 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 229 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9523809552192688\n","\n","Trial 019 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 28\n","filters: 150\n","kernel_size: 7\n","units_hidden: 480\n","optimizer: rmsprop\n","Score: 0.9047619104385376\n","\n","Trial 029 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 200\n","kernel_size: 6\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9047619104385376\n","\n","Trial 045 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 350\n","kernel_size: 8\n","units_hidden: 448\n","optimizer: rmsprop\n","Score: 0.9047619104385376\n","\n","Trial 046 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 28\n","filters: 400\n","kernel_size: 7\n","units_hidden: 384\n","optimizer: rmsprop\n","Score: 0.9047619104385376\n","\n","Trial 041 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 32\n","filters: 300\n","kernel_size: 6\n","units_hidden: 416\n","optimizer: rmsprop\n","Score: 0.9047619104385376\n","\n","Trial 043 summary\n","Hyperparameters:\n","activation: selu\n","output_dim: 28\n","filters: 400\n","kernel_size: 8\n","units_hidden: 512\n","optimizer: rmsprop\n","Score: 0.9047619104385376\n"]}]},{"cell_type":"code","source":["models = tuner_cnn.get_best_models(num_models=60)"],"metadata":{"id":"B6q7sD0vvqxo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_descrs = test['–¢–µ–∫—Å—Ç']\n","test_sequences = tokenizer.texts_to_sequences(test_descrs)\n","x_test = pad_sequences(test_sequences, maxlen = max1)"],"metadata":{"id":"RSYsZfGpyON9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# models[0].summary()"],"metadata":{"id":"tFIG2sqd_OM7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for model in models:\n","  model.summary()\n","  model.evaluate(x_test, y_test)\n","  print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7XFxnA6YvuDM","executionInfo":{"status":"ok","timestamp":1711867790922,"user_tz":-300,"elapsed":33583,"user":{"displayName":"Andrey Sokolov","userId":"00506076773780018870"}},"outputId":"c54ef21f-9619-421f-de89-a323594a9c0e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 295, 350)          67550     \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 350)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               179712    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3449827 (13.16 MB)\n","Trainable params: 3449827 (13.16 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 1s 49ms/step - loss: 0.8871 - accuracy: 0.7093\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 294, 300)          67500     \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 300)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               154112    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3424177 (13.06 MB)\n","Trainable params: 3424177 (13.06 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 54ms/step - loss: 0.9348 - accuracy: 0.5930\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 448)               179648    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2245      \n","                                                                 \n","=================================================================\n","Total params: 3484693 (13.29 MB)\n","Trainable params: 3484693 (13.29 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 1s 53ms/step - loss: 1.0212 - accuracy: 0.5698\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 294, 400)          90000     \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 448)               179648    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2245      \n","                                                                 \n","=================================================================\n","Total params: 3471893 (13.24 MB)\n","Trainable params: 3471893 (13.24 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 1s 50ms/step - loss: 1.0043 - accuracy: 0.5698\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 28)           2800000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          90000     \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 416)               166816    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2085      \n","                                                                 \n","=================================================================\n","Total params: 3058901 (11.67 MB)\n","Trainable params: 3058901 (11.67 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x79db269b0b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["3/3 [==============================] - 1s 50ms/step - loss: 0.9047 - accuracy: 0.6628\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x79db269b1900> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["3/3 [==============================] - 0s 37ms/step - loss: 0.9133 - accuracy: 0.6977\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 480)               192480    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2405      \n","                                                                 \n","=================================================================\n","Total params: 3497685 (13.34 MB)\n","Trainable params: 3497685 (13.34 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 33ms/step - loss: 0.9573 - accuracy: 0.7093\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 295, 400)          77200     \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 448)               179648    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2245      \n","                                                                 \n","=================================================================\n","Total params: 3459093 (13.20 MB)\n","Trainable params: 3459093 (13.20 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 29ms/step - loss: 0.8067 - accuracy: 0.7093\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 294, 400)          90000     \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 448)               179648    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2245      \n","                                                                 \n","=================================================================\n","Total params: 3471893 (13.24 MB)\n","Trainable params: 3471893 (13.24 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 30ms/step - loss: 1.0959 - accuracy: 0.4302\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 32ms/step - loss: 0.9697 - accuracy: 0.5581\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 39ms/step - loss: 1.0854 - accuracy: 0.4419\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 294, 400)          90000     \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 448)               179648    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2245      \n","                                                                 \n","=================================================================\n","Total params: 3471893 (13.24 MB)\n","Trainable params: 3471893 (13.24 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 29ms/step - loss: 0.9073 - accuracy: 0.7326\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 294, 400)          90000     \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 448)               179648    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2245      \n","                                                                 \n","=================================================================\n","Total params: 3471893 (13.24 MB)\n","Trainable params: 3471893 (13.24 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 31ms/step - loss: 0.8470 - accuracy: 0.7209\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 294, 400)          90000     \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 448)               179648    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2245      \n","                                                                 \n","=================================================================\n","Total params: 3471893 (13.24 MB)\n","Trainable params: 3471893 (13.24 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 29ms/step - loss: 1.0649 - accuracy: 0.4302\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 480)               192480    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2405      \n","                                                                 \n","=================================================================\n","Total params: 3497685 (13.34 MB)\n","Trainable params: 3497685 (13.34 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 32ms/step - loss: 1.1326 - accuracy: 0.4302\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 32ms/step - loss: 0.8751 - accuracy: 0.7093\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 32ms/step - loss: 0.9127 - accuracy: 0.7093\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 296, 350)          56350     \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 350)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 480)               168480    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2405      \n","                                                                 \n","=================================================================\n","Total params: 3427235 (13.07 MB)\n","Trainable params: 3427235 (13.07 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 23ms/step - loss: 0.7874 - accuracy: 0.7558\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 350)          89950     \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 350)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               179712    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3472227 (13.25 MB)\n","Trainable params: 3472227 (13.25 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 29ms/step - loss: 0.8755 - accuracy: 0.7093\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 33ms/step - loss: 0.9618 - accuracy: 0.5814\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 35ms/step - loss: 0.9102 - accuracy: 0.7326\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 34ms/step - loss: 0.9489 - accuracy: 0.6860\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 294, 350)          78750     \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 350)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               179712    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3461027 (13.20 MB)\n","Trainable params: 3461027 (13.20 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 28ms/step - loss: 0.8333 - accuracy: 0.7093\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 32ms/step - loss: 0.8347 - accuracy: 0.7093\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 35ms/step - loss: 0.8125 - accuracy: 0.7442\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 48ms/step - loss: 0.8959 - accuracy: 0.6977\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 1s 52ms/step - loss: 0.9032 - accuracy: 0.7209\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 49ms/step - loss: 0.8565 - accuracy: 0.7209\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 1s 64ms/step - loss: 0.8808 - accuracy: 0.7209\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 1s 54ms/step - loss: 0.9649 - accuracy: 0.5814\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 1s 54ms/step - loss: 1.0064 - accuracy: 0.5465\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 32ms/step - loss: 0.9748 - accuracy: 0.5814\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 35ms/step - loss: 0.9265 - accuracy: 0.7093\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 32ms/step - loss: 0.9549 - accuracy: 0.5698\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 294, 400)          90000     \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 448)               179648    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2245      \n","                                                                 \n","=================================================================\n","Total params: 3471893 (13.24 MB)\n","Trainable params: 3471893 (13.24 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 32ms/step - loss: 0.8422 - accuracy: 0.7442\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 294, 400)          90000     \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 448)               179648    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2245      \n","                                                                 \n","=================================================================\n","Total params: 3471893 (13.24 MB)\n","Trainable params: 3471893 (13.24 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 31ms/step - loss: 1.0085 - accuracy: 0.5698\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 480)               192480    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2405      \n","                                                                 \n","=================================================================\n","Total params: 3497685 (13.34 MB)\n","Trainable params: 3497685 (13.34 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 35ms/step - loss: 0.9802 - accuracy: 0.5698\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 32ms/step - loss: 0.8341 - accuracy: 0.7326\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 34ms/step - loss: 0.8759 - accuracy: 0.7209\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 33ms/step - loss: 0.9586 - accuracy: 0.5698\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 296, 400)          64400     \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3472277 (13.25 MB)\n","Trainable params: 3472277 (13.25 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 24ms/step - loss: 0.8317 - accuracy: 0.7209\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 35ms/step - loss: 0.9137 - accuracy: 0.7326\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 37ms/step - loss: 0.9173 - accuracy: 0.6744\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 34ms/step - loss: 0.9797 - accuracy: 0.6977\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 35ms/step - loss: 0.9149 - accuracy: 0.7093\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 34ms/step - loss: 0.8523 - accuracy: 0.7093\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 34ms/step - loss: 0.8847 - accuracy: 0.7093\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 294, 400)          90000     \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 448)               179648    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2245      \n","                                                                 \n","=================================================================\n","Total params: 3471893 (13.24 MB)\n","Trainable params: 3471893 (13.24 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 33ms/step - loss: 0.9254 - accuracy: 0.5814\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 32ms/step - loss: 0.8698 - accuracy: 0.7209\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 294, 350)          78750     \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 350)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               179712    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3461027 (13.20 MB)\n","Trainable params: 3461027 (13.20 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 30ms/step - loss: 1.1043 - accuracy: 0.4302\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 294, 350)          78750     \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 350)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               179712    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3461027 (13.20 MB)\n","Trainable params: 3461027 (13.20 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 30ms/step - loss: 0.9319 - accuracy: 0.7093\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 1s 61ms/step - loss: 1.0069 - accuracy: 0.5698\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 51ms/step - loss: 1.0152 - accuracy: 0.5233\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          102800    \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3510677 (13.39 MB)\n","Trainable params: 3510677 (13.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 1s 56ms/step - loss: 0.9843 - accuracy: 0.6628\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 28)           2800000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 294, 150)          29550     \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 150)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 480)               72480     \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2405      \n","                                                                 \n","=================================================================\n","Total params: 2904435 (11.08 MB)\n","Trainable params: 2904435 (11.08 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 26ms/step - loss: 0.8622 - accuracy: 0.7093\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 295, 200)          38600     \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 200)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               102912    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3344077 (12.76 MB)\n","Trainable params: 3344077 (12.76 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 25ms/step - loss: 0.8287 - accuracy: 0.7326\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 350)          89950     \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 350)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 448)               157248    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2245      \n","                                                                 \n","=================================================================\n","Total params: 3449443 (13.16 MB)\n","Trainable params: 3449443 (13.16 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 1s 49ms/step - loss: 1.1276 - accuracy: 0.4302\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 28)           2800000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 294, 400)          78800     \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 384)               153984    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 1925      \n","                                                                 \n","=================================================================\n","Total params: 3034709 (11.58 MB)\n","Trainable params: 3034709 (11.58 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 30ms/step - loss: 0.8843 - accuracy: 0.7093\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 32)           3200000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 295, 300)          57900     \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 300)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 416)               125216    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2085      \n","                                                                 \n","=================================================================\n","Total params: 3385201 (12.91 MB)\n","Trainable params: 3385201 (12.91 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 22ms/step - loss: 0.8854 - accuracy: 0.5814\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 28)           2800000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 293, 400)          90000     \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 400)               0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 512)               205312    \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 3097877 (11.82 MB)\n","Trainable params: 3097877 (11.82 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","3/3 [==============================] - 0s 31ms/step - loss: 0.8529 - accuracy: 0.7093\n","\n"]}]},{"cell_type":"code","source":["#–¢—é–Ω–µ—Ä GRU\n","# tuner_gru.search(x_train,                  # –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n","#              y_train,                  # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã\n","#              batch_size=128,           # –†–∞–∑–º–µ—Ä –º–∏–Ω–∏-–≤—ã–±–æ—Ä–∫–∏\n","#              epochs=15,                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è\n","#              validation_split=0.1,     # –ß–∞—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏\n","#              )"],"metadata":{"id":"c82LvH5CPPFh"},"execution_count":null,"outputs":[]}]}